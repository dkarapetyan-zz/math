\documentclass[12pt,reqno]{amsart}
\usepackage[showonlyrefs=true]{mathtools} %amsmath extension package
\usepackage{cancel}  %for cancelling terms explicity on pdf
\usepackage{yhmath}   %makes fourier transform look nicer, among other things
\usepackage{framed}  %for framing remarks, theorems, etc.
\usepackage{enumerate} %to change enumerate symbols
\usepackage[margin=2.5cm]{geometry}  %page layout
%\usepackage[pdftex]{graphicx} %for importing pictures into latex--pdf compilation
\numberwithin{equation}{section}  %eliminate need for keeping track of counters
\usepackage{hyperref}
\hypersetup{colorlinks=true,
linkcolor=blue,
citecolor=blue,
urlcolor=blue,
}
\usepackage[alphabetic, initials, msc-links]{amsrefs} %for the bibliography; uses cite pkg. Must be loaded after hyperref, otherwise doesn't work properly (conflicts with cref in particular)
\newcommand{\ds}{\displaystyle}
\newcommand{\ts}{\textstyle}
\newcommand{\nin}{\noindent}
\newcommand{\rr}{\mathbb{R}}
\newcommand{\nn}{\mathbb{N}}
\newcommand{\zz}{\mathbb{Z}}
\newcommand{\cc}{\mathbb{C}}
\newcommand{\ci}{\mathbb{T}}
\newcommand{\zzdot}{\dot{\zz}}
\newcommand{\wh}{\widehat}
\newcommand{\p}{\partial}
\newcommand{\ee}{\varepsilon}
\newcommand{\vp}{\varphi}
\newcommand{\wt}{\widetilde}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{no}[theorem]{Notation}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}{Example}[section]
\newtheorem{exercise}[theorem]{Exercise}
\begin{document}
\title{Notes on Numerical Computation }
\author{David Karapetyan}
\date{\today}
%
\maketitle
%
%
%
%
%
%
\section{Interpolation}
While finding a least fit line to a series of data points
(see the least squares minimization problem in these notes)
makes sense if the data points do indeed have a linear
relation to one another, it is grossly inaccurate otherwise.
One way around this is to find nonlinear functions that
interpolate the data. If we assume these functions are analytic,
then we end up with a possibly infinite polynomial interpolating
the data in question. We now direct our attention to deriving
the appropriate interpolating polynomial, given a set of associations
$ \left\{ (x_i, y_i \right\}_{i = 0}^{n}$, where $f(x_i) = y_i$. 
We shall first establish a lemma.
\begin{lemma}[Vandermonde Determinant]
\label{vander}
Let 
\begin{equation*}
V_{n-k} \doteq  \begin{bmatrix}
1 & x_k & x_k^{2} & \ldots & x_k^{n} \\
1 & x_{k+1} & x_{k+1}^{2} & \ldots & x_{k+1}^{n} \\
\vdots \\
1 & x_n & x_n^{2} & \ldots & x_n^{n} \\
\end{bmatrix}
\end{equation*}
Then \[ \det V_{n-k} = \prod_{k < i < j < n}(x_i - x_j) \]
\end{lemma}
\begin{proof}
We first transform the first row and column to $e_1$ 
via row and column operations. More precisely, we multiply the $n$ column by $-x_0$ and then
add to the $n+1$ column, then multiply the $n-1$ column by $x_0$ and add
to the $n$ column, all the way down to the first column. This gives
\begin{equation*}
V_{n-k}' = \begin{bmatrix}
1 & 0 & 0 & 0 & \ldots  & 0 \\
1 & (x_{k+1} - x_k) & x_{k+1}(x_{k+1} - x_k) & x_{k+1}^2(x_{k+1} - x_k) & \ldots & x_{k+1}^{n-1}(x_{k+1} - x_k) \\
1 & (x_{k+2} - x_k) & x_{k+2}(x_{k+2} - x_k) & x_{k+2}^2(x_{k+2} - x_k) & \ldots & x_{k+2}^{n-1}(x_{k+2} - x_k) \\
\vdots \\
1 & (x_n - x_k) & x_n(x_n - x_k) & x_n^2(x_n - x_k) & \ldots & x_n^{n-1}(x_n - x_k) \\
\end{bmatrix}
\end{equation*}
Now, multiplying the first row by $-1$ and adding it to the remaining rows,
we obtain
\begin{equation*}
V_{n-k}' = \begin{bmatrix}
1 & 0 & 0 & 0 & \ldots & 0 \\
0 & (x_{k+1} - x_k) & x_{k+1}(x_{k+1} - x_k) & x_{k+1}^2(x_{k+1} - x_k) & \ldots & x_{k+1}^{n-1}(x_{k+1} - x_k) \\
0 & (x_{k+2} - x_k) & x_{k+2}(x_{k+2} - x_k) & x_{k+2}^2(x_{k+2} - x_k) & \ldots & x_{k+2}^{n-1}(x_{k+2} - x_k) \\
\vdots \\
0 & (x_n - x_k) & x_n(x_n - x_k) & x_n^2(x_n - x_k) & \ldots & x_n^{n-1}(x_n - x_k) \\
\end{bmatrix}
\end{equation*}
Observe that factoring $x_{k+i} -x_k$ from the $i+1$ row, for each $1 \le i \le n$, we obtain
\begin{equation*}
\begin{bmatrix}
1 & 0 \\
0 & V_{n-k-1}
\end{bmatrix}
\end{equation*}
Hence, the problem of computing the Vandermonde matrix determinant is inherently
recursive. Recall that multiplying rows and columns by non-zero constants and adding them to each
other does not impact the determinant of a matrix. However, if $A \mapsto A_c$
via multiplication of a row by some nonzero constant $c$, then
$|A_c| = c |A|$. Hence, from our above computations and a cofactor expansion
of $V_n'$, it follows that 
\begin{align*}
& |V_{n-k}| = \prod_{k < i \le n} (x_i - x_k) |V_{n-k-1}|\\
& | V_2 | = \left |
\begin{bmatrix}
1 & x_{n-1} \\
1 & x_{n}
\end{bmatrix}
\right | = x_n - x_{n-1}
\end{align*}
which gives
\begin{align*}
|V_{n-k}| & = \prod_{k=0}^{n-2} \prod_{1 \le i \le n} (x_i - x_k)\\
& = \prod_{k < i < j \le n} (x_i - x_k)
\end{align*}
completing the proof.
\end{proof}
\begin{theorem}
For an arbitrary data set $ \left\{ (x_i, y_i) \right\}_{i = 0}^{n}$ with the $x_i$
distinct, there
exists a unique polynomial $p$ of degree at most $n$ such that
\begin{equation*}
p(x_i) = y_i, \quad 0 \le i \le n
\end{equation*}
\end{theorem}
\begin{proof}
For the existence and uniqueness, we write an ansatz of degree at most $n$
\begin{equation*}
p(x) = a_0 + a_1x + a_2x^2 + \ldots + a_nx^n
\end{equation*}
and try to find the $a_i$. 
Our problem reduces to solving
\begin{equation*}
V_n a^{T} = y^{T}
\end{equation*}
where $\vec{a} = (a_0, a_1, \ldots, a_n)$, $\vec{y} = (y_0, y_1, \ldots, y_n)$,
and where $V_n$ is the $(n+1) \times (n+1)$ Vandermonde matrix
\begin{equation*}
\begin{bmatrix}
1 & x_0 & x_0^{2} & \ldots & x_0^{n} \\
1 & x_1 & x_1^{2} & \ldots & x_1^{n} \\
\vdots \\
1 & x_n & x_n^{2} & \ldots & x_n^{n} \\
\end{bmatrix}
\end{equation*}
which is invertible by Lemma \ref{vander}. Hence, the $a_i$ are uniquely determined.
\end{proof}
\begin{remark}
Observe that we can find infinitely many polynomials of degree $m > n$
interpolating a given data set $\left\{ (x_i, y_i) \right\}_{i=0}^n$. To see
this, note that we can simply extend this data set to $ \left\{ (x_1, y_i
\right\}_{i=0}^m$, where $(x_i, y_i)$, $n < i < m$ are chosen arbitrarily with
the $x_i$ distinct, and then compute the inverse of the resulting Vandermonde
matrix.
\end{remark}
\subsubsection{Newton Interpolation}
As we have seen, finding exact solutions to $n \times n$
systems of form $Ax = b$ takes $O(n^3)$ computations in general.
Our advantage for our linear system is that we are dealing with polynomials,
which have an incredible amount of structure. Hence, we are motivated
to utilize this to develop an $O(n)$ algorithm for finding the explicit
form of our polynomial. 
To do so, we implement an idea of Newton, rewriting our polynomial as
\begin{align*}
p(x) &= c_0 + c_1(x - x_0) + c_2(x - x_0)(x - x_1) + \ldots
+ c_{n}(x - x_0)(x - x_1)\cdots(x - x_{n-1})
\\
& = \sum_{i=0}^{n} c_i \prod_{j=0}^{i-1} (x - x_j), \quad \prod_{j=0}^{1} (x - x_j) \doteq 1.
\end{align*}
The $c_i$ can be computed in a naive fashion by plugging in $x_i$ 
and solving for $c_i$, for each $i$. Computation of $c_i$ will then depend
on computations of all $c_{< i}$. Let us formalize all this. 
\begin{definition}
Let $f$ be a function which we wish to approximate via a polynomial,
and $ \left\{ x_i, y_i \right\}_{i=0}^{n} $ a given data set with the
$x_i$ distinct. Let
\begin{equation*}
p(x) = c_0 + c_1(x - x_0) + c_2(x - x_0)(x - x_1) + \ldots
+ c_{n}(x - x_0)(x - x_1)\cdots(x - x_{n-1})
\end{equation*}
be the unique polynomial of at most degree $n$ interpolating $f$
on the given data set. We call
\begin{equation*}
f[x_0, x_1, \ldots, x_n] \doteq c_n
\end{equation*}
a \emph{divided difference} of $f$. It is the coefficient of the
highest degree term in the polynomial of at most degree $n$ interpolating the
given data set. 
\end{definition}
\begin{theorem}[Divided Differences]
We have the recursive formula
\begin{equation*}
f[x_0, x_1, x_2, \ldots, x_n] = \frac{f[x_0, x_1, \ldots, x_{n-1}] - f[x_1, x_2, \ldots, x_n]}{x_n - x_0}
\end{equation*}
\end{theorem}
\begin{proof}
Let $p_{0,n}$ denote the unique polynomial of degree at most $n$ that
interpolates $f$ at $x_0, x_1, \ldots, x_n$, and  We define
$p_{1,n}$ the unique polynomial of degree at most $n-1$ that interpolates
$f$ at $x_0, x_1, \ldots, x_n$. Then it is easy to check that
\begin{equation*}
p_{0,n} = p_{1,n} + \frac{x - x_n}{x_n - x_0}[p_{1,n} - p_{0,n-1}]
\end{equation*}
Observe that the coefficient of $x^n$ is given by the coefficient of $x_n$ in the expression 
\begin{equation*}
\frac{x(p_{1,n} - p_{0,n-1})}{x_n - x_0}
\end{equation*}
completing the proof.
\end{proof}
\subsection{Hermite}
Let $f(x) \in C^m(\rr)$ with
\begin{equation}
\label{hinterp-data}
\begin{split}
f^i(x_j) = b_{i j }, \qquad 0 \le j \le k, \ 0 \le i \le n_j
\end{split}
\end{equation}
where $b_{j k}$ is a constant. Can we find a polynomial $p(x)$ such that
\begin{equation}
\label{h-poly-data}
\begin{split}
p^i(x_j) = b_{i j}, \qquad 0 \le j \le k, \ 0 \le i \le n_j,
\end{split}
\end{equation}
holds? If so, we call $p(x)$ a \emph{Hermite polynomial}, and we say that $p(x)$
\emph{interpolates} $f(x)$ at the \emph{nodes} $x_k$.
\begin{theorem}
If $\sum_{j=0}^{k} (n_{j}+1) = m$, then there exists a unique polynomial of
degree less than or equal to $m$ satisfying \eqref{h-poly-data}.
\end{theorem}
\begin{proof} 
Write
\begin{equation*}
\begin{split}
p(x) = a_0 + a_{1}x + a_{2}x^2 +\cdots+ a_{m}x^m
\end{split}
\end{equation*}
where the constants $\{a_i\}_{0 \le i \le m}$ are to be determined. We wish to solve
the system given by
\begin{gather*}
a_{0}+a_{1}x_{j} + a_{2}x_{j}^2 +\cdots+a_{m}x_{j}^{m} = b_{0 j}
\\
a_{1} + 2 a_{2}x_{j} + 3a_{3}x_{j}^2 + \cdots+ m a_{m}x_{j}^{m-1} =
b_{1j}
\\
\vdots
\\
n_j! a_{ n_{j}} x_{j}^{m -n_{j}} +
(n_j + 1)! a_{n_{j}+1}
x_{j}^{m - n_{j}-1} + \cdots + \frac{m!}{(m-n_j)!} a_{m}x_{j} = b_{n_j j}
\end{gather*}
ranging over $0 \le j \le k$. In matrix form, this is expressed by $A\vec{a}=B$, where
\begin{gather*}
A= \begin{bmatrix}
A_1
\\
A_2
\\
\vdots
\\
A_k
\end{bmatrix}
, \ A_j=\begin{bmatrix}
1 & x_{j} & x_{j}^{2} & \cdots &\cdots &\cdots & \cdots &x_{j}^{m}\\
0 &1 &2x_{j} & \cdots &\cdots &\cdots &\cdots &mx_{j}^{m-1}\\
0 &0 &2 & \cdots &\cdots &\cdots &\cdots &m(m-1)x_{j}^{m-2}\\
\vdots & \vdots &\vdots &\ddots &\\
0 &0 &0 &\cdots & n_j! \  & (n_j + 1)! x_j & \cdots &\frac{m!}{(m-n_{j})!} x_j^{m -n_{j}}
\end{bmatrix},
\\
\vec{a}=\begin{bmatrix}
a_{0}\\
a_{2}\\
\vdots\\
a_{m}
\end{bmatrix}
\\
B = \begin{bmatrix}
B_1\\
B_2\\
\vdots \\
B_k
\end{bmatrix}
, \ B_j=
\begin{bmatrix}
b_{0j}\\
b_{1j}\\
\vdots \\
b_{n_j j}.
\end{bmatrix}
\end{gather*}
Note that $A_j$ is a $(n_j +1) \times (m+1)$ matrix. Hence, for $A$ to be a 
square matrix, we must have $\sum_{j = 0}^k (n_j+1) = m$, or
$\sum_{j = 0}^{k} n_j = m - k - 1$

Therefore, assuming this condition, to complete
the proof it will be enough to show that $A$ is invertible, or, equivalently,
that if $A \vec{a}=\vec{0}$, then $\vec{a} = \vec{0}$. Proceeding, suppose
$A \vec{a}=\vec{0}$. Then $\vec{a}$ defines a polynomial $p(x)$ with root $x_j$ of
multiplicity $n_j +1$. Hence,
\begin{equation*}
\begin{split}
p(x) = q(x) \prod_{j=0}^{k}(x-x_{j})^{n_{j}+1}.
\end{split}
\end{equation*}
However, note that $\prod_{j=0}^{k}(x-x_{j})^{n_{j}+1}$ is of order $m+1$,
whereas the coefficients of $\vec{a}$ define $p(x)$ to be of order at most $m$.
It follows that $q \equiv 0$. 
\end{proof}
\begin{remark}
Observe that if $\sum_{j=0}^{k}(n_j + 1) < m$, then the system is underdetermined.
It is not too hard to show that we will have infinitely many solutions. That is, there will be infinitely
many polynomials of degree $m$ which interpolate the given data.
\end{remark}
\section{Finite Difference Method}
\subsection{Burgers Equation}
Recall the Burgers initial value problem (ivp)
\begin{gather}
\label{burgers-eqn}
u_{t} = \frac{1}{2}(u^{2})_{x},
\\
\label{burgers-init-data}
u(x, 0) = u_{0}(x), \qquad x, t \in \rr.
\end{gather}
Note that for small \emph{stepsize} (or \emph{mesh}) $h, k >0$,
\begin{equation*}
\begin{split}
u_{t}(x,t) &\approx \frac{u(x, t+k) - u(x, t)}{k},
\\
u_{x}(x, t) & \approx \frac{u(x+h, t) - u(x, t)}{h}.
\end{split}
\end{equation*}
Set
\begin{gather*}
x_i = ih, \quad i \in \mathbb{Z}
\\
t_{j}=jk, \quad j \in \mathbb{N}
\end{gather*}
and let
\begin{gather*}
u_{i,j} = u(x_{i}, t_{j}).
\end{gather*}
Then the discretized Burgers ivp takes the form
\begin{gather*}
\frac{u_{i, j+1}- u_{i,j}}{k}=\frac{1}{2h}\left( u_{i+1,j}^{2} -
u_{i,j}^{2} \right),
\\
u_{i,0} = u_{0}(ih)
\end{gather*}
or
\begin{gather}
\label{burgers-discrete}
u_{i, j+1}=u_{i,j} + \frac{k}{2h}\left( u_{i+1,j}^{2} -
u_{i,j}^{2} \right),
\\
\label{burgers-discrete-init}
u_{i,0} = u_{0}(ih).
\end{gather}
Note that \eqref{burgers-discrete}-\eqref{burgers-discrete-init} gives us an
explicit numerical solution to the Burgers ivp
\eqref{burgers-eqn}-\eqref{burgers-init-data}. To illustrate this, we set
$j=0$, and obtain
\begin{equation}
\label{case-j=0}
\begin{split}
u_{i,1} = \underbrace{u_{i,0} + \frac{k}{2h}\left( u_{i+1,0}^{2} -
u_{i,0}^{2} \right)}_{\text{known from initial data
\eqref{burgers-discrete-init}}}.
\end{split}
\end{equation}
Similarly, for $j=1$, we have
\begin{equation*}
\begin{split}
u_{i,2} = \underbrace{u_{i,1}}_{\text{known from
\eqref{case-j=0}}} + \frac{k}{2h}\left(
\underbrace{u_{i+1,1}^{2}}_{\text{known from \eqref{case-j=0}}} -
\underbrace{u_{i,1}^{2}}_{\text{known from
\eqref{case-j=0}}} \right).
\end{split}
\end{equation*} 
This process can be
continued indefinitely to find $u_{i, j}$ for any $j \in \mathbb{N}$, and is
called an \emph{explicit finite difference method} for numerically solving the
Burgers ivp. An important drawback to this method is that it converges very
slowly to solutions of the Burgers ivp.
\section{Orders of Convergence}
We'd like to understand how fast a sequence convergence to its limit, without
having to worry about silly constants.
\begin{definition}
Let $\{x_{n}\} \in \mathbb{C}$ be a sequence converging to $x$. We say that
$x_{n} = O(\alpha_{n})$ if $|x_{n}| \le C |\alpha_{n}|$  for $n \ge N$, and $ x_n 
= o(\alpha_n )$ if $|x_n| \le \ee_n |\alpha_n |$, where $\ee_n \to 0$.
Similarly, for functions $f,g: \cc \to \cc$, we say that $f = O(g)$ in a
neighborhood $N$ of $x$ if $|f(x_n) | \le C |g(x_n)|$ for all $x_n \in N$, and 
$f = o(g)$ in a neighborhood $N$ of $x$ if $|f(x_n) | \le \ee_n |g(x_n)|$,
where $\ee_n \to 0$,  for all $x_n \in N$.
\end{definition}

\begin{example}
\begin{equation*}
\frac{n+1}{n^2} \le \frac{2n}{n^2} = \frac{2}{n} = O\left(\frac{1}{n} \right)	
\end{equation*}
\end{example}
\begin{example}
\begin{equation*}
\frac{1}{n \log n} \le \frac{\ee_n}{n} = o\left(\frac{1}{n}\right).
\end{equation*}
\end{example}

\begin{example}
\begin{equation*}
\frac{5}{n} + e^{-n} = O\left( \frac{1}{n}\right)
\end{equation*}
\end{example}

\begin{remark}
It is a true statement that $5/n^4 = O(1/n)$. However, this grossly
misrepresents the asymptotic behavior of the sequence $5/n^4$. Typically, when
we discuss the order of convergence of a sequence $x_n$ to its limit, we seek
the sharpest order $\alpha_n$ of convergence. By sharp, we mean that if $|x_n| =
O(\alpha_n)$, then $|x_n| \neq O(\alpha_n n^{-\ee})$, for any $\ee > 0$.
\end{remark}

\begin{example}
Using Taylor series,
\begin{equation*}
e^x - \sum_{k = 0}^{n-1} \frac{1}{k!}x^k = \frac{1}{n!} f^{n+1}(\xi)x^n =
O\left(\frac{1}{n!}\right).
\end{equation*}
\end{example}

\section{Linear Difference Equations and Operators}

Algorithms emit sequences. If we can detect a pattern between successive
iterates that is ``linear'' in nature, we can use powerful tools from linear
algebra to study the computational complexity and convergence properties of our
algorithm.
\begin{definition}
Denote $V$ to be the set of all infinite complex sequences
\begin{equation*}
\vec{x} = [x_1, x_2, \ldots, x_n, \ldots]
\end{equation*}
equipped with addition
\begin{equation*}
\vec{x} + \vec{y} = [x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n, \ldots]	
\end{equation*}
scalar multiplication
\begin{equation*}
\lambda \vec{x} = [\lambda x_1, \lambda x_2, \ldots, \lambda x_n, \ldots].
\end{equation*}
and norm
\begin{equation*}
\| \vec{x} \| = \sup \{|x_i| \}
\end{equation*}
It is not hard to see that $V$ is an separable infinite-dimensional vector
space.
\end{definition}
We wish to study the vector space $\mathcal{L}(V)$ of bounded linear operators
$T:
V \to V$, equipped with the standard definitions for addition, scalar
multiplication, and operator norm.
In particular, we are interested in the shift operator
\begin{align*}
& E: V \to V,
\\
& E \vec{x} = [x_2, x_3, \ldots, x_{n+1}, \ldots].
\end{align*}
which has the property 
\begin{align*}
& (E^k x)_{n} = x_{n+k}.
\end{align*}
\begin{definition}
Observe that the set of shift operators ${E_k}$ form a basis for a
linear subspace of $\mathcal{L}(V)$. We call this the space of
\emph{linear difference operators}, and denote it by $\mathcal{D}(V)$.
\end{definition}
\begin{definition}
Consider an element $T = \sum_{i=0}^{m}c_i E^i \doteq p(E)$ of $D(V).$ We call
$p(x) = \sum_{i=0}^{m} c_i x^i$ the \emph{characteristic polynomial} of $T$.
\end{definition}
\begin{remark}
Studying a linear difference equation is equivalent to studying the nullspace
of some operator $T \in D(V)$. Since $V$ is separable, we can think of
$\mathcal{D}(V)$ heuristically as nothing more than the space of $\mathbb{N}
\times \mathbb{N}$ matrices. 
\end{remark}
\begin{example}
\begin{equation*}
x_{n+2} - 3x_{n+1} + 2x_{n} = 0 \longleftrightarrow (E^2 - 3E^1 + 2)\vec{x} = 0
\longleftrightarrow p(E) \vec{x} = 0, \quad p(\lambda) = \lambda^2 - 3 \lambda
+2.
\end{equation*}
\end{example}
The tools from finite-dimensional matrix algebra extend to the
countable-dimension case. More precisely, we can categorize the solutions of the
of certain finite difference problem entirely in terms of eigenvectors and
eigenvalues.
\begin{theorem}
\label{thm:dif-eq}
Let $p(E) \vec{x} = 0$ be a linear difference equation. 
\begin{enumerate}
  \item 
If $\lambda$ is a root
to its characteristic polynomial, then $\vec{u} = [\lambda, \lambda^2,
\lambda^3, \ldots]$ is a solution to $p(E) \vec{x} = 0$. 
  \item 
If all roots $\lambda_k$ of $p$ have multiplicity $1$ and are nonzero, then
$ \bigcup_k \{ [\lambda_k, \lambda_k^2, \ldots] \}$ is a 
a basis for the solution space of $p(E) \vec{x} = 0$.	
\item If all roots $\lambda_k$ of $p$ are nonzero, and have multiplicity $k_n$
greater than or equal to $1$, then $\bigcup_k \left\{ [\lambda_k, \lambda_k^2,
\ldots], [\lambda_k, \lambda_k^2, \ldots]', \ldots, [\lambda_k, \lambda_k^2,
\ldots ]^{(k_n -1)} \right\}$ is a basis for the solution space of $p(E)
\vec{x} = 0$.
\end{enumerate}
\end{theorem}
\begin{proof} We split the proof into two parts.
\begin{enumerate}
  \item 
  We first show that $\vec{u}$ is an eigenvector of $p(E)$ with corresponding
  eigenvalue $p(\lambda)$. Observe that
  \begin{align*}
p(E) \vec{u} = \left(\sum_{i=0}^m c_i E^i \right) \vec{u}
& = \sum_{ i = 0}^m c_i \left( E^i \vec{u}\right)
\\
& = \sum_{i = 0}^m c_i [\lambda^{i+1}, \lambda^{i+2}, \ldots]
\\
& = \sum_{i = 0}^m c_i \lambda^i \vec{u}
\\
& = p(\lambda) \vec{u}.	 
\end{align*}
Hence, if $p(\lambda) = 0$, $\vec{u}$ is a solution.
\item
\label{simple}
For each root $\lambda_k$, consider the corresponding solution $u^{(k)} =
[\lambda_k, \lambda_k^2, \ldots]$. Let $\vec{x}$ be an arbitrary solution to our
linear finite difference problem. We want to show that $\vec{x} = \sum_{k=1}^m
a_k u^{(k)}$. We can write
the first $m$ components of $\vec{x} = [x_1, x_2, \ldots]$ as 
\begin{equation}
\label{det}
x_i = \sum_{k=1}^{m} a_k \lambda_k^i, \quad 1 \le i \le m.
\end{equation}
Writing this in matrix form, we obtain
\begin{equation*}
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_m
\end{bmatrix}
= 
\underbrace{
\begin{bmatrix}
\lambda_1 & \lambda_2 & \ldots & \lambda_m \\
\lambda_1^2 & \lambda_2^2 & \ldots & \lambda_m^2 \\
\vdots & \vdots & \ddots & \vdots \\
\lambda_1^m & \lambda_2^m & \ldots & \lambda_m^m 
\end{bmatrix}
}_\text{A}
\begin{bmatrix}
a_1 \\ a_2 \\ \vdots \\ a_m
\end{bmatrix}
\end{equation*}
We claim that the columns of $A$ are linearly
independent. To see this, we wish to show that the determinant of $A$ is
non-zero, via induction. For the base case, we consider the $1 \times 1$
sub-matrix $[\lambda_m^m]$, which has non-zero determinant (we assumed
a priori that the $\lambda_i$ are non-zero). For the inductive step,
we assume the sub-matrix with diagonal entries $\lambda_2^2$ to $\lambda_m^m$ has non-zero
determinant. Then it is easy to see that $A$ has non-zero determinant by taking
its principal minor at $\lambda_1$. 
Since A is a map from $\rr^m$ to itself, it follows that dimension of the nullspace of $A$ is $0$. Hence, $A$ is
non-singular, and so completely determines the $a_i$, for $1 \le i \le m$. We
can repeat the argument to obtain the next $m$ constants $a_i$. A simple 
induction completes the proof. 
\item
In the case where roots of multiplicity greater than $1$, we have more
difficulties: if we construct a matrix $A$ as in the proof of
Theorem \ref{thm:dif-eq}, it will be singular (we will have columns that are
identical). Hence, we must somehow construct new solutions out of roots with
multiplicity greater than $1$. We do so as
follows. Assume $\lambda$ has multiplicity $n$. Then for $k \le n-1$
\begin{align*}
p(E)x^{(k)}(\lambda)
& = [ p(E)x(\lambda) ]^{k} 
\\
& = [ p(\lambda)x(\lambda) ]^{k} 
\\
& = \sum_{i = 0}^{k} \binom{k}{i}p^{(k-i)}(\lambda) x^{(i)}(\lambda) = 0.
\end{align*}
Assume the set $ \left\{ x(\lambda)^{(k)} \right\}_{k=0}^{n-1} $ is not linearly
independent.
Then
\begin{equation*}
x^{(n-1)}(\lambda) = \sum_{i=0}^{n-2} c_i x^{(i)}(\lambda)
\end{equation*}
where some $c_i \neq 0$. By differentiation
\begin{equation*}
x^{(n)}(\lambda) = \sum_{i=0}^{n-1} c_i x^{(i)}(\lambda)
\end{equation*}
and hence
\begin{equation*}
p(E)x^{(n)}(\lambda) = 0
\end{equation*}
which implies $x(\lambda)$ has multiplicity greater than $n$, which is
a contradiction. Lastly, we run an argument similar to that in \eqref{simple}
to complete the proof, with the observation that we can now fill the
``redundant'' columns generated by non-simple roots 
with the new vectors we construct out of their derivatives.
\end{enumerate}
\end{proof}


\section{Least Squares Problem}
\subsection{Orthogonal Transformations and Hilbert Spaces}
We recall that linear transformations on finite-dimensional spaces are a combination
of scalings, translations, and rotations. Consequently, since matrices are linear transformations,
we would like to factor our matrices into a product of rotation matrices, and scaling matrices.
However, we must first formalize our notion of rotation. 
\begin{definition}
A Hilbert Space $V$ is a vector space equipped with a conjugate bilinear form 
$ \langle \cdot, \cdot \rangle : H \times H \to \cc $. More precisely, we have
\begin{enumerate}[a)]
\item{Symmetry:} $\langle x,y \rangle  = \overline{ \langle y,x \rangle   }$.
\item{Linearity:} $\langle \lambda x + y, z \rangle  =
\lambda \langle x, z \rangle  + \langle y, z \rangle$.
\item{Reflexivity:} $\langle x, x \rangle  $ is real and positive, 
and $\langle x, x \rangle = 0 $ if and only if $x = 0$.            
\end{enumerate}
\end{definition}
It is easy to check that the relation
\begin{equation*}
\| v \| = \sqrt{\langle v, v \rangle}
\end{equation*}
defines a norm on any inner product space. If $ \langle x, y \rangle $ is real
valued, we can think of it as the size of the angle between $x$
and $y$. Indeed, in $\rr^2$ equipped with the standard euclidean inner product,
one can check that
\begin{equation*}
\langle x, y \rangle  = \|x\| \|y\|\cos \theta
\end{equation*}
Hilbert spaces have other important properties. We shall utilize the next one 
frequently. 

\begin{theorem}
Let $H$ be a Hilbert space, and $A : H \to H$ a bounded linear operator. Then
there exists a map $A^{\star} : H \to H$ with the property that
\begin{equation*}
\langle Au, v \rangle  = \langle u, A^{\star}v \rangle 
\end{equation*}
for any $u, v \in H$. We call $A^{\star}$ the \emph{adjoint} of $A$.
\end{theorem}
\subsection{Simplification of Least Squares Problem via Adjoints} 
Ideally, we wish to find exact solutions to the linear problem $Ax = b$, where
$A$ is an $m \times n$ complex matrix, $x$ is an $n \times 1$ column vector, and
$b$ is an $m \times 1$ column vector. However, this is not always possible. For
example, if $\text{rank A} < m$, then the column space of $A$ may not span $b$.
In these situations, we look for $\tilde{x}$ such that $ \| b - A\tilde{x}\|$ is
minimized. The choice of norm is irrelevant in finite-dimensional vector spaces (a
simple but useful corollary of Theorem \ref{thm:adj-ls}),
though the Euclidean norm ($\ell^2$) is a powerful choice, since we can then
equip our normed space with the inner product
\begin{equation*}
\begin{split}
& \langle u, v \rangle \doteq \sum_i u_i \bar{v_i}
\\
&  \langle u, u \rangle =\| u \|_{\ell^2}^2.
\end{split}
\end{equation*}
Recall that operators on inner-product spaces always posses an adjoint. We now
make use of this important property to simplify our minimization problem
considerably.
\begin{theorem}
\label{thm:adj-ls}
Let $A$ be an $m \times n$ complex matrix with $\text{rank}(A) = n$. 
If $x$ is a point such that $A^{\star} (Ax - b) = 0$, then $x$ is the unique solution 
to the least squares problem.
\end{theorem} 
\begin{proof}
Assume $A^{\star}(Ax-b) = 0$. We first prove that $x$ must be a least squares solution. Observe that
 $\overline{A^{\star}(Ax-b)} = 0$, or $A^T
\overline{Ax-b} = 0$. Hence, $Ax-b$ is orthogonal to the column space of $A$. 
Let $y$ be any other point. Since $A(x-y)$ lives in the column space of $A$
, we see that $\langle Ax-b, A(x-y) \rangle = 0$. Hence, by the Pythagorean theorem,
\begin{equation*}
\| Ax - b \|_{\ell^2}^2 = \| Ay - b + A(x-y)
\|_{\ell^2}^2 = \| Ay - b \|_{\ell^2}^2 + \| A(x-y) \|_{\ell^2}^2 \ge \| Ay - b
\|_{\ell^2}^2.
\end{equation*}
Hence, $x$ is a least squares solution. Assume $y$ is a least squares solution. Then by our previous work, 
we must have $ \| A(x - y) \|_{\ell^2} \neq 0$. This implies $x-y$ lives in the nullspace of $A$.
But since $\text{rank}(A) = n$, by the rank-nullity theorem, the dimension of the nullspace 
is $0$. We conclude that $x = y$. 
\end{proof}
Observe that if $A$ is unitary, we obtain $x$ immediately:
\begin{equation*}
0 = A^{\star} (Ax-b) = x - A^{\star} b  \implies x = A^{\star} b.
\end{equation*}
This is a simplistic example that illustrates that unitary matrices simplify our least squares
computations considerably. Hence, it makes sense to look at decompositions of $A$ 
utilizing unitary matrices, in an attempt to simplify equations of form $A^{\star}(Ax-b = 0)$ 
even further.
More generally, suppose we can find a decomposition $A = QR$, where $Q$ is $m \times m$
unitary, and $R$ is $m \times n$ upper-triangular. Then
\begin{equation*}
0 = A^{\star}(Ax-b) = R^{\star } Q^{\star}(Ax-b) = R^{\star} Q^{\star}(QRx - b) \implies R^{\star}Rx = R^{\star}Q^{\star} b.
\end{equation*}
Hence, the $QR$ decomposition permits us to convert our problem of finding a least square
into the equivalent problem of finding a solution $x$ to a problem of $LU$ type, which is much
more tractable.

We now discuss the two most popular $QR$ decomposition algorithms.
\subsubsection{Gram-Schmidt}
\begin{theorem}
Let $\{x_1, x_2, \ldots, x_n\}$ be a linearly independent in $C^n$. Then the set
 \\ $ \left\{ u_1, u_2, \ldots, u_n \right\} $ is an orthonormal basis for $C^n$,
where
\begin{equation*}
u_k = \frac{x_k - \sum_{i < k } \langle x_k, u_i \rangle u_i}{\|x_k -
\sum_{i < k } \langle x_k, u_i \rangle u_i\|_{\ell^2}}
\end{equation*}
\end{theorem}
\begin{proof}
It is a straightforward computation of $ \langle u_i, u_j \rangle $.
\end{proof}
Applying the Gram-Schmidt process to the columns of a matrix  $ A $, we obtain
the decomposition $ BT $, where $ B $ is an $ m \times n $ matrix consisting of
the orthonormalized column vectors of $ A $, and $ T $ consists of an $ n \times
n $ upper triangular matrix with positive diagonal, whose entries come from
saved computations in the Gram-Schmidt algorithm.

Substituting this decomposition into $A^{\star}(Ax-b) = 0$, we 
\begin{equation*}
T^{\star}Tx = T^{\star}B^{\star}b
\end{equation*}
which is again a problem of $LU$ type. 

\begin{remark}
Instead of resorting to Gram-Schmidt or some other factorization, we could
attempt to solve $ A^{\star}(Ax - b) = 0 $ directly. That is, we attempt to
solve $ A^{\star}Ax = A^{\star}b $ via a Cholesky factorization ($ A^{\star}A$
is Hermitian and positive semidefinite). The problem with this approach is that
we may have $\kappa(A^{\star}A) >> k(A)$. A good example of this is 
\begin{equation*}
A = 
\begin{bmatrix}
 1 & 1 & 1 \\
 \ee & 0 & 0 \\
 0 & \ee & 0 \\
 0 & 0 & \ee 
\end{bmatrix}
\end{equation*}
\end{remark}
\subsubsection{Householder}
A Householder decomposition is of form $A = QR$, where $Q$ is a $m \times m$ unitary matrix, and
$R$ is an $m \times n$ upper triangular matrix. Observe that the columns of a unitary matrix
are orthonormal (they have other useful properties), so this decomposition has better properties
in general than the Gram-Schmidt decomposition. Indeed, substituting into $A^{\star}(Ax-b)$, we 
obtain the $LU$ type problem
\begin{equation*}
R^{\star}R x = (QR)^{\star} b.
\end{equation*}
\subsubsection{Singular Value Decompositions}
We now discuss arguably the most useful decomposition of an arbitrary $m \times
n$ matrix. We first require a lemma.
\begin{lemma}
\label{lem:hermite}
A Hermitian, positive semidefinite matrix has real positive eigenvalues.
\end{lemma}
\begin{proof}
We first show that any Hermitian matrix has real eigenvalues. Suppose $A$ is Hermitian.
Then
\begin{equation*}
v^{\star} A^{\star} v = (A v)^{\star} v = \bar{\lambda} \| v \|_{\ell^2}^{2}
\end{equation*}
and
\begin{equation*}
 v^{\star} A v = v^{\star} \lambda v =  \lambda \| v \|_{\ell^2}^{2}
 \end{equation*}
 Since $A$ is Hermitian, $A = A^{\star}$, so we must have $\bar{\lambda} =
\lambda$, which implies that $\lambda$ is real. Furthermore, Hermitian matrices
are not degenerate. That is, an $n \times n$ Hermitian matrix has n distinct
eigenvectors, since any $n \times n$ Hermitian matrix is similar to a diagonal matrix (which
has eigenvectors $e_1, e_2, \ldots, e_n$, where the $e_i$ are the standard 
canonical basis vectors for $C^n$). 
\end{proof}
\begin{theorem}[Singular Value Decomposition]
Let $A$ be an arbitrary $m \times n$ matrix with complex entries.
Then there exists a decomposition $A = PDQ$, where $P$ is $m \times m$ unitary,
$D$ is a $m \times n$ diagonal matrix, and $Q$ is a $n \times n$ unitary matrix.
This decomposition is not necessarily unique.
\end{theorem} 
\begin{proof}
The non-uniqueness of the $PDQ$ decomposition is easy: the $0$ matrix has multiple $PDQ$ type decompositions (just take $D$ = 0, and $P, Q$ to be
arbitrary unitary matrices).
To prove existence of the $PDQ$ decomposition for arbitrary $m \times n$ matrices $A$, we
first assume without loss of generality that $m \ge n$ (otherwise, we consider $A^T$).
By Lemma \ref{lem:hermite}, for each of $n$ linearly independent eigenvectors
$u_i$ of $A^{\star}A$, there exist associated positive eigenvalues $\sigma_i^2$.
Furthermore,
\begin{equation}
\label{u-trans}
\| A u_i \|_{\ell^2}^2 = \langle Au_i, Au_i \rangle  = \langle u_i, A^{\star}A
u_i \rangle = \sigma_i^{2} u_i
\end{equation}
We can also choose these eigenvectors such that they are orthonormal. To see
this, recall that $A^{\star}A$ is Hermitian, and that Hermitian matrices are
similar to diagonal matrices. Since square diagonal matrices have eigenvectors
$e_i$, the eigenvectors of a Hermitian matrix are given by a unitary transformation
of the $e_i$, which preserves orthonormality since 
\begin{equation*}
\langle Q u_i, Qu_j \rangle  = \langle u_i, Q^{\star}Q u_j \rangle  = \langle
u_i, u_j \rangle  = \delta_{ij}.
\end{equation*}
Hence, we have the decomposition $A^{\star}A = \tilde{D}^{2}Q$ where $\tilde{D}$ is an $n \times n$
matrix consisting of the positive square roots of eigenvalues $\sigma_i^2$ of
$A^{\star}A$, and $Q$ is the $n \times n$ unitary matrix with rows
$u_i^{\star}$. 
 
We now massage this decomposition to come up with $PDQ$. In effect, we seek to
factor $A^{\star}\tilde{D} = PD$, where $D$ is a $m \times n$ diagonal matrix
with diagonal consisting of the $ |\sigma_i|$, and $P$ is $m \times m$ unitary. 
Assume non-zero eigenvalues for $i \le r$, where $r \le n$, and zero otherwise.
Then by \eqref{u-trans}, $A u_i \neq 0$ for $i \le r$, and $A u_i = 0$ otherwise.
Set $v_i = Au_i/\sigma_i, 1 \le i \le r$. It is easy to check that these $v_i$ are orthonormal.
Using Gram-Schmidt, extend the set ${v_1, \ldots, v_r}$ to an orthonormal set
$\{v_1, \ldots, v_m \}$, and set $P$ to be the matrix with the $v_i$ on its columns.
Then one can verify that $A = PDQ$. Notice that this decomposition is not unique: it depends on how we
arrange our orthonormal vectors in $P$ and $Q$. 
\end{proof}
Substituting a singular value decomposition into our minimization result $A^{\star}(Ax - b) = 0$, 
we obtain
\begin{equation*}
 0 = Q^{\star} D P^{\star}(PDQx - b) = Q^{\star} D^{2} Qx - Q^{\star}D P^{\star}b \implies D^{2}Qx = DP^{\star}b
\end{equation*}
Let $D^{+}$ be the matrix with $1/\sigma_i$ in its diagonal
for $\sigma_i \neq$, and $0's$ otherwise. This is what's known as a \emph{pseudo-inverse} of $D$. 
Multiplying both sides of the last expression by $Q^{\star}(D^{+})^{2}$, we obtain
\begin{equation*}
x =Q^{\star} D^{+} P^{\star} b  
\end{equation*}
Hence, we have proved the following result. 

\begin{theorem}
Let $Ax = b$ be a system to which we seek least-squares solution, where
$A$ has singular value decomposition $PDQ$.
Then the solution is given by $x = A^{+}b$, where $A^{+} = Q^{\star} D^{+} P^{\star}$
\end{theorem}
This motivates the following definition.
\begin{definition}
Let $A$ be an arbitrary $m \times n$ matrix with singular value decomposition $PDQ$. 
Then we call $A^{+} = Q^{\star}D^{+} P^{\star}$ its \emph{pseudoinverse}.
\end{definition}
It can be shown that even though singular value decompositions for a matrix $A$ are not
unique, the pseudoinverse $A^{+}$ is. 
\section{Finding Eigenvectors and Eigenvalues}
Having seen the importance of the singular value decomposition in the preceding
section, we would like to develop an algorithm for finding the eigenvalues and
corresponding eigenvectors of $A^{\star}A$ for any given matrix $A$. From there,
we saw in the last section that we can orthonormalize the set of eigenvectors
via Gram-Schmidt and, if necessary, extend this set to form a basis for $C^{n}$.
Once we do that, our $PDQ$ decomposition will be complete.
\subsection{The Power Method}
We now develop an algorithm for finding the eigenvalue of maximum modulus 
for a non-zero $n \times n$ matrix $A$
and its corresponding eigenvector. We require that there is a unique
eigenvalue of maximum modulus, and that $A$ be diagonalizable. 
Proceeding, suppose $\lambda_1$ is the eigenvalue of maximum modulus.
If $\lambda_1 = 0$, then $A = 0$, contradicting our earlier assumption. 
Hence, $\lambda_1 \neq 0$, with corresponding eigenvector $u_1$.
Choose $x^{(0)} \in \cc^n$ that is $n$-dimensional. Then we must have
\begin{equation*}
x^{(0)} = \sum_{i=1}^n a_i u_i, \quad a_i \neq 0 \ \forall i
\end{equation*}
It follows that
\begin{align*}
x^{(k)} 
= A^{k}x^{(0)}
= \sum_{i=1}^n a_i \lambda_i^{k} u_i 
= \lambda_1^{k}\sum_{i=1}^n a_i (\lambda_i/\lambda_1)^k u_i = a_1 \lambda_1^k u_1 + \ee_k
\end{align*}
Let $\phi: \cc^n \to \cc$ be the projection function of an input onto
its first coordinate. Then
\begin{equation*}
\phi(x^{(k)})/ \phi(x^{(k-1)}) \to \lambda_1
\end{equation*}
from which we obtain 
\begin{equation*}
x^{(k)}/\lambda_1^k \to a_1 u_1
\end{equation*}
which is the corresponding eigenvector. 
From a theoretical standpoint, using this for finding the corresponding
eigenvector is perfectly fine. However, numerically it is a bit imprecise,
since we are using an approximation to $\lambda_1$ (due to machine rounding
and termination of our algorithm after a pre-specified number of iterations)
and then taking powers of it to compute our eigenvector. There is a risk that 
this will compound the errors already present, introducing complexity
to our problem. There is a simple way to bypass this: 
we can normalize $x^{(k)} \to x^{(k)}/\| x^{(k)} \| \doteq
\tilde{x}^{(k)}$ at each step, and still obtain
\begin{equation*}
\phi(\tilde{x}^{(k)})/ \phi(\tilde{x}^{(k-1)}) \to \lambda_1. 
\end{equation*}
However, now we also have 
\begin{equation*}
\tilde{x}^{(k)} = \frac{a_1 \lambda_1^k u_1 + \ee_k}{\| a_1 \lambda_1^k u_1 + \ee_k\|}
\to u_1/\| u_1 \|
\end{equation*}
which is an eigenvector corresponding to the eigenvalue $\lambda_1$.
\subsection{Inverse Power Method}
This is almost identical to the Power method, with the additionally caveat that
we require $A$ to be invertible. We observe that for square matrices, $Av =
\lambda v$ implies $A^{-1}v \lambda^{-1}v$ so long as $A$ is invertible or,
equivalently, that $\lambda \neq 0$. Hence, if $\lambda$ is the unique
eigenvalue of minimum modulus of a diagonalizable matrix $A$, and it is the
unique eigenvalue of maximum modulus of the diagonalizable matrix $A^{-1}$.
Hence, we can apply the power method on $A^{-1}$ to find $\lambda^{-1}$, from
which we recover $\lambda$. In practice, it is not computationally efficient to
compute $A^{-1}$ and then run the power method. Rather, we 
rewrite $x^{(k)} = (A^{-1})^k x^{(0)}$ in equivalent form as
\begin{equation*}
x^{(k)} = Ax^{(k-1)}
\end{equation*}
We can now apply an $LU$ decomposition to make it easy to compute $x^{(k)}$.
To see that an invertible diagonalizable $n \times n$ matrix $A$ has cofactors that are all
zero (and hence, admits an $LU$ decomposition), simply observe that $A = PDP^{-1}$ and
where $P$ has columns consisting of $n$ linearly independent
eigenvectors of $A$, and $D$ has diagonal with no zeroes in it. 
The rest of the algorithm is then analogous to the power method.
\subsection{Shifted Power Method}	
Lastly, we would like to find all eigenvalues between those with largest and
smallest modulus, respectively. First, observe that if a random chosen $\mu$ is
closest to a simple eigenvalue $\lambda$ of a square matrix $A$, then $\lambda -
\mu$ is the unique eigenvalue of minimum modulus of $A - \mu I$. If $A - \mu I$
is invertible, we can run the inverse power method to obtain $\lambda - \mu$,
from which we are able to obtain $\lambda$.

\section{Linear Iteration}
All linear iterations are of form
\begin{equation*}
x^{k+1} = Ax^{k} + b.
\end{equation*}
We would like to know under what conditions the iteration converges. To explore
this line of questioning further, we will need a series of lemmas.
\begin{lemma}[Schur's Lemma]
Every square matrix is unitarily similar to an upper triangular matrix
whose off diagonal entries are arbitrarily small.
\end{lemma}
\begin{corollary}
\label{cor:spec}
If $A$ is an $n \times n$ matrix with complex entries, then 
\begin{equation*}
\rho(A) = \inf_{ \| \cdot \|} \| A \|
\end{equation*}
\end{corollary}
\begin{proof}
We shall show that $\rho(A) \le \inf_{ \| \cdot \|} \| A \|$ and $\rho(A) \ge
\inf_{ \| \cdot \|} \| A \|$, respectively. If $A$ is the $0$ matrix, there is
nothing to prove. Otherwise, $A$ has s
ome nonzero eigenvalue, with a corresponding eigenvector which cannot be $0$.
Hence,
\begin{equation*}
|\lambda| \cancel{\| x \|} = \| Ax \| \le \| A \| \cancel{\| x \|}.
\end{equation*}
from which it follows that $\rho(A) \le \| A \|$. 

For the reverse direction, we first apply Schur's Lemma to 
decompose $SAS^{-1} = D + T_{\ee}$. Observing that similar matrices have the same spectrum,
 we obtain 
\begin{equation*}
\| SAS^{-1} \|_{\ell_\infty} \le \rho(A) + \ee.
\end{equation*}
It is easy to check that, for fixed $S$,  $ \| A \|_{\ell^{\infty'}} \doteq \| S A S^{-1} \|_{\ell^\infty}$
is a norm. 
Since $\ee > 0$ can be taken to be arbitrarily small, we conclude that
$\rho(A) \ge
\inf_{ \| \cdot \|} \| A \|$.
\end{proof}
We now have the tools to prove the following.
\begin{theorem}
\label{thm:it}
The linear iteration $x^{k+1} = Ax^{k} + b$ converges for any initial
vector $x^{0}$ if and only if $\rho(A) < 1$. If so, then $x^{k} \to
(I-A)^{-1}b$.
\end{theorem}
\begin{proof}
To prove sufficiency, we first observe that convergence implies
a fixed point to the map $x \mapsto Ax + b \doteq Tx$.
This map is a contraction if \[\| Tx - Ty \| \le \| x - y \|,\] which simplifies
via linearity of $T$ to \[\| A(x-y) \| \le \| x - y \|. \] But
\begin{equation*}
\| A(x-y) \| \le \| A \| \| x-y \| 
\end{equation*}
If $\rho(A) < 1$, by Corollary \ref{cor:spec} there exists a norm $\| \cdot \|$ such that
$ \| A \| < 1.$ Hence, $T : C^n \to C^n$ is a contraction under this norm, and so Therefore,
$x^{k}$ converges to some $x$ in this norm. Since all norms are equivalent on finite dimensional 
spaces, it follows that $x^{k}$ converges to $x$ in any norm. Since $x$ is a fixed point of $T$, 
we must have 
$x = Ax + b$, or $x = (I-A)b$.
To prove necessity, assume that $\rho(A) \ge 1$. Let $b = 0$ and $x^{0}$ be the eigenvector
with largest eigenvalue $\lambda$ amongst all eigenvalues. Then $|\lambda| \ge 1$, and so 
\begin{equation*}
\| x^{k+1} - x^{k} \| = \| (\lambda^{k} - \lambda^{k-1})x^{0} \| =
|\lambda|^{k} \| (1/\lambda -1) x^{0} \| \to \infty, \quad \lambda > 1
\end{equation*}
If $\lambda =1$, then $-\lambda$ is also an eigenvalue, with corresponding
eigenvector $-x^{0}$. Therefore, assuming without loss
of generality that $\lambda = -1$, we obtain
\begin{equation*}
\| x^{k+1} - x^{k} \| = \| (\lambda^{k} - \lambda^{k-1})x^{0} \| =
2 \|x^{0} \| 
\end{equation*}
which implies $\{x^{k}\}$ is not Cauchy.
\end{proof}
\begin{corollary}[Neumann Iteration]
The sum
\begin{equation*}
S \doteq \sum_{k = 0}^{\infty} A^{k}
\end{equation*}
converges if and only if
$\rho(A) < 1$. If so, then $S = (I - A)^{-1}$.
\end{corollary}
\begin{proof}
One can generalize the proof of Theorem \ref{thm:it} to let $b$ and our initial
choice $x^{0}$ be matrices. Choosing $b = 0$ and $x^{0} = I$, we see that
\begin{align*}
& x^{1} = I,  \\
& x^{2} = A + I,\\
& x^{3} = A^{2} + A + I, \\
& \vdots \\
& x^{k} = \sum_{i = 0}^k A^{i}
\end{align*} 
which by Theorem \ref{thm:it} converges to $(I -A)^{-1}$.
\end{proof}
\begin{remark}
This corollary is important in that it provides us  an $O(n^3)$ iterative method
(multiplication of two $n \times n$ matrices takes $O(n^3)$ computations) for
computing the inverses of matrices. More precisely, note that if $\rho(I + A)
\ge 1$, then $\rho(I + cA) < 1$ for some $-1 < c < 1$. Hence, we can apply
Neumann iteration to $I + cA$ to recover the inverse of $cA$. From there, it is
easy to find the inverse of $A$.
\end{remark}

\subsection{Iterative Refinement Schemes}
Observe that if $B = A^{-1}$, then
\begin{equation*}
x = x + B(b - Ax).
\end{equation*}
If $B \approx A^{-1}$, then we set
\begin{equation*}
x^{k+1} = x^{k} + B(b - Ax^{k}).
\end{equation*}
Let $Q = B^{-1}$. Then we obtain
\begin{equation*}
x^{k+1} = Q^{-1}(Q - A)x^{k} + Q^{-1}b.
\end{equation*}
\begin{definition}
We call $Q$ a \emph{splitting} matrix. If $Q = I$, the iterative scheme is
called \emph{Richardson Iteration}. If $Q$ is a diagonal matrix with diagonal
equal to that of $A$, then it is called \emph{Jacobi Iteration}. Lastly, if $Q$
is a lower triangular matrix with lower triangular part (including the diagonal)
equal to that of $A$, the iterative scheme is called \emph{Gauss-Seidel
Iteration}.
\end{definition}
Notice that in all three cases, $Q^{-1}$ is easily computed. Computing the exact inverse
of an $n \times n$ matrix is generally an $O(n^3)$ computation, and computing
an approximate inverse via Neumann iteration is also, in general, $O(n^3)$. For these special $Q$, 
it is $O(n)$ (the exact inverse is computed via backward substitution of $n$ rows, while in the
Neumann iteration we are multiplying by zero on $O(n^2)$ times) .

However, if we wish to bypass finding the inverse of $Q$, we can do so. More precisely, if
$Q = I$, we obtain
observe that
\begin{equation*}
x^{k+1} = x^{k} + e^{k}, \quad e^{k} \doteq A^{-1}(b - Ax^{k})
\end{equation*}
Then one way to run our iterative refinement as follows: Given base case $x^{0}$, we compute 
\begin{enumerate}[i)]
\item $r^{k} = b - Ax^{k}$
\item $Ae^{k} = r^{k}$
\item $x^{k+1} = x^{k} + e^{k}$
\end{enumerate}
With an appropriate chosen splitting matrix $Q$, the analogous iteration may become
more computationally efficient. We then have
\begin{equation*}
Qx^{k+1} = (Q - A)x^{k} + b.
\end{equation*}
Note that for Gauss-Seidel iteration, the choice of $Q$ is clever, in that this
iteration  is converted to 
\begin{equation*}
L x^{k+1} = - U x^{k} + b
\end{equation*}
where $L$ and $U$ are the upper and lower triangular parts of $A$ respectively,
where $U$ has zeroes on its diagonals. 

\begin{remark}
Hence, it is quite clear that it is more efficient to choose a splitting matrix
before iterating. However, one must be careful to first identify whether or not
the Gauss-Seidel scheme converges for a given matrix $A$. If it does, then in
general it converges faster in general than the Jacobi and Richardson iterative
schemes. If it does not, then we must resort to using another scheme.
\end{remark}
\section{Well-Conditioned and Ill-Conditioned Linear Problems}
Now that we have developed techniques for solving $n \times n$
systems $Ax = b$, we'd like to understand how sensitive our system
is to perturbations. More precisely, we can ask the following two questions:
\begin{enumerate}[a)]
\item If we shift $x \mapsto \tilde{x}$, how much does $b \mapsto 
\tilde{b}$ shift, relative to the size of $b$?
\item If we shift $b \to \tilde{b}$, how much does the solution $x \mapsto
\tilde{x}$ shift, relative to the size of $x$?
\end{enumerate}
\begin{definition}
Let $A$ be a square, invertible matrix. We call $\kappa(A) \doteq \| A \| \| A^{-1} \|$
its \emph{condition number}.
\end{definition}
\begin{theorem}
We have the estimates
\begin{equation*}
\kappa(A)^{-1} \frac{\| b - \tilde{b}\|}{\| b\|} \le \frac{\| x - \tilde{x} \|}{\| x \|}
\le \kappa(A) \frac{ \| b - \tilde{b} \|}{ \| b \| }.
\end{equation*}
\end{theorem}
\begin{proof}
Observe that
\begin{equation*}
\| b - \tilde{b} \| \| x \| = \| A(x - \tilde{x})\| \| A^{-1}b \| 
\le \kappa(A) \| x - \tilde{x} \| \| b \|
\end{equation*}
and
\begin{equation*}
\| x - \tilde{x} \| \| b \| = \| A^{-1}(b - \tilde{b})\| \| Ax \| 
\le \kappa(A) \| b - \tilde{b} \| \| x \|
\end{equation*}
Rearranging the terms in each expression, we obtain the lower and upper
bounds for the size of the relative shift of $\tilde{x}$ from $x$, completing
the proof.
\end{proof}
Hence, large condition numbers imply that small perturbations in $b$ can have
large impacts on the resulting perturbation of $x$, and vice versa.
Motivated by this, we have the following definitions.
\begin{definition}
For a square matrix $A$, if $\kappa(A) \le 1$, we say $A$ is well-conditioned.
Otherwise, we say $A$ is ill-conditioned.
\end{definition}
\begin{remark}
The upper-bound we use for the upper bound necessary for $A$ to be well-conditioned
is somewhat arbitrary, and ultimately depends on the problem we are analyzing
and how accurate our collected data $b$ is. If we are dubious as to 
the accuracy of our data-collecting techniques, we will 
need $\kappa(A)$ to be small,
perhaps much smaller than just $1$, in order to
ensure that the $x$ we come up with as solution to $Ax = b$ is not far
from the actual $x$ occurring in the real-world. In this case we would define
a well-conditioned matrix as having a condition number less than some $c$, where
$c << 1$. 
\end{remark}
\begin{remark}
The condition number can also be thought of as a measure of how close a matrix
is to not being invertible. For example, consider
\begin{equation*}
A_\ee = 
\begin{bmatrix}
1 & 1+ \ee \\
1 - \ee & 1 \\
\end{bmatrix}
\end{equation*}
A simple computation shows that $\kappa(A_\ee) = O(1/\ee^2)$.
Hence, $\kappa(A_\ee) \to \infty$ as $\ee \to 0$. Of course,
the matrix
\begin{equation*}
A = 
\begin{bmatrix}
1 & 1 \\
1  & 1 \\
\end{bmatrix}
 = \lim_{\ee \to 0} A_\ee
\end{equation*}
is not invertible.
\end{remark}
\end{document}
