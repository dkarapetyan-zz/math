\documentclass[12pt,reqno]{amsart}
\usepackage[showonlyrefs=true]{mathtools} %amsmath extension package
\usepackage{amssymb} %for symbols like lesssim
\usepackage{cancel}  %for cancelling terms explicity on pdf
\usepackage{yhmath}   %makes fourier transform look nicer, among other things
\usepackage{framed}  %for framing remarks, theorems, etc.
\usepackage{enumerate} %to change enumerate symbols
\usepackage[margin=2.5cm]{geometry}  %page layout
%\usepackage[pdftex]{graphicx} %for importing pictures into latex--pdf compilation
\numberwithin{equation}{section}  %eliminate need for keeping track of counters
\usepackage[alphabetic, initials, msc-links]{amsrefs} %for the bibliography; uses cite pkg. Must be loaded after hyperref, otherwise doesn't work properly (conflicts with cref in particular)
\newcommand{\ds}{\displaystyle}
\newcommand{\ts}{\textstyle}
\newcommand{\nin}{\noindent}
\newcommand{\rr}{\mathbb{R}}
\newcommand{\nn}{\mathbb{N}}
\newcommand{\zz}{\mathbb{Z}}
\newcommand{\cc}{\mathbb{C}}
\newcommand{\ci}{\mathbb{T}}
\newcommand{\zzdot}{\dot{\zz}}
\newcommand{\wh}{\widehat}
\newcommand{\p}{\partial}
\newcommand{\ee}{\varepsilon}
\newcommand{\vp}{\varphi}
\newcommand{\wt}{\widetilde}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}{Example}[section]
\newtheorem{exercise}{Exercise}
\begin{document}
\title{Notes on Numerical Computation }
\author{David Karapetyan}
\date{\today}
\maketitle
\section{Limitations of the Machine}
\subsection{Introduction}
A computer is just a sophisticated collection of circuits
with ``off'' and ``on'' switches. We represent ``off'' and
``on'' by $0$ and $1$, respectively. A computer program 
at its lowest level is just a collection of a string of $0$'s and $1$'s.
When we write a computer program in a higher level language such as C
or Python, a compiler translates the program into $0$'s and $1$'s,
which are then then processed by the computer.

Besides the base $10$ system, it is important to understand binary (base $2$),
octal (base $8$), and hexadecimal (base $16$). Binary is encountered most often,
since that is the number system computer instructions (``on'' and ``off'' switches)
are encoded in.

\subsubsection{Loss of Precision}
Each digit in a binary representation of a number takes a bit of memory to hold.
Since there is a finite amount of memory in the computer, we must chop all digits
after a specific digit in a binary number. 
\begin{example}
$1/10 = (0.001100110010011\ldots)_2 \approx (0.00110011)_2$
\end{example}
If we chop at the $nth$ digit, then the error bound is $10^{-n}$.
We have the alternative of rounding up or down after the $nth$ digit. 
In this case the error bound is $1/2 \times 10^{-n}$.

For binary, we have the scientific notation $x = q \times 2^{m}$, where
$1/2 < |q| < 1$. This is entirely analogous to the base $10$ case.

In these days of increasing RAM and decreasing prices, one need not be too worried
about truncation or rounding errors, unless the numerical simulation one is performing
is highly unstable (in which case, small errors can lead to wildly incorrect results---more
on this later).

\subsubsection{Loss of Significance}
\begin{definition}
Let $x^\star \in \cc$ be an approximation to $x \in \cc$. Then we define
the \emph{absolute error} and the \emph{relative error} as $|x - x^\star|$
and $|(x - x^\star)/x|$, respectively.
\end{definition}
In practice, absolute error matters very little. Its prominent uses
include tricking the masses into believing you are benevolent. They don't understand
math. But we do!
\begin{example}
Angelina Jolie donates $20,000$ to SomeRidiculousCharity(SRC). The masses clap,
the masses cheer, and celebrate her as a saint. Angelina Jolie's net
worth is $20,000,000$. In absolute terms, this seems like a lot. In relative
terms, she has donated \\ $20,000/20,000,000 \times 100 \%$, or $0.1 \%$ of
one year's salary. For perspective, if Joe Johnson, average bloke's 
annual salary is $60,000$, then $0.1 \%$ of this is $60$.
\end{example}
\begin{example}
Take $x = 10^6 + 10^3$ and $x^\star = 10^6$. Then $x - x^\star = 10^3$,
but $(x - x^\star)/x = 10^{-3}$.
\end{example}
In short, relative errors are much more important. 
Loss of significance occurs when two nearly equal quantities are subtracted.
\begin{example}
Let $x = 0.3721478613$ and $y = 0.3720230572$. Then $x-y = 0.0001248121$.
The repetitive $0's$ are taking up memory that could be better used storing
significant digits (digits other than $0$). If our computer truncates
after the $6th$ digit, then
$x^\star = 0.37215$, $y^\star = 0.37202$, and the relative error
of $x^\star-y^\star$ from $x-y$ is approximately $4\%$. This is because
$x^\star - y^\star = 0.00013$. We have wasted three digits out of
five digits on storing $0's$.
\end{example}
Hence, if possible, one should avoid subtracting nearly equal quantities
in an algorithm.
\begin{example}
If $y = \sqrt{x^2 + 1} -1$ and we are inputting $x \approx 1$, we rewrite this as $x^2/\sqrt{x^2 + 1} + 1$.
No loss of significance occurs in this latter form.
\end{example}
A loss of precision also occurs when dividing by a numbers very close to $0$.
Indeed, we now demonstrate the link between subtracting two nearby numbers
and division by a number close to $0$.
\begin{example}
 Let $x, y \approx 0$ be real, strictly positive
numbers with a decimal component, and let $x^\star, y^\star$ be their truncated
machine approximations, where the truncation occurs after the $5th$ decimal digit.
We compute the relative error of $x^\star/y^\star$ from $x$; that is
\begin{align*}
!\frac{x/y - x^\star/y^\star}{x/y} |  = | \frac{x - x^\star y/y^\star}{x} |
\end{align*}
It is easy to see that $\lim_{y \to 0 }y/y^\star = 0$ (for a machine, $y/y^\star$
will become undefined once $y$ gets small enough, but, while defined, it is
a strictly decreasing quantity tending towards $0$). Therefore, our relative
error approaches $x/x = 1$, or $100\%$.
\end{example}
As a last example, observe that even if we are careful not to subtract nearby
quantities or divide by numbers close to $0$, we can still have precision errors
if we are inputting approximated values into highly oscillatory functions.
\begin{example}
Consider the function $f(x) = \cos(nx)$, where $n >> 1$ is an integer.
Then $f(2\pi) = 1$, but $f(2 \pi^\star)$ can be quite far from $1$.
One way around this is to use interval arithmetic---that is, try to
compute what interval values near $2 \pi$ will get mapped to. This is
outside the scope of this course.
\end{example}
One way to partially offset all these worries about loss of precision
and significance is to have a lot of RAM, and to take advantage
of $64$ bit processors (which will eventually become $128$ bit, $256$ bit,
and so on.
\subsubsection{Numerical Instability}
There are certain numerical iterations that are more sensitive
to loss of precision and significance than others.
\begin{example}
Consider
\begin{align*}
& x_{n+1} = 13x_n/3 - 4x_{n-1}/3
\\
& x_0 = 1
\\
& x_1 = 1/3
\end{align*}
This has the theoretical solutions $x_n = (1/3)^n$. However, numerically,
$x_n$ diverges to infinity. This is because of the loss of precision
emanating from repeated subtractions of nearby quantities. These
errors accumulate with each iteration. We call such a numerical iteration
\emph{unstable}.
\end{example}
\begin{remark}
A stable version of the same iteration is given by
\begin{align*}
& 2 x_n /3 - x_{n-1}/3
\\
& x_0 = 1
\\
& x_1 = 1/3
\end{align*}
\end{remark}
We can extend our notion of instability to functions as well.
For example, we can ask how sensitive a function's output value is
to perturbations of its input. If $f$ is differentiable, we compute
the relative error of $f$ at $x$:
\begin{align*}
\frac{f(x + h) - f(x)}{f(x)} \approx \frac{h f'(x)}{f(x)} = 
\left[ \frac{x f'(x)}{f(x)}\right]\left( \frac{h}{x}\right)
\end{align*}
which motivates the following definition.
\begin{definition}
Let $I$ be an open subset of $\rr$. Then for $f \in C^1(I)$, 
we call 
\begin{align*}
\left[ \frac{x f'(x)}{f(x)}\right]
\end{align*}
the \emph{condition number} of $f$ in $I$.
\end{definition}
Note that the condition number can potentially magnify
the relative error ($h/x$) of the perturbed input, resulting
in a potentially large relative error for the output. Heuristically,
the condition number is a measure of how sensitive $f$ is to perturbations
in its input.
\begin{example}
Let $f, g \in C^2$, $F = f + \ee g$, and $r$ a simple root of $f \in C^2$.
Then we must perturb $r \mapsto r + h$ to obtain the corresponding
root of $F$. We are interested in the magnitude of $h$. We have
\begin{align*}
0 = F(r + h) & \approx f(r) + hf'(r) + \ee g(r) + h\ee g'(r) + O(h^2)
\\
& = h[f'(r)  + \ee g'(r)] + \ee g(r)  + O(h^2)
\end{align*}
Assuming $h, \ee \ll 1$, we obtain 
\begin{align*}
h \approx -\frac{\ee g(r)}{f'(r)}
\end{align*}
Letting $f(x) = \prod_{k=1}^{20} (x-k)$ and $g(x) = x^{20}$, we obtain
\[h \approx -\ee g(20)/f'(20) = -\ee 20^20/20! \approx -\ee 10^8.\]
This suggests that even for small $\ee$, if $\ee \not < 10^{-8}$,
then $h$ may potentially be very large. 
\end{example}
\section{Finding Roots of Functions}
We are interested in finding $x$ such that
$f(x) = a$, where $a \in \rr$. This is equivalent to finding
the roots of the function $g(x) \doteq f(x) - a$.
\subsection{Bisection Method}
We assume that $f$ is continuous. Then if $f(a)f(b) \le 0$ for $a < b$,
then we must have $f(x) = 0$ for some $x \in [a,b]$. The algorithm is as follows.
\subsubsection{Convergence and Error Analysis} 
We would like to first establish that our method converges to a root of
$f(x)$ in the interval $[a,b]$, given that $f$ has a root in this interval.
Now $ \left\{ a_n \right\} $ and $ \left\{ b_n \right\}  $ 
are increasing, and decreasing sequences, respectively, both of which
are bounded from below and above by $a$ and $b$. Hence, both converge
to a limit. Indeed,
\begin{equation}
\label{b-a-dif}
 b_{n+1} - a_{n+1} = 1/2(b_n - a_n)
\end{equation}
and so
\begin{equation}
\label{bd}
b_{n} - a_{n} = 2^{-n}(b_0 - a_0)
\end{equation}
which implies
\begin{equation*}
\lim a_n = \lim b_{n} = c.
\end{equation*}
However, $a_n$ and $b_n$ have the property that
\begin{equation*}
f(a_n)f(b_n) \le 0.
\end{equation*}
By continuity
\begin{equation*}
0 \ge \lim f(a_n)f(b_n) = f(c)^2
\end{equation*}
which implies that $f(c) = 0$. Hence, the bisection method has converged to a root
$c$ in $[a,b]$.
Hence, one approach to approximating $c$ is to prescribe an error threshold, and
take $a_n$, for $n$ sufficiently large. However, we can get a better estimate
if we take $c_n \doteq 1/2(a_n + b_n)$. By triangle inequality and \eqref{b-a-dif}, we obtain
\begin{equation*}
|c - c_n| \le 1/2 (b_n - a_n)
\end{equation*}
and so
\begin{equation*}
|c - c_n| \le 2^{-n - 1}(b_0 - a_0)
\end{equation*}
Compare this with the estimate \eqref{bd}. It guarantees convergence
that is at least twice as fast as that given by taking $a_n$ or $b_n$
as our approximation to $c$. Note that the speed of convergence is linear.
\subsection{Newton's Method}
The main advantages of Newton's method over the bisection method is that if it
converges to a root, then its convergence is quadratic. The drawbacks is that it
may not converge to a root at all if the starting point $x_0$ of the algorithm
is not sufficiently close to a root. (In particular, a general heuristic is that
if the function becomes steep near a root, then our choice $x_0$ has to be very
close to that root in order to converge to it.) The essence of the Newton
algorithm is to approximate a function via its linearization. More precisely,
assuming $r$ is a root of $f \in C^2$, and $x_0$ is an initial guess a positive
distance $h$ from $r$, we write
\begin{align*}
0 = f(r)  = f(x_0 + h) \approx f(x_0) + h f'(x_0) + O(h^2)
\end{align*}
from which we obtain $h \approx -f(x_0)/f'(x_0)$.
Hence
\begin{align*}
r & \approx x_0 + h \\
& \approx x_0 - \frac{f(x_0)}{f'(x_0)}.
\end{align*}
The Newton algorithm is then given by
\begin{align*}
x_{n+1} = x_{n} - f(x_n)/f'(x_n).
\end{align*}
\subsubsection{Error Analysis}
Assume we have a suitable $x_0 \in I$ and $f \in C^2(I)$ such that the algorithm converges
to a \emph{simple} root $r \in I$. Then we have
\begin{align*}
e_{n+1} = x_{n+1 - r} = e_n - f(x_n)/f'(x_n) = \frac{e_n f'(x_n) - f(x_n)}{f'(x_n)}
\end{align*}
and 
\begin{align*}
0 = f(r) = f(x_n - e_n) = f(x_n) - e_n f'(x_n) + e_n^2 f''(\xi_n)/2.
\end{align*}
Substituting, we obtain 
\begin{align*}
| e_{n+1} | = \left | \frac{e_n^2 f''(\xi_n)}{2 f'(x_n)} \right |
\end{align*}
Since $r$ is a simple root by assumption, we can always choose a subsequence
$\{x_{n_k} \}$ such that $f'(x_{n_k}) > c > 0$. Hence, using this fact
that the continuity of $f''$, we obtain the estimate
\begin{align*}
| e_{n+1} | \lesssim | e_n|^2.
\end{align*}
Hence, if the Newton method converges to a simple root for a $C^2$ function,
it converges quadratically. 
\subsection{Secant Method}
Note that for each iteration of Newton's method, we have
two costly computations: computing $f(x_0)$ and $f'(x_0)$.
To reduce this cost, we introduce the approximation
\begin{align*}
f'(x_0) \approx \frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}.
\end{align*}
and substitute this into Newton's method to get
\begin{align*}
x_{n+1} = x_n - \frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}
\end{align*}
Observe that now we have only one costly computation per iteration.
That is, we only need to compute $f(x_n)$, since $f(x_{n-1})$ is known
from the previous iteration. 
\subsubsection{Error Analysis}
We write
\begin{align*}
e_{n+1} = x_{n+1}-r
& = \frac{f(x_n)x_{n-1} - f(x_{n-1})x_n}{f(x_n) - f(x_{n-1})} - r
\\
& = \frac{f(x_n)e_{n-1} - f(x_{n-1})e_n}{f(x_n) - f(x_{n-1})}
\\
& = \frac{[f(x_n)/e_n - f(x_{n-1})/e_{n-1}](x_n - x_{n-1})}{[f(x_n) - f(x_{n-1})]/[x_n - x_{n-1}]}e_n e_{n-1}
\end{align*}
which by the mean value theorem is bounded by
\begin{align*}
& [f(x_n)/e_n - f(x_{n-1})/e_{n-1}][e_n + r - (e_{n-1} + r)]e_n e_{n-1}
\\
& = [f(x_n)  + f(x_{n-1}) - f(x_{n-1})e_n / e_{n-1} - f(x_n)e_{n-1}/e_n]e_n e_{n-1} 
\\
& \lesssim e_n e_{n-1}
\end{align*}
Therefore,
\begin{align*}
e_{n+1} \lesssim e_n e_{n-1}.
\end{align*}
\subsection{Fixed Points}
Observe that Newton's method can be rewritten as 
\begin{align*}
x_{n+1} = F(x_n), \quad F(x) = x - f(x)/f'(x).
\end{align*}
More generally, observe that if arbitrary $F$ is continuous, and
$x_n \to s$, then
\begin{align*}
s = \lim {x_{n+1}} = \lim F(x_n) = F(s)
\end{align*}

\begin{definition}
Let $F: X \to X$, where $X$ is a vector space. We say $s \in X$ is a 
\emph{fixed point} of $F$ if $F(s) = s$.
\end{definition}
Hence, the problem of finding the roots of a function $f$ is equivalent
to finding the fixed points of $F(x) \doteq x - f(x)/f'(x)$.
What conditions do we need to impose on $F$ in order to guarantee
the existence of a fixed point?
\begin{definition}
  Let $\left( X, d \right)$ be a metric space. A mapping $T: X \to X$ is called a
\emph{contraction} if there exists an $\alpha$, $0 \le \alpha <1$ such that
\begin{equation*}
	\begin{split}
		d(Tx, Ty) \le \alpha d(x,y), \qquad \forall x, y \in X.
	\end{split}
\end{equation*}
\end{definition}
\begin{remark}
	Observe that a contraction mapping is always continuous.
\end{remark}
\begin{example}
	The function $T(x) = 0.1x^2$ defines a contraction mapping in the set 
	$X = [-4, 4]$ equipped with the metric $d(x,y) = |x-y|$. 
\end{example}
We now give some examples of iterations with a contractive $F: \rr \to \rr$.
\begin{example}
\begin{align*}
& x_0 = -15
\\
& x_{n+1} = 3 - \frac{x_n}{2}
\end{align*}
\end{example}
\begin{example}
\begin{align*}
& x_0 = \pi/4
\\
& x_{n+1} = 2 - \sin(2x)/3
\end{align*}
\end{example} 

	\begin{proposition}
		\label{lem:fixed-point}
	Let $(X,d)$ be a complete metric space, and $T: X \to X$ a contraction
	mapping. Then $T$ has a unique fixed point in $X$. That is, there is a unique
	point $x^* \in X$ such that $Tx^* = x^*$. Furthermore, if $x_0$ is any point
  in $X$, and we define the sequence $x_{n+1} = Tx_n$, then $x_n \xrightarrow{X} x^*$ as $n
	\to \infty$.
	\end{proposition}
  \begin{proof} First we show uniqueness. If $x^*$ and $x^{**}$ are two fixed
	points, then
	\begin{equation*}
		\begin{split}
			d(x^*, x^{**}) = d(Tx^*, Tx^{**}) \le \alpha d(x^*, x^{**}) \implies d(x^*,
			x^{**}) = 0 \implies x^* = x^{**}.
		\end{split}
	\end{equation*}
To prove existence, we observe that since $X$ is complete it suffices to show
that $x_n$ is Cauchy. A repeated application of the
contraction inequality gives
\begin{equation*}
	\begin{split}
		d\left( x_{n+1},x_n \right)
		& = d\left( Tx_n, Tx_{n-1} \right)
		\\
		& \le \alpha d\left( x_n, x_{n-1} \right)
		\\
		& \le \alpha^2 d\left( x_{n-1}, x_{n-2} \right)
		\\
		& \cdots
		\\
		& \le \alpha^n d\left( x_1, x_0 \right).
	\end{split}
\end{equation*}
Hence
\begin{equation*}
\begin{split}
  d\left( x_{n+k},x_n \right)
  & \le (\alpha^{n } +\alpha^{n+1} + \cdots +
  \alpha^{n+k-2} + \alpha^{n+k-1})d(x_{1}, x_{0}) 
  \\
  & = \alpha^{n}(1 + \alpha + \cdots + \alpha^{k-2} + \alpha^{k-1})
  \\
  & \le \alpha^{n}\left( \frac{1}{1 - \alpha} \right)
  \\
  & \to 0 \ \text{as} \ n \to \infty
\end{split}
\end{equation*}
since $0 \le \alpha < 1$. 
\begin{remark}
Note that it is not necessary that $F$ be contractive in order to 
have convergence of an iteration. For example,
\begin{align*}
& x_{n+1} = 1/2 - x_n
\\
& x_0 = 1/4
\end{align*}
converges to $1/4$, but $F(x) \doteq 1/2 - x_n$ is not a contraction mapping
on any closed subset $I \in \rr$. The key thing to realize is that
for \emph{special} $x_0$ we can still have convergence for non-contractive $F$.
The advantage of having contractive $F$ on a closed set $X$ is that we do not need to worry
about our initial data--as long as it lives in $X$,
we are guaranteed convergence of our iteration.
\end{remark}
\end{proof}
It turns out that $F$ also gives great insight into the speed
of convergence.
\begin{proposition}
Consider the iteration $x_{n+1} = F(x_n)$, where $F \in C^{q}(I)$ for some integer
$q \ge 1$ and open $I \in \rr$. Assume $F$ is contractive on a closed subset $I' \subset I$.
Then the iteration
converges to $s = s(x_0)$, and the order of convergence is the first strictly positive integer $k \le q$
such that $F^{(k)}(s) \neq 0$.
\end{proposition}
\begin{proof}
Observe that
\begin{align*}
e_{n+1} = F(x_n) - s = F(s + e_n) - F(s).
\end{align*}
A Taylor expansion then gives
\begin{align*}
e_{n+1} = e_nF'(s) + e_n^2 F''(s)/2 + \ldots + e_n^{q-1}F^{(q-1)}(s)/(q-1)! + 
e_n^q F^{(q)}(\xi_n)/q!.
\end{align*}
Furthermore,
\begin{align*}
e_{n+1} = F(x_n) - F(s) = F'(\xi_n)e_n, \quad s < \xi_n < x_n
\end{align*}
Since $F$ is contractive on $I'$, it follows from the mean value theorem that 
\begin{align*}
| F'(x) | < 1, \quad \forall x \in I'.
\end{align*}
which guarantees that, for suitably large $N$, we have $e_n < 1$ for all $n > N$.
Assume $k \le q$ is the first strictly positive integer such that $F^{(k)}(s) \neq 0$. Then from our
Taylor expansion, the continuity of $F^{(j)}$, $j \le q$, and the fact that
$e_i < e_i^j$ for any $j \ge 1$, we obtain
\begin{align*}
|e_{n+1}| = O(|e_n|^k)
\end{align*}
completing the proof.
\end{proof}
\section{Roots of Polynomials}
Polynomials have much nicer properties than arbitrary functions,
so we'd like to use that to our advantage when looking for
roots. 
\begin{theorem}
All zeroes of a polynomial $p(z) = a_0 + a_1 z + \ldots + a_{n-1}z^{n-1} + 
a_n z^n$, $a_n \neq 0$ lie in a closed disc centered at the origin, with radius
\begin{align*}
r = 1 + |a_n|^{-1} \max_{0 \le k \le n-1} |a_k|.
\end{align*}
\end{theorem}
\begin{proof}
It is enough to show that there are no zeroes outside this disc. Assuming
$z$ is outside the disc, we estimate 
\begin{align*}
|p(z) |
& \ge |a_n z^n| - | a_{n-1}z^{n-1} + \ldots + a_0 |
\\
& \ge |a_n| |z|^n | - c \sum_{k=0}^{n-1} |z|^{k}, \quad 
c \doteq \max_{0 \le k \le n-1} |a_{k}|
\\
& = |a_n| |z|^n  - c \left[ \frac{|z|^n - 1}{|z|-1}\right]
\\
& > |a_n| |z|^n - c \left[ \frac{|z|^n - 1}{1 + |a_n|^{-1}c - 1}\right]
\\
& = |a_n| |z|^n - |a_n|(|z|^n - 1)
\\
& = |a_n| > 0.
\end{align*}
\end{proof}
\begin{corollary}
If all zeroes of a polynomial $p$ are in the disc $ \left\{ z: |z| \le r \right\}$,
then all non-zero zeroes of $p$ are outside the disc $ \left\{ z: |z| < 1/r \right\}$.
\end{corollary}
\begin{proof}
Let $s(z) \doteq z^n p(1/z)$. Then for nonzero $z_0$, $s(z_0) = 0$ if and only if
$p(1/z_0) = 0$. But for nonzero $z_0$, $|z_0| \le r$ is equivalent to
$|1/z_0 \ge 1/r|$, which completes the proof.
\end{proof}
\subsection{Polynomial Evaluation}
For a polynomial $p(z)$, computing the value of $p(z_0)$ by computing
$a_2 z^2, a_3 z^3, \ldots, a_n z^n$ in succession is inefficient.
The standard method for evaluating a polynomial is via \emph{Horner's algorithm},
which we now derive.

Observe that if $p$ is $n$th order, then $q(z) = [p(z) - p(z_0)]/(z-z_0)$
is $(n-1)$th order. Let \[p(x) = a_0 + a_1 z + \ldots + a_n z^n,
\quad q(x) = b_0 + b_1z + \ldots + b_{n-1} b^{n-1}.\] Then
writing $p(z) = (z- z_0)q(z) + p(z_0)$ and substituting, we obtain
\[a_0 + \ldots + a_n z^n = (z - z_0)(b_0 + \ldots + b_{n-1}z^{n-1}) + p(z_0),\]
which admits the following relations
\begin{align*}
& a_0 = p(z_0) - z_0b_0 \\
& a_1 = b_0 - z_0 b_1 \\
& a_2 = b_1 - z_0 b_2 \\
& \vdots \\
& a_{n-1} = b_{n-2} - z_0 b_{n-1} \\
& a_n = b_{n-1}.
\end{align*}
Rewriting this, we obtain
\begin{align*}
& p(z_0) = a_0  + z_0b_0 \\
& b_0 = a_1 + z_0 a_1 \\
& b_2 = a_2 -+z_0 a_2 \\
& \vdots \\
& b_{n-1} = a_{n-2} + z_0 a_{n-1} \\
& b_n = a_{n-1}
\end{align*}
Note that $b_n$ is known, since $a_{n-1}$ is known. Working backwards
from this line by line, we obtain $p(z_0)$. Notice that there is a total
of $n$ multiplications needed to do so. Contrast this with the
$1 + 2 + \ldots + n+1 = O(n^2)$ multiplications needed when 
evaluating $p(z_0)$ by computing $a_2 z^2, a_3 z^3$ and so on in succession.
\section{Interpolation}
While finding a least fit line to a series of data points
(see the least squares minimization problem in these notes)
makes sense if the data points do indeed have a linear
relation to one another, it is grossly inaccurate otherwise.
One way around this is to find nonlinear functions that
interpolate the data. If we assume these functions are analytic,
then we end up with a possibly infinite polynomial interpolating
the data in question. We now direct our attention to deriving
the appropriate interpolating polynomial, given a set of associations
$ \left\{ (x_i, y_i \right\}_{i = 0}^{n}$, where $f(x_i) = y_i$. 
We shall first establish a lemma.
\begin{lemma}[Vandermonde Determinant]
\label{vander}
Let 
\begin{equation*}
V_{n-k} \doteq  \begin{bmatrix}
1 & x_k & x_k^{2} & \ldots & x_k^{n} \\
1 & x_{k+1} & x_{k+1}^{2} & \ldots & x_{k+1}^{n} \\
\vdots \\
1 & x_n & x_n^{2} & \ldots & x_n^{n} \\
\end{bmatrix}
\end{equation*}
Then \[ \det V_{n-k} = \prod_{k < i < j < n}(x_i - x_j) \]
\end{lemma}
\begin{proof}
We first transform the first row and column to $e_1$ 
via row and column operations. More precisely, we multiply the $n$ column by $-x_0$ and then
add to the $n+1$ column, then multiply the $n-1$ column by $x_0$ and add
to the $n$ column, all the way down to the first column. This gives
\begin{equation*}
V_{n-k}' = \begin{bmatrix}
1 & 0 & 0 & 0 & \ldots  & 0 \\
1 & (x_{k+1} - x_k) & x_{k+1}(x_{k+1} - x_k) & x_{k+1}^2(x_{k+1} - x_k) & \ldots & x_{k+1}^{n-1}(x_{k+1} - x_k) \\
1 & (x_{k+2} - x_k) & x_{k+2}(x_{k+2} - x_k) & x_{k+2}^2(x_{k+2} - x_k) & \ldots & x_{k+2}^{n-1}(x_{k+2} - x_k) \\
\vdots \\
1 & (x_n - x_k) & x_n(x_n - x_k) & x_n^2(x_n - x_k) & \ldots & x_n^{n-1}(x_n - x_k) \\
\end{bmatrix}
\end{equation*}
Now, multiplying the first row by $-1$ and adding it to the remaining rows,
we obtain
\begin{equation*}
V_{n-k}' = \begin{bmatrix}
1 & 0 & 0 & 0 & \ldots & 0 \\
0 & (x_{k+1} - x_k) & x_{k+1}(x_{k+1} - x_k) & x_{k+1}^2(x_{k+1} - x_k) & \ldots & x_{k+1}^{n-1}(x_{k+1} - x_k) \\
0 & (x_{k+2} - x_k) & x_{k+2}(x_{k+2} - x_k) & x_{k+2}^2(x_{k+2} - x_k) & \ldots & x_{k+2}^{n-1}(x_{k+2} - x_k) \\
\vdots \\
0 & (x_n - x_k) & x_n(x_n - x_k) & x_n^2(x_n - x_k) & \ldots & x_n^{n-1}(x_n - x_k) \\
\end{bmatrix}
\end{equation*}
Observe that factoring $x_{k+i} -x_k$ from the $i+1$ row, for each $1 \le i \le n$, we obtain
\begin{equation*}
\begin{bmatrix}
1 & 0 \\
0 & V_{n-k-1}
\end{bmatrix}
\end{equation*}
Hence, the problem of computing the Vandermonde matrix determinant is inherently
recursive. Recall that multiplying rows and columns by non-zero constants and adding them to each
other does not impact the determinant of a matrix. However, if $A \mapsto A_c$
via multiplication of a row by some nonzero constant $c$, then
$|A_c| = c |A|$. Hence, from our above computations and a cofactor expansion
of $V_n'$, it follows that 
\begin{align*}
& |V_{n-k}| = \prod_{k < i \le n} (x_i - x_k) |V_{n-k-1}|\\
& | V_2 | = \left |
\begin{bmatrix}
1 & x_{n-1} \\
1 & x_{n}
\end{bmatrix}
\right | = x_n - x_{n-1}
\end{align*}
which gives
\begin{align*}
|V_{n-k}| & = \prod_{k=0}^{n-2} \prod_{1 \le i \le n} (x_i - x_k)\\
& = \prod_{k < i < j \le n} (x_i - x_k)
\end{align*}
completing the proof.
\end{proof}
\begin{theorem}
For an arbitrary data set $ \left\{ (x_i, y_i) \right\}_{i = 0}^{n}$ with the $x_i$
distinct, there
exists a unique polynomial $p$ of degree at most $n$ such that
\begin{equation*}
p(x_i) = y_i, \quad 0 \le i \le n
\end{equation*}
\end{theorem}
\begin{proof}
For the existence and uniqueness, we write an ansatz of degree at most $n$
\begin{equation*}
p(x) = a_0 + a_1x + a_2x^2 + \ldots + a_nx^n
\end{equation*}
and try to find the $a_i$. 
Our problem reduces to solving
\begin{equation*}
V_n a^{T} = y^{T}
\end{equation*}
where $\vec{a} = (a_0, a_1, \ldots, a_n)$, $\vec{y} = (y_0, y_1, \ldots, y_n)$,
and where $V_n$ is the $(n+1) \times (n+1)$ Vandermonde matrix
\begin{equation*}
\begin{bmatrix}
1 & x_0 & x_0^{2} & \ldots & x_0^{n} \\
1 & x_1 & x_1^{2} & \ldots & x_1^{n} \\
\vdots \\
1 & x_n & x_n^{2} & \ldots & x_n^{n} \\
\end{bmatrix}
\end{equation*}
which is invertible by Lemma \ref{vander}. Hence, the $a_i$ are uniquely determined.
\end{proof}
\begin{remark}
Observe that we can find infinitely many polynomials of degree $m > n$
interpolating a given data set $\left\{ (x_i, y_i) \right\}_{i=0}^n$. To see
this, note that we can simply extend this data set to $ \left\{ (x_1, y_i
\right\}_{i=0}^m$, where $(x_i, y_i)$, $n < i < m$ are chosen arbitrarily with
the $x_i$ distinct, and then compute the inverse of the resulting Vandermonde
matrix.
\end{remark}
\subsubsection{Newton Interpolation}
As we have seen, finding exact solutions to $n \times n$
systems of form $Ax = b$ takes $O(n^3)$ computations in general.
Our advantage for our linear system is that we are dealing with polynomials,
which have an incredible amount of structure. Hence, we are motivated
to utilize this to develop an $O(n)$ algorithm for finding the explicit
form of our polynomial. 
To do so, we implement an idea of Newton, rewriting our polynomial as
\begin{align*}
p(x) &= c_0 + c_1(x - x_0) + c_2(x - x_0)(x - x_1) + \ldots
+ c_{n}(x - x_0)(x - x_1)\cdots(x - x_{n-1})
\\
& = \sum_{i=0}^{n} c_i \prod_{j=0}^{i-1} (x - x_j), \quad \prod_{j=0}^{1} (x - x_j) \doteq 1.
\end{align*}
The $c_i$ can be computed in a naive fashion by plugging in $x_i$ 
and solving for $c_i$, for each $i$. Computation of $c_i$ will then depend
on computations of all $c_{< i}$. Let us formalize all this. 
\begin{definition}
Let $f$ be a function which we wish to approximate via a polynomial,
and $ \left\{ x_i, y_i \right\}_{i=0}^{n} $ a given data set with the
$x_i$ distinct. Let
\begin{equation*}
p(x) = c_0 + c_1(x - x_0) + c_2(x - x_0)(x - x_1) + \ldots
+ c_{n}(x - x_0)(x - x_1)\cdots(x - x_{n-1})
\end{equation*}
be the unique polynomial of at most degree $n$ interpolating $f$
on the given data set. We call
\begin{equation*}
f[x_0, x_1, \ldots, x_n] \doteq c_n
\end{equation*}
a \emph{divided difference} of $f$. It is the coefficient of the
highest degree term in the polynomial of at most degree $n$ interpolating the
given data set. 
\end{definition}
\begin{theorem}[Divided Differences]
We have the recursive formula
\begin{equation*}
f[x_0, x_1, x_2, \ldots, x_n] = \frac{f[x_0, x_1, \ldots, x_{n-1}] - f[x_1, x_2, \ldots, x_n]}{x_n - x_0}
\end{equation*}
\end{theorem}
\begin{proof}
Let $p_{0,n}$ denote the unique polynomial of degree at most $n$ that
interpolates $f$ at $x_0, x_1, \ldots, x_n$, and  We define
$p_{1,n}$ the unique polynomial of degree at most $n-1$ that interpolates
$f$ at $x_0, x_1, \ldots, x_n$. Then it is easy to check that
\begin{equation*}
p_{0,n} = p_{1,n} + \frac{x - x_n}{x_n - x_0}[p_{1,n} - p_{0,n-1}]
\end{equation*}
Observe that the coefficient of $x^n$ is given by the coefficient of $x_n$ in the expression 
\begin{equation*}
\frac{x(p_{1,n} - p_{0,n-1})}{x_n - x_0}
\end{equation*}
completing the proof.
\end{proof}
\subsection{Hermite}
Let $f(x) \in C^m(\rr)$ with
\begin{equation}
\label{hinterp-data}
\begin{split}
f^i(x_j) = b_{i j }, \qquad 0 \le j \le k, \ 0 \le i \le n_j
\end{split}
\end{equation}
where $b_{j k}$ is a constant. Can we find a polynomial $p(x)$ such that
\begin{equation}
\label{h-poly-data}
\begin{split}
p^i(x_j) = b_{i j}, \qquad 0 \le j \le k, \ 0 \le i \le n_j,
\end{split}
\end{equation}
holds? If so, we call $p(x)$ a \emph{Hermite polynomial}, and we say that $p(x)$
\emph{interpolates} $f(x)$ at the \emph{nodes} $x_k$.
\begin{theorem}
If $\sum_{j=0}^{k} (n_{j}+1) = m$, then there exists a unique polynomial of
degree less than or equal to $m$ satisfying \eqref{h-poly-data}.
\end{theorem}
\begin{proof} 
Write
\begin{equation*}
\begin{split}
p(x) = a_0 + a_{1}x + a_{2}x^2 +\cdots+ a_{m}x^m
\end{split}
\end{equation*}
where the constants $\{a_i\}_{0 \le i \le m}$ are to be determined. We wish to solve
the system given by
\begin{gather*}
a_{0}+a_{1}x_{j} + a_{2}x_{j}^2 +\cdots+a_{m}x_{j}^{m} = b_{0 j}
\\
a_{1} + 2 a_{2}x_{j} + 3a_{3}x_{j}^2 + \cdots+ m a_{m}x_{j}^{m-1} =
b_{1j}
\\
\vdots
\\
n_j! a_{ n_{j}} x_{j}^{m -n_{j}} +
(n_j + 1)! a_{n_{j}+1}
x_{j}^{m - n_{j}-1} + \cdots + \frac{m!}{(m-n_j)!} a_{m}x_{j} = b_{n_j j}
\end{gather*}
ranging over $0 \le j \le k$. In matrix form, this is expressed by $A\vec{a}=B$, where
\begin{gather*}
A= \begin{bmatrix}
A_1
\\
A_2
\\
\vdots
\\
A_k
\end{bmatrix}
, \ A_j=\begin{bmatrix}
1 & x_{j} & x_{j}^{2} & \cdots &\cdots &\cdots & \cdots &x_{j}^{m}\\
0 &1 &2x_{j} & \cdots &\cdots &\cdots &\cdots &mx_{j}^{m-1}\\
0 &0 &2 & \cdots &\cdots &\cdots &\cdots &m(m-1)x_{j}^{m-2}\\
\vdots & \vdots &\vdots &\ddots &\\
0 &0 &0 &\cdots & n_j! \  & (n_j + 1)! x_j & \cdots &\frac{m!}{(m-n_{j})!} x_j^{m -n_{j}}
\end{bmatrix},
\\
\vec{a}=\begin{bmatrix}
a_{0}\\
a_{2}\\
\vdots\\
a_{m}
\end{bmatrix}
\\
B = \begin{bmatrix}
B_1\\
B_2\\
\vdots \\
B_k
\end{bmatrix}
, \ B_j=
\begin{bmatrix}
b_{0j}\\
b_{1j}\\
\vdots \\
b_{n_j j}.
\end{bmatrix}
\end{gather*}
Note that $A_j$ is a $(n_j +1) \times (m+1)$ matrix. Hence, for $A$ to be a 
square matrix, we must have $\sum_{j = 0}^k (n_j+1) = m$.

Therefore, assuming this condition, to complete
the proof it will be enough to show that $A$ is invertible, or, equivalently,
that if $A \vec{a}=\vec{0}$, then $\vec{a} = \vec{0}$. Proceeding, suppose
$A \vec{a}=\vec{0}$. Then $\vec{a}$ defines a polynomial $p(x)$ with root $x_j$ of
multiplicity $n_j +1$. Hence,
\begin{equation*}
\begin{split}
p(x) = q(x) \prod_{j=0}^{k}(x-x_{j})^{n_{j}+1}.
\end{split}
\end{equation*}
However, note that $\prod_{j=0}^{k}(x-x_{j})^{n_{j}+1}$ is of order $m+1$,
whereas the coefficients of $\vec{a}$ define $p(x)$ to be of order at most $m$.
It follows that $q \equiv 0$. 
\end{proof}
\begin{remark}
Observe that if $\sum_{j=0}^{k}(n_j + 1) < m$, then the system is underdetermined.
It is not too hard to show that we will have infinitely many solutions. That is, there will be infinitely
many polynomials of degree $m$ which interpolate the given data.
\end{remark}
\subsection{Cubic Splines}
The problem with Newton or Hermite interpolation is that if we have
many data points, our interpolating polynomial $p$ will have degree $n \gg 1$.
Evaluating this polynomial at a point $x_0$ will then take $O(n)$ computations.
To sidestep this problem, we seek to divide our data up into pieces
and to interpolate each piece individually via a cubic polynomial.
Once we have done this, we will then stitch together.
\begin{definition}
A spline on an interval $[a,b]$ are polynomial pieces defined on 
disjoint sub-intervals $[a_j,b_j]$ which cover $[a,b]$. 
\end{definition}
Let $S$ be the cubic spline that we seek to construct. We make
the following assumptions.
\begin{enumerate}
\item On each interval $[t_{i-1}, t_i]$, $S$ is a polynomial of degree
at most $3$.
\item $S$ is $C^2$ on the boundary of each sub-interval.
\end{enumerate}
By the first assumption, $S''$ is linear on each sub-interval.
Hence, if $S''_i(t_i) = z_i$ and $S''_{i}(t_{i+1}) = z_{i+1}$,
then
\begin{align*}
S_{i}''(x) = \frac{z_i}{h_i}(t_{i+1} - x) + \frac{z_{i+1}}{h_i}(x- t_i),
\quad h_i \doteq
t_{i+1}- t_i	
\end{align*}
Integrating this, we obtain an at most cubic polynomial with  two unknown
constants of integration $C, D$, which must be found along with the $z_i$.
However, our regularity assumptions on the spline are powerful enough to allow
us to do so.
\section{Finite Difference Method}
\subsection{Burgers Equation}
Recall the Burgers initial value problem (ivp)
\begin{gather}
\label{burgers-eqn}
u_{t} = \frac{1}{2}(u^{2})_{x},
\\
\label{burgers-init-data}
u(x, 0) = u_{0}(x), \qquad x, t \in \rr.
\end{gather}
Note that for small \emph{stepsize} (or \emph{mesh}) $h, k >0$,
\begin{equation*}
\begin{split}
u_{t}(x,t) &\approx \frac{u(x, t+k) - u(x, t)}{k},
\\
u_{x}(x, t) & \approx \frac{u(x+h, t) - u(x, t)}{h}.
\end{split}
\end{equation*}
Set
\begin{gather*}
x_i = ih, \quad i \in \mathbb{Z}
\\
t_{j}=jk, \quad j \in \mathbb{N}
\end{gather*}
and let
\begin{gather*}
u_{i,j} = u(x_{i}, t_{j}).
\end{gather*}
Then the discretized Burgers ivp takes the form
\begin{gather*}
\frac{u_{i, j+1}- u_{i,j}}{k}=\frac{1}{2h}\left( u_{i+1,j}^{2} -
u_{i,j}^{2} \right),
\\
u_{i,0} = u_{0}(ih)
\end{gather*}
or
\begin{gather}
\label{burgers-discrete}
u_{i, j+1}=u_{i,j} + \frac{k}{2h}\left( u_{i+1,j}^{2} -
u_{i,j}^{2} \right),
\\
\label{burgers-discrete-init}
u_{i,0} = u_{0}(ih).
\end{gather}
Note that \eqref{burgers-discrete}-\eqref{burgers-discrete-init} gives us an
explicit numerical solution to the Burgers ivp
\eqref{burgers-eqn}-\eqref{burgers-init-data}. To illustrate this, we set
$j=0$, and obtain
\begin{equation}
\label{case-j=0}
\begin{split}
u_{i,1} = \underbrace{u_{i,0} + \frac{k}{2h}\left( u_{i+1,0}^{2} -
u_{i,0}^{2} \right)}_{\text{known from initial data
\eqref{burgers-discrete-init}}}.
\end{split}
\end{equation}
Similarly, for $j=1$, we have
\begin{equation*}
\begin{split}
u_{i,2} = \underbrace{u_{i,1}}_{\text{known from
\eqref{case-j=0}}} + \frac{k}{2h}\left(
\underbrace{u_{i+1,1}^{2}}_{\text{known from \eqref{case-j=0}}} -
\underbrace{u_{i,1}^{2}}_{\text{known from
\eqref{case-j=0}}} \right).
\end{split}
\end{equation*} 
This process can be
continued indefinitely to find $u_{i, j}$ for any $j \in \mathbb{N}$, and is
called an \emph{explicit finite difference method} for numerically solving the
Burgers ivp. An important drawback to this method is that it converges very
slowly to solutions of the Burgers ivp.
\section{Orders of Convergence}
We'd like to understand how fast a sequence convergence to its limit, without
having to worry about silly constants.
\begin{definition}
Let $\{x_{n}\} \in \mathbb{C}$ be a sequence converging to $x$. We say that
$x_{n} = O(\alpha_{n})$ if $|x_{n}| \le C |\alpha_{n}|$  for $n \ge N$, and $ x_n 
= o(\alpha_n )$ if $|x_n| \le \ee_n |\alpha_n |$, where $\ee_n \to 0$.
Similarly, for functions $f,g: \cc \to \cc$, we say that $f = O(g)$ in a
neighborhood $N$ of $x$ if $|f(x_n) | \le C |g(x_n)|$ for all $x_n \in N$, and 
$f = o(g)$ in a neighborhood $N$ of $x$ if $|f(x_n) | \le \ee_n |g(x_n)|$,
where $\ee_n \to 0$,  for all $x_n \in N$.
\end{definition}

\begin{example}
\begin{equation*}
\frac{n+1}{n^2} \le \frac{2n}{n^2} = \frac{2}{n} = O\left(\frac{1}{n} \right)	
\end{equation*}
\end{example}
\begin{example}
\begin{equation*}
\frac{1}{n \log n} \le \frac{\ee_n}{n} = o\left(\frac{1}{n}\right).
\end{equation*}
\end{example}

\begin{example}
\begin{equation*}
\frac{5}{n} + e^{-n} = O\left( \frac{1}{n}\right)
\end{equation*}
\end{example}

\begin{remark}
It is a true statement that $5/n^4 = O(1/n)$. However, this grossly
misrepresents the asymptotic behavior of the sequence $5/n^4$. Typically, when
we discuss the order of convergence of a sequence $x_n$ to its limit, we seek
the sharpest order $\alpha_n$ of convergence. By sharp, we mean that if $|x_n| =
O(\alpha_n)$, then $|x_n| \neq O(\alpha_n n^{-\ee})$, for any $\ee > 0$.
\end{remark}

\begin{example}
Using Taylor series,
\begin{equation*}
e^x - \sum_{k = 0}^{n-1} \frac{1}{k!}x^k = \frac{1}{n!} f^{n+1}(\xi)x^n =
O\left(\frac{1}{n!}\right).
\end{equation*}
\end{example}

\section{Linear Difference Equations and Operators}

Algorithms emit sequences. If we can detect a pattern between successive
iterates that is ``linear'' in nature, we can use powerful tools from linear
algebra to study the computational complexity and convergence properties of our
algorithm.
\begin{definition}
Denote $V$ to be the set of all infinite complex sequences
\begin{equation*}
\vec{x} = [x_1, x_2, \ldots, x_n, \ldots]
\end{equation*}
equipped with addition
\begin{equation*}
\vec{x} + \vec{y} = [x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n, \ldots]	
\end{equation*}
scalar multiplication
\begin{equation*}
\lambda \vec{x} = [\lambda x_1, \lambda x_2, \ldots, \lambda x_n, \ldots].
\end{equation*}
and norm
\begin{equation*}
\| \vec{x} \| = \sup \{|x_i| \}
\end{equation*}
It is not hard to see that $V$ is an separable infinite-dimensional vector
space.
\end{definition}
We wish to study the vector space $\mathcal{L}(V)$ of bounded linear operators
$T:
V \to V$, equipped with the standard definitions for addition, scalar
multiplication, and operator norm.
In particular, we are interested in the shift operator
\begin{align*}
& E: V \to V,
\\
& E \vec{x} = [x_2, x_3, \ldots, x_{n+1}, \ldots].
\end{align*}
which has the property 
\begin{align*}
& (E^k x)_{n} = x_{n+k}.
\end{align*}
\begin{definition}
Observe that the set of shift operators ${E_k}$ form a basis for a
linear subspace of $\mathcal{L}(V)$. We call this the space of
\emph{linear difference operators}, and denote it by $\mathcal{D}(V)$.
\end{definition}
\begin{definition}
Consider an element $T = \sum_{i=0}^{m}c_i E^i \doteq p(E)$ of $D(V).$ We call
$p(x) = \sum_{i=0}^{m} c_i x^i$ the \emph{characteristic polynomial} of $T$.
\end{definition}
\begin{remark}
Studying a linear difference equation is equivalent to studying the nullspace
of some operator $T \in D(V)$. Since $V$ is separable, we can think of
$\mathcal{D}(V)$ heuristically as nothing more than the space of $\mathbb{N}
\times \mathbb{N}$ matrices. 
\end{remark}
\begin{example}
\begin{equation*}
x_{n+2} - 3x_{n+1} + 2x_{n} = 0 \longleftrightarrow (E^2 - 3E^1 + 2)\vec{x} = 0
\longleftrightarrow p(E) \vec{x} = 0, \quad p(\lambda) = \lambda^2 - 3 \lambda
+2.
\end{equation*}
\end{example}
The tools from finite-dimensional matrix algebra extend to the
countable-dimension case. More precisely, we can categorize the solutions of the
of certain finite difference problem entirely in terms of eigenvectors and
eigenvalues.
\begin{theorem}
\label{thm:dif-eq}
Let $p(E) \vec{x} = 0$ be a linear difference equation. 
\begin{enumerate}
  \item 
If $\lambda$ is a root
to its characteristic polynomial, then $\vec{u} = [\lambda, \lambda^2,
\lambda^3, \ldots]$ is a solution to $p(E) \vec{x} = 0$. 
  \item 
If all roots $\lambda_k$ of $p$ have multiplicity $1$ and are nonzero, then
$ \bigcup_k \{ [\lambda_k, \lambda_k^2, \ldots] \}$ is a 
a basis for the solution space of $p(E) \vec{x} = 0$.	
\item If all roots $\lambda_k$ of $p$ are nonzero, and have multiplicity $k_n$
greater than or equal to $1$, then $\bigcup_k \left\{ [\lambda_k, \lambda_k^2,
\ldots], [\lambda_k, \lambda_k^2, \ldots]', \ldots, [\lambda_k, \lambda_k^2,
\ldots ]^{(k_n -1)} \right\}$ is a basis for the solution space of $p(E)
\vec{x} = 0$.
\end{enumerate}
\end{theorem}
\begin{proof} We split the proof into two parts.
\begin{enumerate}
  \item 
  We first show that $\vec{u}$ is an eigenvector of $p(E)$ with corresponding
  eigenvalue $p(\lambda)$. Observe that
  \begin{align*}
p(E) \vec{u} = \left(\sum_{i=0}^m c_i E^i \right) \vec{u}
& = \sum_{ i = 0}^m c_i \left( E^i \vec{u}\right)
\\
& = \sum_{i = 0}^m c_i [\lambda^{i+1}, \lambda^{i+2}, \ldots]
\\
& = \sum_{i = 0}^m c_i \lambda^i \vec{u}
\\
& = p(\lambda) \vec{u}.	 
\end{align*}
Hence, if $p(\lambda) = 0$, $\vec{u}$ is a solution.
\item
\label{simple}
For each root $\lambda_k$, consider the corresponding solution $u^{(k)} =
[\lambda_k, \lambda_k^2, \ldots]$. Let $\vec{x}$ be an arbitrary solution to our
linear finite difference problem. We want to show that $\vec{x} = \sum_{k=1}^m
a_k u^{(k)}$. We can write
the first $m$ components of $\vec{x} = [x_1, x_2, \ldots]$ as 
\begin{equation}
\label{det}
x_i = \sum_{k=1}^{m} a_k \lambda_k^i, \quad 1 \le i \le m.
\end{equation}
Writing this in matrix form, we obtain
\begin{equation*}
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_m
\end{bmatrix}
= 
\underbrace{
\begin{bmatrix}
\lambda_1 & \lambda_2 & \ldots & \lambda_m \\
\lambda_1^2 & \lambda_2^2 & \ldots & \lambda_m^2 \\
\vdots & \vdots & \ddots & \vdots \\
\lambda_1^m & \lambda_2^m & \ldots & \lambda_m^m 
\end{bmatrix}
}_\text{A}
\begin{bmatrix}
a_1 \\ a_2 \\ \vdots \\ a_m
\end{bmatrix}
\end{equation*}
We claim that the columns of $A$ are linearly
independent. To see this, we wish to show that the determinant of $A$ is
non-zero, via induction. For the base case, we consider the $1 \times 1$
sub-matrix $[\lambda_m^m]$, which has non-zero determinant (we assumed
a priori that the $\lambda_i$ are non-zero). For the inductive step,
we assume the sub-matrix with diagonal entries $\lambda_2^2$ to $\lambda_m^m$ has non-zero
determinant. Then it is easy to see that $A$ has non-zero determinant by taking
its principal minor at $\lambda_1$. 
Since A is a map from $\rr^m$ to itself, it follows that dimension of the nullspace of $A$ is $0$. Hence, $A$ is
non-singular, and so completely determines the $a_i$, for $1 \le i \le m$. 
As a corollary of this argument, we obtain that the vectors $u^{(k)}$ are linearly
independent. We now repeat the argument to obtain the next $m$ constants $a_i$. A simple 
induction completes the proof. 
\item
In the case where roots of multiplicity greater than $1$, we have more
difficulties: if we construct a matrix $A$ as in the proof of
Theorem \ref{thm:dif-eq}, it will be singular (we will have columns that are
identical). Hence, we must somehow construct new solutions out of roots with
multiplicity greater than $1$. We do so as
follows. Assume $\lambda$ has multiplicity $n$. Then for $k \le n-1$
\begin{align*}
p(E)x^{(k)}(\lambda)
& = [ p(E)x(\lambda) ]^{k} 
\\
& = [ p(\lambda)x(\lambda) ]^{k} 
\\
& = \sum_{i = 0}^{k} \binom{k}{i}p^{(k-i)}(\lambda) x^{(i)}(\lambda) = 0.
\end{align*}
Assume the set $ \left\{ x(\lambda)^{(k)} \right\}_{k=0}^{n-1} $ is not linearly
independent.
Then
\begin{equation*}
x^{(n-1)}(\lambda) = \sum_{i=0}^{n-2} c_i x^{(i)}(\lambda)
\end{equation*}
where some $c_i \neq 0$. By differentiation
\begin{equation*}
x^{(n)}(\lambda) = \sum_{i=0}^{n-1} c_i x^{(i)}(\lambda)
\end{equation*}
and hence
\begin{equation*}
p(E)x^{(n)}(\lambda) = 0
\end{equation*}
which implies $x(\lambda)$ has multiplicity greater than $n$, which is
a contradiction. Lastly, we run an argument similar to that in \eqref{simple}
to complete the proof, with the observation that we can now fill the
``redundant'' columns generated by non-simple roots 
with the new vectors we construct out of their derivatives.
\end{enumerate}
\end{proof}
Lastly, we would like to identify whether or not solutions  to a finite
difference problem have the potential to blow up.
\begin{definition}
We say $x \in [x_1, x_2, \ldots, x_n, \ldots]$ is \emph{bounded}
if $\sup_n | x_n | < \infty$. A difference equation $p(E)x = 0$ is 
\emph{stable} if all its solutions are bounded.
\end{definition}
\begin{theorem}
For a polynomial $p$ with $p(0) = 0$, $p(E)x = 0$ is stable if and only if
all simple roots of $p$ satisfy $|\lambda| \le 1$, and all non-simple roots
satisfy $|\lambda| < 1$. 
\end{theorem}
\begin{proof}
To prove necessity, we observe that if $|\lambda| > 1$ for some simple root, or
$|\lambda| \ge 1$ for some non-simple root, then $[\lambda, \lambda^2, \lambda^3,
\ldots]$ or $[\lambda, \lambda^2, \lambda^3, \ldots]'$ are unbounded, respectively.
To prove sufficiency, we observe that
\begin{equation*}
[\lambda, \lambda^2, \lambda^3,
\ldots]^{(n)} = \left [ \frac{n!}{(n - k)!} \lambda^{n - k} \right ]_{n =
k}^{\infty} 
\end{equation*}
is bounded for all $n \ge 0$ if $|\lambda| < 1$. This follows immediately from
the estimate 
\begin{equation*}
\frac{n!}{(n - k)!} \lambda^{n - k}  \le n^{k} \lambda^{n-k} \to 0 \ \text{as} \
n \to \infty
\end{equation*}
where the right hand side decays to $0$ as $n \to \infty$ if $|\lambda| < 1$. Lastly,
every  solution to $p(E)x = 0$ is a finite linear combination of vectors of form
$[\lambda, \lambda^2, \lambda^3, \ldots]^{(n)}$ by Theorem \ref{thm:dif-eq}.
\end{proof}
\begin{example}
The difference equation $4x_n + 7x_{n-1} + 2 x_{n-2} - x_{n-3} = 0$ is
unstable, because its characteristic polynomial 
$4\lambda^{3} + 7\lambda^{2} + 2 \lambda - 1$ has a root $\lambda= -1$  
of multiplicity $2$.
\end{example}
\section{Least Squares Problem}
\subsection{Orthogonal Transformations and Hilbert Spaces}
We recall that linear transformations on finite-dimensional spaces are a combination
of scalings, translations, and rotations. Consequently, since matrices are linear transformations,
we would like to factor our matrices into a product of rotation matrices, and scaling matrices.
However, we must first formalize our notion of rotation. 
\begin{definition}
A Hilbert Space $V$ is a vector space equipped with a conjugate bilinear form 
$ \langle \cdot, \cdot \rangle : H \times H \to \cc $. More precisely, we have
\begin{enumerate}[a)]
\item{Symmetry:} $\langle x,y \rangle  = \overline{ \langle y,x \rangle   }$.
\item{Linearity:} $\langle \lambda x + y, z \rangle  =
\lambda \langle x, z \rangle  + \langle y, z \rangle$.
\item{Reflexivity:} $\langle x, x \rangle  $ is real and positive, 
and $\langle x, x \rangle = 0 $ if and only if $x = 0$.            
\end{enumerate}
\end{definition}
It is easy to check that the relation
\begin{equation*}
\| v \| = \sqrt{\langle v, v \rangle}
\end{equation*}
defines a norm on any inner product space. If $ \langle x, y \rangle $ is real
valued, we can think of it as the size of the angle between $x$
and $y$. Indeed, in $\rr^2$ equipped with the standard euclidean inner product,
one can check that
\begin{equation*}
\langle x, y \rangle  = \|x\| \|y\|\cos \theta
\end{equation*}
Hilbert spaces have other important properties. We shall utilize the next one 
frequently. 

\begin{theorem}
Let $H$ be a Hilbert space, and $A : H \to H$ a bounded linear operator. Then
there exists a map $A^{\star} : H \to H$ with the property that
\begin{equation*}
\langle Au, v \rangle  = \langle u, A^{\star}v \rangle 
\end{equation*}
for any $u, v \in H$. We call $A^{\star}$ the \emph{adjoint} of $A$.
\end{theorem}
\subsection{Simplification of Least Squares Problem via Adjoints} 
Ideally, we wish to find exact solutions to the linear problem $Ax = b$, where
$A$ is an $m \times n$ complex matrix, $x$ is an $n \times 1$ column vector, and
$b$ is an $m \times 1$ column vector. However, this is not always possible. For
example, if $\text{rank A} < m$, then the column space of $A$ may not span $b$.
In these situations, we look for $\tilde{x}$ such that $ \| b - A\tilde{x}\|$ is
minimized. The choice of norm is irrelevant in finite-dimensional vector spaces (a
simple but useful corollary of Theorem \ref{thm:adj-ls}),
though the Euclidean norm ($\ell^2$) is a powerful choice, since we can then
equip our normed space with the inner product
\begin{equation*}
\begin{split}
& \langle u, v \rangle \doteq \sum_i u_i \bar{v_i}
\\
&  \langle u, u \rangle =\| u \|_{\ell^2}^2.
\end{split}
\end{equation*}
Recall that operators on inner-product spaces always posses an adjoint. We now
make use of this important property to simplify our minimization problem
considerably.
\begin{theorem}
\label{thm:adj-ls}
Let $A$ be an $m \times n$ complex matrix with $\text{rank}(A) = n$. 
If $x$ is a point such that $A^{\star} (Ax - b) = 0$, then $x$ is the unique solution 
to the least squares problem.
\end{theorem} 
\begin{proof}
Assume $A^{\star}(Ax-b) = 0$. We first prove that $x$ must be a least squares solution. Observe that
 $\overline{A^{\star}(Ax-b)} = 0$, or $A^T
\overline{Ax-b} = 0$. Hence, $Ax-b$ is orthogonal to the column space of $A$. 
Let $y$ be any other point. Since $A(x-y)$ lives in the column space of $A$
, we see that $\langle Ax-b, A(x-y) \rangle = 0$. Hence, by the Pythagorean theorem,
\begin{equation*}
\| Ax - b \|_{\ell^2}^2 = \| Ay - b + A(x-y)
\|_{\ell^2}^2 = \| Ay - b \|_{\ell^2}^2 + \| A(x-y) \|_{\ell^2}^2 \ge \| Ay - b
\|_{\ell^2}^2.
\end{equation*}
Hence, $x$ is a least squares solution. Assume $y$ is a least squares solution. Then by our previous work, 
we must have $ \| A(x - y) \|_{\ell^2} \neq 0$. This implies $x-y$ lives in the nullspace of $A$.
But since $\text{rank}(A) = n$, by the rank-nullity theorem, the dimension of the nullspace 
is $0$. We conclude that $x = y$. 
\end{proof}
Observe that if $A$ is unitary, we obtain $x$ immediately:
\begin{equation*}
0 = A^{\star} (Ax-b) = x - A^{\star} b  \implies x = A^{\star} b.
\end{equation*}
This is a simplistic example that illustrates that unitary matrices simplify our least squares
computations considerably. Hence, it makes sense to look at decompositions of $A$ 
utilizing unitary matrices, in an attempt to simplify equations of form $A^{\star}(Ax-b = 0)$ 
even further.
More generally, suppose we can find a decomposition $A = QR$, where $Q$ is $m \times m$
unitary, and $R$ is $m \times n$ upper-triangular. Then
\begin{equation*}
0 = A^{\star}(Ax-b) = R^{\star } Q^{\star}(Ax-b) = R^{\star} Q^{\star}(QRx - b) \implies R^{\star}Rx = R^{\star}Q^{\star} b.
\end{equation*}
Hence, the $QR$ decomposition permits us to convert our problem of finding a least square
into the equivalent problem of finding a solution $x$ to a problem of $LU$ type, which is much
more tractable.

We now discuss the two most popular $QR$ decomposition algorithms.
\subsubsection{Gram-Schmidt}
\begin{theorem}
Let $\{x_1, x_2, \ldots, x_n\}$ be a linearly independent in $C^n$. Then the set
 \\ $ \left\{ u_1, u_2, \ldots, u_n \right\} $ is an orthonormal basis for $C^n$,
where
\begin{equation*}
u_k = \frac{x_k - \sum_{i < k } \langle x_k, u_i \rangle u_i}{\|x_k -
\sum_{i < k } \langle x_k, u_i \rangle u_i\|_{\ell^2}}
\end{equation*}
\end{theorem}
\begin{proof}
It is a straightforward computation of $ \langle u_i, u_j \rangle $.
\end{proof}
Applying the Gram-Schmidt process to the columns of a matrix  $ A $, we obtain
the decomposition $ BT $, where $ B $ is an $ m \times n $ matrix consisting of
the orthonormalized column vectors of $ A $, and $ T $ consists of an $ n \times
n $ upper triangular matrix with positive diagonal, whose entries come from
saved computations in the Gram-Schmidt algorithm.

Substituting this decomposition into $A^{\star}(Ax-b) = 0$, we 
\begin{equation*}
T^{\star}Tx = T^{\star}B^{\star}b
\end{equation*}
which is again a problem of $LU$ type. 

\begin{remark}
Instead of resorting to Gram-Schmidt or some other factorization, we could
attempt to solve $ A^{\star}(Ax - b) = 0 $ directly. That is, we attempt to
solve $ A^{\star}Ax = A^{\star}b $ via a Cholesky factorization ($ A^{\star}A$
is Hermitian and positive semidefinite). The problem with this approach is that
we may have $\kappa(A^{\star}A) >> k(A)$. A good example of this is 
\begin{equation*}
A = 
\begin{bmatrix}
 1 & 1 & 1 \\
 \ee & 0 & 0 \\
 0 & \ee & 0 \\
 0 & 0 & \ee 
\end{bmatrix}
\end{equation*}
\end{remark}
\subsubsection{Householder}
A Householder decomposition is of form $A = QR$, where $Q$ is a $m \times m$ unitary matrix, and
$R$ is an $m \times n$ upper triangular matrix. Observe that the columns of a unitary matrix
are orthonormal (they have other useful properties), so this decomposition has better properties
in general than the Gram-Schmidt decomposition. Indeed, substituting into $A^{\star}(Ax-b)$, we 
obtain the $LU$ type problem
\begin{equation*}
R^{\star}R x = (QR)^{\star} b.
\end{equation*}
\subsubsection{Singular Value Decompositions}
We now discuss arguably the most useful decomposition of an arbitrary $m \times
n$ matrix. We first require a lemma.
\begin{lemma}
\label{lem:hermite}
A Hermitian, positive semidefinite matrix has real positive eigenvalues.
\end{lemma}
\begin{proof}
We first show that any Hermitian matrix has real eigenvalues. Suppose $A$ is Hermitian.
Then
\begin{equation*}
v^{\star} A^{\star} v = (A v)^{\star} v = \bar{\lambda} \| v \|_{\ell^2}^{2}
\end{equation*}
and
\begin{equation*}
 v^{\star} A v = v^{\star} \lambda v =  \lambda \| v \|_{\ell^2}^{2}
 \end{equation*}
 Since $A$ is Hermitian, $A = A^{\star}$, so we must have $\bar{\lambda} =
\lambda$, which implies that $\lambda$ is real. Furthermore, Hermitian matrices
are not degenerate. That is, an $n \times n$ Hermitian matrix has n distinct
eigenvectors, since any $n \times n$ Hermitian matrix is similar to a diagonal matrix (which
has eigenvectors $e_1, e_2, \ldots, e_n$, where the $e_i$ are the standard 
canonical basis vectors for $C^n$). 
\end{proof}
\begin{theorem}[Singular Value Decomposition]
Let $A$ be an arbitrary $m \times n$ matrix with complex entries.
Then there exists a decomposition $A = PDQ$, where $P$ is $m \times m$ unitary,
$D$ is a $m \times n$ diagonal matrix, and $Q$ is a $n \times n$ unitary matrix.
This decomposition is not necessarily unique.
\end{theorem} 
\begin{proof}
The non-uniqueness of the $PDQ$ decomposition is easy: the $0$ matrix has multiple $PDQ$ type decompositions (just take $D$ = 0, and $P, Q$ to be
arbitrary unitary matrices).
To prove existence of the $PDQ$ decomposition for arbitrary $m \times n$ matrices $A$, we
first assume without loss of generality that $m \ge n$ (otherwise, we consider $A^T$).
By Lemma \ref{lem:hermite}, for each of $n$ linearly independent eigenvectors
$u_i$ of $A^{\star}A$, there exist associated positive eigenvalues $\sigma_i^2$.
Furthermore,
\begin{equation}
\label{u-trans}
\| A u_i \|_{\ell^2}^2 = \langle Au_i, Au_i \rangle  = \langle u_i, A^{\star}A
u_i \rangle = \sigma_i^{2} u_i
\end{equation}
We can also choose these eigenvectors such that they are orthonormal. To see
this, recall that $A^{\star}A$ is Hermitian, and that Hermitian matrices are
similar to diagonal matrices. Since square diagonal matrices have eigenvectors
$e_i$, the eigenvectors of a Hermitian matrix are given by a unitary transformation
of the $e_i$, which preserves orthonormality since 
\begin{equation*}
\langle Q u_i, Qu_j \rangle  = \langle u_i, Q^{\star}Q u_j \rangle  = \langle
u_i, u_j \rangle  = \delta_{ij}.
\end{equation*}
Hence, we have the decomposition $A^{\star}A = \tilde{D}^{2}Q$ where $\tilde{D}$ is an $n \times n$
matrix consisting of the positive square roots of eigenvalues $\sigma_i^2$ of
$A^{\star}A$, and $Q$ is the $n \times n$ unitary matrix with rows
$u_i^{\star}$. 
 
We now massage this decomposition to come up with $PDQ$. In effect, we seek to
factor $A^{\star}\tilde{D} = PD$, where $D$ is a $m \times n$ diagonal matrix
with diagonal consisting of the $ |\sigma_i|$, and $P$ is $m \times m$ unitary. 
Assume non-zero eigenvalues for $i \le r$, where $r \le n$, and zero otherwise.
Then by \eqref{u-trans}, $A u_i \neq 0$ for $i \le r$, and $A u_i = 0$ otherwise.
Set $v_i = Au_i/\sigma_i, 1 \le i \le r$. It is easy to check that these $v_i$ are orthonormal.
Using Gram-Schmidt, extend the set ${v_1, \ldots, v_r}$ to an orthonormal set
$\{v_1, \ldots, v_m \}$, and set $P$ to be the matrix with the $v_i$ on its columns.
Then one can verify that $A = PDQ$. Notice that this decomposition is not unique: it depends on how we
arrange our orthonormal vectors in $P$ and $Q$. 
\end{proof}
Substituting a singular value decomposition into our minimization result $A^{\star}(Ax - b) = 0$, 
we obtain
\begin{equation*}
 0 = Q^{\star} D P^{\star}(PDQx - b) = Q^{\star} D^{2} Qx - Q^{\star}D P^{\star}b \implies D^{2}Qx = DP^{\star}b
\end{equation*}
Let $D^{+}$ be the matrix with $1/\sigma_i$ in its diagonal
for $\sigma_i \neq$, and $0's$ otherwise. This is what's known as a \emph{pseudo-inverse} of $D$. 
Multiplying both sides of the last expression by $Q^{\star}(D^{+})^{2}$, we obtain
\begin{equation*}
x =Q^{\star} D^{+} P^{\star} b  
\end{equation*}
Hence, we have proved the following result. 

\begin{theorem}
Let $Ax = b$ be a system to which we seek least-squares solution, where
$A$ has singular value decomposition $PDQ$.
Then the solution is given by $x = A^{+}b$, where $A^{+} = Q^{\star} D^{+} P^{\star}$
\end{theorem}
This motivates the following definition.
\begin{definition}
Let $A$ be an arbitrary $m \times n$ matrix with singular value decomposition $PDQ$. 
Then we call $A^{+} = Q^{\star}D^{+} P^{\star}$ its \emph{pseudoinverse}.
\end{definition}
It can be shown that even though singular value decompositions for a matrix $A$ are not
unique, the pseudoinverse $A^{+}$ is. 
\section{Finding Eigenvectors and Eigenvalues}
Having seen the importance of the singular value decomposition in the preceding
section, we would like to develop an algorithm for finding the eigenvalues and
corresponding eigenvectors of $A^{\star}A$ for any given matrix $A$. From there,
we saw in the last section that we can orthonormalize the set of eigenvectors
via Gram-Schmidt and, if necessary, extend this set to form a basis for $C^{n}$.
Once we do that, our $PDQ$ decomposition will be complete.
\subsection{The Power Method}
We now develop an algorithm for finding the eigenvalue of maximum modulus 
for a non-zero $n \times n$ matrix $A$
and its corresponding eigenvector. We require that there is a unique
eigenvalue of maximum modulus, and that $A$ be diagonalizable. 
Proceeding, suppose $\lambda_1$ is the eigenvalue of maximum modulus.
If $\lambda_1 = 0$, then $A = 0$, contradicting our earlier assumption. 
Hence, $\lambda_1 \neq 0$, with corresponding eigenvector $u_1$.
Choose $x^{(0)} \in \cc^n$ that is $n$-dimensional. Then we must have
\begin{equation*}
x^{(0)} = \sum_{i=1}^n a_i u_i, \quad a_i \neq 0 \ \forall i
\end{equation*}
It follows that
\begin{align*}
x^{(k)} 
= A^{k}x^{(0)}
= \sum_{i=1}^n a_i \lambda_i^{k} u_i 
= \lambda_1^{k}\sum_{i=1}^n a_i (\lambda_i/\lambda_1)^k u_i = a_1 \lambda_1^k u_1 + \ee_k
\end{align*}
Let $\phi: \cc^n \to \cc$ be the projection function of an input onto
its first coordinate. Then
\begin{equation*}
\phi(x^{(k)})/ \phi(x^{(k-1)}) \to \lambda_1
\end{equation*}
from which we obtain 
\begin{equation*}
x^{(k)}/\lambda_1^k \to a_1 u_1
\end{equation*}
which is the corresponding eigenvector. 
From a theoretical standpoint, using this for finding the corresponding
eigenvector is perfectly fine. However, numerically it is a bit imprecise,
since we are using an approximation to $\lambda_1$ (due to machine rounding
and termination of our algorithm after a pre-specified number of iterations)
and then taking powers of it to compute our eigenvector. There is a risk that 
this will compound the errors already present, introducing complexity
to our problem. There is a simple way to bypass this: 
we can normalize $x^{(k)} \to x^{(k)}/\| x^{(k)} \| \doteq
\tilde{x}^{(k)}$ at each step, and still obtain
\begin{equation*}
\phi(\tilde{x}^{(k)})/ \phi(\tilde{x}^{(k-1)}) \to \lambda_1. 
\end{equation*}
However, now we also have 
\begin{equation*}
\tilde{x}^{(k)} = \frac{a_1 \lambda_1^k u_1 + \ee_k}{\| a_1 \lambda_1^k u_1 + \ee_k\|}
\to u_1/\| u_1 \|
\end{equation*}
which is an eigenvector corresponding to the eigenvalue $\lambda_1$.
\subsection{Inverse Power Method}
This is almost identical to the Power method, with the additionally caveat that
we require $A$ to be invertible. We observe that for square matrices, $Av =
\lambda v$ implies $A^{-1}v \lambda^{-1}v$ so long as $A$ is invertible or,
equivalently, that $\lambda \neq 0$. Hence, if $\lambda$ is the unique
eigenvalue of minimum modulus of a diagonalizable matrix $A$, and it is the
unique eigenvalue of maximum modulus of the diagonalizable matrix $A^{-1}$.
Hence, we can apply the power method on $A^{-1}$ to find $\lambda^{-1}$, from
which we recover $\lambda$. In practice, it is not computationally efficient to
compute $A^{-1}$ and then run the power method. Rather, we 
rewrite $x^{(k)} = (A^{-1})^k x^{(0)}$ in equivalent form as
\begin{equation*}
x^{(k)} = Ax^{(k-1)}
\end{equation*}
We can now apply an $LU$ decomposition to make it easy to compute $x^{(k)}$.
To see that an invertible diagonalizable $n \times n$ matrix $A$ has cofactors that are all
zero (and hence, admits an $LU$ decomposition), simply observe that $A = PDP^{-1}$ and
where $P$ has columns consisting of $n$ linearly independent
eigenvectors of $A$, and $D$ has diagonal with no zeroes in it. 
The rest of the algorithm is then analogous to the power method.
\subsection{Shifted Power Method}	
Lastly, we would like to find all eigenvalues between those with largest and
smallest modulus, respectively. First, observe that if a random chosen $\mu$ is
closest to a simple eigenvalue $\lambda$ of a square matrix $A$, then $\lambda -
\mu$ is the unique eigenvalue of minimum modulus of $A - \mu I$. If $A - \mu I$
is invertible, we can run the inverse power method to obtain $\lambda - \mu$,
from which we are able to obtain $\lambda$.

\section{Linear Iteration}
All linear iterations are of form
\begin{equation*}
x^{k+1} = Ax^{k} + b.
\end{equation*}
We would like to know under what conditions the iteration converges. To explore
this line of questioning further, we will need a series of lemmas.
\begin{lemma}[Schur's Lemma]
Every square matrix is unitarily similar to an upper triangular matrix
whose off diagonal entries are arbitrarily small.
\end{lemma}
\begin{corollary}
\label{cor:spec}
If $A$ is an $n \times n$ matrix with complex entries, then 
\begin{equation*}
\rho(A) = \inf_{ \| \cdot \|} \| A \|
\end{equation*}
\end{corollary}
\begin{proof}
We shall show that $\rho(A) \le \inf_{ \| \cdot \|} \| A \|$ and $\rho(A) \ge
\inf_{ \| \cdot \|} \| A \|$, respectively. If $A$ is the $0$ matrix, there is
nothing to prove. Otherwise, $A$ has 
one nonzero eigenvalue, with a corresponding eigenvector which cannot be $0$.
Hence,
\begin{equation*}
|\lambda| \cancel{\| x \|} = \| Ax \| \le \| A \| \cancel{\| x \|}.
\end{equation*}
from which it follows that $\rho(A) \le \| A \|$. 

For the reverse direction, we first apply Schur's Lemma to 
decompose $SAS^{-1} = D + T_{\ee}$. Observing that similar matrices have the same spectrum,
 we obtain 
\begin{equation*}
\| SAS^{-1} \|_{\ell_\infty} \le \rho(A) + \ee.
\end{equation*}
It is easy to check that, for fixed $S$,  $ \| A \|_{\ell^{\infty'}} \doteq \| S A S^{-1} \|_{\ell^\infty}$
is a norm. 
Since $\ee > 0$ can be taken to be arbitrarily small, we conclude that
$\rho(A) \ge
\inf_{ \| \cdot \|} \| A \|$.
\end{proof}
We now have the tools to prove the following.
\begin{theorem}
\label{thm:it}
The linear iteration $x^{k+1} = Ax^{k} + b$ converges for any initial
vector $x^{0}$ if and only if $\rho(A) < 1$. If so, then $x^{k} \to
(I-A)^{-1}b$.
\end{theorem}
\begin{proof}
To prove sufficiency, we first observe that convergence implies
a fixed point to the map $x \mapsto Ax + b \doteq Tx$.
This map is a contraction if \[\| Tx - Ty \| \le \| x - y \|,\] which simplifies
via linearity of $T$ to \[\| A(x-y) \| \le \| x - y \|. \] But
\begin{equation*}
\| A(x-y) \| \le \| A \| \| x-y \| 
\end{equation*}
If $\rho(A) < 1$, by Corollary \ref{cor:spec} there exists a norm $\| \cdot \|$ such that
$ \| A \| < 1.$ Hence, $T : C^n \to C^n$ is a contraction under this norm, and so Therefore,
$x^{k}$ converges to some $x$ in this norm. Since all norms are equivalent on finite dimensional 
spaces, it follows that $x^{k}$ converges to $x$ in any norm. Since $x$ is a fixed point of $T$, 
we must have 
$x = Ax + b$, or $x = (I-A)b$.
To prove necessity, assume that $\rho(A) \ge 1$. Let $b = 0$ and $x^{0}$ be the eigenvector
with largest eigenvalue $\lambda$ amongst all eigenvalues. Then $|\lambda| \ge 1$, and so 
\begin{equation*}
\| x^{k+1} - x^{k} \| = \| (\lambda^{k} - \lambda^{k-1})x^{0} \| =
|\lambda|^{k} \| (1/\lambda -1) x^{0} \| \to \infty, \quad \lambda > 1
\end{equation*}
If $\lambda =1$, then $-\lambda$ is also an eigenvalue, with corresponding
eigenvector $-x^{0}$. Therefore, assuming without loss
of generality that $\lambda = -1$, we obtain
\begin{equation*}
\| x^{k+1} - x^{k} \| = \| (\lambda^{k} - \lambda^{k-1})x^{0} \| =
2 \|x^{0} \| 
\end{equation*}
which implies $\{x^{k}\}$ is not Cauchy.
\end{proof}
\begin{corollary}[Neumann Iteration]
The sum
\begin{equation*}
S \doteq \sum_{k = 0}^{\infty} A^{k}
\end{equation*}
converges if and only if
$\rho(A) < 1$. If so, then $S = (I - A)^{-1}$.
\end{corollary}
\begin{proof}
One can generalize the proof of Theorem \ref{thm:it} to let $b$ and our initial
choice $x^{0}$ be matrices. Choosing $b = 0$ and $x^{0} = I$, we see that
\begin{align*}
& x^{1} = I,  \\
& x^{2} = A + I,\\
& x^{3} = A^{2} + A + I, \\
& \vdots \\
& x^{k} = \sum_{i = 0}^k A^{i}
\end{align*} 
which by Theorem \ref{thm:it} converges to $(I -A)^{-1}$.
\end{proof}
\begin{remark}
This corollary is important in that it provides us  an $O(n^3)$ iterative method
(multiplication of two $n \times n$ matrices takes $O(n^3)$ computations) for
computing the inverses of matrices. More precisely, note that if $\rho(I + A)
\ge 1$, then $\rho(I + cA) < 1$ for some $-1 < c < 1$. Hence, we can apply
Neumann iteration to $I + cA$ to recover the inverse of $cA$. From there, it is
easy to find the inverse of $A$.
\end{remark}

\subsection{Iterative Refinement Schemes}
Observe that if $B = A^{-1}$, then
\begin{equation*}
x = x + B(b - Ax).
\end{equation*}
If $B \approx A^{-1}$, then we set
\begin{equation*}
x^{k+1} = x^{k} + B(b - Ax^{k}).
\end{equation*}
Let $Q = B^{-1}$. Then we obtain
\begin{equation*}
x^{k+1} = Q^{-1}(Q - A)x^{k} + Q^{-1}b.
\end{equation*}
\begin{definition}
We call $Q$ a \emph{splitting} matrix. If $Q = I$, the iterative scheme is
called \emph{Richardson Iteration}. If $Q$ is a diagonal matrix with diagonal
equal to that of $A$, then it is called \emph{Jacobi Iteration}. Lastly, if $Q$
is a lower triangular matrix with lower triangular part (including the diagonal)
equal to that of $A$, the iterative scheme is called \emph{Gauss-Seidel
Iteration}.
\end{definition}
Notice that in all three cases, $Q^{-1}$ is easily computed. Computing the exact inverse
of an $n \times n$ matrix is generally an $O(n^3)$ computation, and computing
an approximate inverse via Neumann iteration is also, in general, $O(n^3)$. For these special $Q$, 
it is $O(n)$ (the exact inverse is computed via backward substitution of $n$ rows, while in the
Neumann iteration we are multiplying by zero on $O(n^2)$ times) .

However, if we wish to bypass finding the inverse of $Q$, we can do so. More precisely, if
$Q = I$, we obtain
observe that
\begin{equation*}
x^{k+1} = x^{k} + e^{k}, \quad e^{k} \doteq A^{-1}(b - Ax^{k})
\end{equation*}
Then one way to run our iterative refinement as follows: Given base case $x^{0}$, we compute 
\begin{enumerate}[i)]
\item $r^{k} = b - Ax^{k}$
\item $Ae^{k} = r^{k}$
\item $x^{k+1} = x^{k} + e^{k}$
\end{enumerate}
With an appropriate chosen splitting matrix $Q$, the analogous iteration may become
more computationally efficient. We then have
\begin{equation*}
Qx^{k+1} = (Q - A)x^{k} + b.
\end{equation*}
Note that for Gauss-Seidel iteration, the choice of $Q$ is clever, in that this
iteration  is converted to 
\begin{equation*}
L x^{k+1} = - U x^{k} + b
\end{equation*}
where $L$ and $U$ are the upper and lower triangular parts of $A$ respectively,
where $U$ has zeroes on its diagonals. 

\begin{remark}
Hence, it is quite clear that it is more efficient to choose a splitting matrix
before iterating. However, one must be careful to first identify whether or not
the Gauss-Seidel scheme converges for a given matrix $A$. If it does, then in
general it converges faster in general than the Jacobi and Richardson iterative
schemes. If it does not, then we must resort to using another scheme.
\end{remark}
\section{Finding Exact Solutions to Matrix Systems}
We are interested in exact solutions to the system $Ax = b$, 
where $A$ is an invertible $n \times n$ matrix. The unique solution
is given by $x = A^{-1}b$; however, it is numerically inefficient
to compute $A^{-1}$ via Gaussian elimination. We have seen in the previous
section that $A^{-1}$ can be estimated to any degree of accuracy we wish
via Neumann iteration. However, another, more common technique is 
to decompose $A$ into a product of matrices that makes the system $Ax = b$
easy to solve. 
\subsection{LU Decompositions} 
Observe that if we can write $A = LU$, where
$L$ and $U$ are lower and upper $n \times n$ matrices respectively,
then solving $Ax = b$ reduces to solving $Ly = b$ and $Ux = y$.
However, these two systems can easily be solved by backwards substitution
and reverse backwards substitution, respectively.

\subsubsection{Doolittle Factorisation}
Assuming that $L$ has unit diagonal, and seek to find the remaining
entries of $L$ and $U$. Let $L_i$ denote the $i$th row of $L$,
and $U^j$ denote the $j$th column of $U$. Observe that
\begin{align*}
a_{ij} = L_i U^j = \sum_{s=1}^n \ell_{is} u_{sj} = \sum_{s=1}^{\min(i,j)} \ell_{is} u_{sj}.
\end{align*}
Using this formula, we compute the first row of $U$ 
the first column of $L$ as follows. First we aim to compute the first row of $U$.
To do so, we look at $L_1 U^k$, $1 \le k \le n$ and
use the key fact that $\ell_{11} = 1$ and $\ell_{1i} = 0, 1 < i \le n$
to compute $u_{11}, u_{12}, \ldots, u_{1n}$. 
Having computed the first row, we now compute the first column of $L$
by looking at $L_i U^1$, $2 \le i \le n$, and using the fact that
$U$ is upper triangular. 
For the inductive step, assume that we have computed the first $k-1$ rows
of $U$, and the first $k-1$ columns of $L$. Note that
\begin{align*}
a_{kk} = \sum_{s=1}^{k-1}l_{ks}u_{sk} + u_{kk}
\end{align*}
which implies
\begin{align*}
u_{kk} = a_{kk} - \sum_{s=1}^{k-1}l_{ks}u_{sk}.
\end{align*}
However, both terms on the right hand side are known. In particular,
we know the second term via the inductive hypothesis. Hence,
we have computed $u_kk$.

Now that we have computed $u_{kk}$, we use it as a pivot to compute
the values in the $k$th column of $L$. 
Observe that for $k < i \le n$
\begin{align*}
a_{ik} = L^i U^k = \sum_{s=1}^{\min(i,k)}\ell_{is}u_{sk}
= \sum_{s=1}^{k-1} \ell_{is} u_{sk} + \ell_{ik} u_{kk}
\end{align*}
We then solve for $\ell_{ik}$. Note that every other in the expression
on the right is known via the inductive hypothesis, and our previous
computation of $u_{kk}$.

Now that we have computed the $k$th column of $L$, we use it
to compute the $k$th row of $U$. Observe that for $k < i \le n$
\begin{align*}
a_{ki} = L^k U^i = \sum_{s=1}^{\min(i,k)}\ell_{ks}u_{si}
= \sum_{s=1}^{k-1} \ell_{ks} u_{si} + \ell_{kk} u_{ki}
\end{align*}
Observe that $u_{ki}$ is the only term on the right hand side
that is not known. Every other term is known thanks to our computation
of the $k$th column of $L$ earlier, and our inductive hypothesis. Hence,
we can solve for $u_{ki}$, so long as divisions by zero do not occur.

This completes the $LU$ decomposition algorithm. 

\begin{definition}
When $L$ is assumed to have unit diagonal
as above, we call the algorithm a \emph{Doolittle factorisation}. 
If $U$ is assumed to have unit diagonal, it is called a \emph{Crout
factorisation}. If $A$ is symmetric and positive definite, it is called a
\emph{Cholesky factorisation}.
\end{definition}
Observe that we have glossed over the fact that the algorithm fails
if divisions by $0$ when solving for terms in $L$ or $U$. Fortunately,
we have the following.
\begin{proposition}
Let $A$ be an invertible $n \times n$ matrix. Then there exists
a permutation of the rows $P$ such that $PA$ admits 
an $LU$ decomposition. More precisely, no divisions by $0$ will
occur when running an $LU$ decomposition algorithm on $PA$.
\end{proposition}
\begin{remark}
Note that $x$ is a solution to $Ax = b$ if and only if $PAx = b$.
\end{remark}
\begin{exercise}
Show that a real, symmetric, positive definite matrix $A$ has a unique
factorisation $L L^T$, where $L$ is lower triangular
with a positive diagonal. Note that no permutation matrix $P$ is needed.
\end{exercise}
\section{Well-Conditioned and Ill-Conditioned Linear Problems}
Now that we have developed techniques for solving $n \times n$
systems $Ax = b$, we'd like to understand how sensitive our system
is to perturbations. More precisely, we can ask the following two questions:
\begin{enumerate}[a)]
\item If we shift $x \mapsto \tilde{x}$, how much does $b \mapsto 
\tilde{b}$ shift, relative to the size of $b$?
\item If we shift $b \to \tilde{b}$, how much does the solution $x \mapsto
\tilde{x}$ shift, relative to the size of $x$?
\end{enumerate}
\begin{definition}
Let $A$ be a square, invertible matrix. We call $\kappa(A) \doteq \| A \| \| A^{-1} \|$
its \emph{condition number}.
\end{definition}
\begin{theorem}
We have the estimates
\begin{equation*}
\kappa(A)^{-1} \frac{\| b - \tilde{b}\|}{\| b\|} \le \frac{\| x - \tilde{x} \|}{\| x \|}
\le \kappa(A) \frac{ \| b - \tilde{b} \|}{ \| b \| }.
\end{equation*}
\end{theorem}
\begin{proof}
Observe that
\begin{equation*}
\| b - \tilde{b} \| \| x \| = \| A(x - \tilde{x})\| \| A^{-1}b \| 
\le \kappa(A) \| x - \tilde{x} \| \| b \|
\end{equation*}
and
\begin{equation*}
\| x - \tilde{x} \| \| b \| = \| A^{-1}(b - \tilde{b})\| \| Ax \| 
\le \kappa(A) \| b - \tilde{b} \| \| x \|
\end{equation*}
Rearranging the terms in each expression, we obtain the lower and upper
bounds for the size of the relative shift of $\tilde{x}$ from $x$, completing
the proof.
\end{proof}
Hence, large condition numbers imply that small perturbations in $b$ can have
large impacts on the resulting perturbation of $x$, and vice versa.
Motivated by this, we have the following definitions.
\begin{definition}
For a square matrix $A$, if $\kappa(A) \le 1$, we say $A$ is well-conditioned.
Otherwise, we say $A$ is ill-conditioned.
\end{definition}
\begin{remark}
The upper-bound we use for the upper bound necessary for $A$ to be well-conditioned
is somewhat arbitrary, and ultimately depends on the problem we are analyzing
and how accurate our collected data $b$ is. If we are dubious as to 
the accuracy of our data-collecting techniques, we will 
need $\kappa(A)$ to be small,
perhaps much smaller than just $1$, in order to
ensure that the $x$ we come up with as solution to $Ax = b$ is not far
from the actual $x$ occurring in the real-world. In this case we would define
a well-conditioned matrix as having a condition number less than some $c$, where
$c \ll 1$. 
\end{remark}
\begin{remark}
The condition number can also be thought of as a measure of how close a matrix
is to not being invertible. For example, consider
\begin{equation*}
A_\ee = 
\begin{bmatrix}
1 & 1+ \ee \\
1 - \ee & 1 \\
\end{bmatrix}
\end{equation*}
A simple computation shows that $\kappa(A_\ee) = O(1/\ee^2)$.
Hence, $\kappa(A_\ee) \to \infty$ as $\ee \to 0$. Of course,
the matrix
\begin{equation*}
A = 
\begin{bmatrix}
1 & 1 \\
1  & 1 \\
\end{bmatrix}
 = \lim_{\ee \to 0} A_\ee
\end{equation*}
is not invertible.
\end{remark}
\section{Numerical Differentiation}
Given a data set $ \left\{ x_i, y_i \right\} $, where $f(x_i) = y_i$, 
we can estimate $f$ at a point $z_i$ by approximating it via a linearization. More
precisely, if $f \in C^2(I)$, where $ \left\{ x_i \right\} \subset I$, we write
$f(x + h) = f(x) + hf'(x) + h^2 f''(x)/2$ and so
\begin{align*}
f'(x) &= \frac{f(x+h) - f(x)}{h} - hf''(\xi)/2 \\
& \approx \frac{f(x+h) - f(x)}{h} 
\end{align*}
However, observe that as $h$ becomes small, we begin to subtract
\emph{nearly equal quantities}. This results in a loss of precision. Worse,
this error from loss of precision is magnified by $1/h$. To offset this error,
we require a better approximation to $f'(x)$ than the $O(h)$ approximation
we have produced above. 
\subsection{Richardson Extrapolation}
The key to improving our approximation is to assume more regularity for $f$.
We develop our improved approximation by first assuming $f$ is analytic; we leave
as an exercise how to adapt the argument for $f \in C^k$, but not necessarily
analytic. Write
\begin{align*}
f(x+h) &= \sum_{k=0}^\infty h^k f^{(k)}(x)/k!
\\
f(x+h) &= \sum_{k=0}^\infty (-1)^{k}h^k f^{(k)}(x)/k!
\end{align*}
Subtracting the second equation from the first and rearranging terms, we obtain 
\begin{align*}
f'(x) = \frac{f(x+h) - f(x-h)}{2h} - \left[ h^2f^{(3)}(x)/3! + h^4f^{(5)}(x)/5! + \ldots \right]
\end{align*}
which we rewrite as
\begin{align*}
L(h) \doteq \phi(h) + a_2 h^2 + a_4 h^4 + \ldots
\end{align*}
The approximation $L \approx \phi(x)$ has $O(h^2)$ error. To obtain greater
accuracy for an approximation to $L$, we seek to eliminate the $h^2$ term.
Observe that $L(h) = L(h/c)$ for any nonzero constant $c$. Then one can check
that
\begin{align*}
3L(\cdot) = 4L(h/2) - L(h) & = [4\phi(h/2) + a_2 h^2 + a_4 h^4/4 + \ldots]
 - [ \phi(h) + a_2 h^2 + a_4 h^4 + \ldots]
 \\
 & = 4 \phi(h/2) - \phi(h) - 3a_4 h^4/4 - 15a_6h^6/16 -\ldots 
\end{align*}
and so
\begin{align*}
L = \frac{4}{3}\phi(h/2) - \frac{1}{3}\phi(h) - \frac{a_4}{4}h^4 - \ldots
\end{align*}
Hence, $L \approx \frac{4}{3}\phi(h/2) - \frac{1}{3}\phi(h) $
is a $O(h^4)$ approximation. 

We can generalize this technique, known as \emph{Richardson extrapolation},
to give an approximation to $L$ of
$O(h^{2j})$, for any desired $j \in \mathbb{N}$. 
\begin{exercise}
Show that if $f \in C^k$ but $f \not \in C^{k+1}$, then we can modify
the Richardson extrapolation algorithm developed above to obtain an 
$O(h^{2j})$ approximation to $L$, for any positive integer $j < k$.
\end{exercise}
\subsection{Numerical Differentiation Using Polynomial Interpolation}
An alternative approach is to use either Newton interpolation,
Lagrange interpolation, or splines to interpolate a given data set.
Once the interpolating polynomial is obtained, we simply differentiate it
to obtain our approximation to $f'(x)$. This has the advantage
that we do not suffer loss of precision due to subtraction of nearly equal
quantities, as we do when computing $f'(x)$ via linearization. However,
constructing a polynomial $p$ via interpolation, and then computing $p'(x)$
for various inputs $x$ is computationally more costly. 
\section{Numerical Integration}
Consider the data set $ \left\{ x_i, y_i \right\}_{i=1}^{n}$, where $x_i = a$,
$x_n = b$, $f(x_i) = y_i$,
and where we assume without loss of generality that $\{x_i$\} is an
increasing set. We can estimate the integral of $f$ in the region $I \subset [x_1, x_n]$
using basic Calculus tools, such as right rectangles, left tangles, and trapezoids.
Observe, however, that all these approaches are produced via linear splines
between our data points. 

We wish to generalize this approach to obtain more sophisticated, and precise,
numerical integrals. To do so, we can either look for a polynomial or a spline
that interpolates the given data points, and use this to find our approximation
to the integral of $f$. More precisely, we have 
\begin{align*}
\int_a^b f(x)\, dx \approx \int_a^b p(x)\, dx
\\
|E_\ell(b-a)| \le | \int_a^b f(x) \, dx| \le |E_u (b-a)|
\end{align*}
where $E_\ell$ and $E_u$ are lower and upper bounds, respectively, for the $L^\infty$
error of our polynomial approximation $p(x)$. Hence, lower and upper bounds for the error of our approximation $p(x)$
are translated into lower and upper bounds for our integral approximation.

Using Lagrange interpolation, it is easy to see that
\begin{align*}
\int_a^b f(x)\, dx \approx \sum_{i=0}^n f(x_i)\int_a^b l_i(x)\, dx 
\doteq \sum_{i=0}^n A_i f(x_i)
\end{align*}
If the nodes are equally spaced, this is called a \emph{Newton-Coates Formula}.
\begin{exercise}
Show that for uniform spacing $h = (b-a)/n$ and $x_i = a + ih$,
the Newton-Coates formula produces the composite trapezoid formula
\begin{align*}
\int_a^b f(x) \, dx \approx \frac{1}{2}\sum_{i=1}^n
(x_i - x_{i-1})[f(x_{i-1}) - f(x_i)]
\end{align*}
\end{exercise}
\begin{exercise}
Derive Simpson's rule
\begin{align*}
\int_a^b f(x)\, dx \approx \frac{b-a}{6} \left [f(a) + 4f((a+b)/2) + f(b) \right]	
\end{align*}
via the Newton-Coates formula. The formula will guarantee that
Simpson's rule is exact for polynomials of degree $\le 2$. Show that it is,
in fact, exact for polynomials of degree $\le 3$.
\end{exercise}
Aside from integrating the Lagrange polynomials, another way we can
compute the $A_i$ is via the method of undetermined coefficients. More precisely,
suppose we seek $A_0, A_1, A_2$ such that
\begin{align*}
\int_0^1 f(x) \, dx \approx A_0 f(0) + A_1 f(1/2) + A_2 f(1).
\end{align*}
One way is to construct three equations for these three unknowns.
Letting $f = 1$, $f = x$, and $f = x^2$, respectively, we compute
\begin{align*}
& 1 = A_0 + A_1 + A_2
\\
& 1/2 = 1/2A_1 + A_2
\\
& 1/3 = 1/4A_1 + A_2
\end{align*}
from which we obtain $A_0 = 1/6$, $A_1 = 2/3$, and $A_2 = 1/6$.
Since $ \left\{ 1, x, x^2 \right\} $ is a basis for the vector space of 
polynomials of degree $\le 2$, and since integration is a linear operation, we see that our approximation is in fact exact
for all polynomials $f(x)$ of degree $\le 2$.
\end{document}