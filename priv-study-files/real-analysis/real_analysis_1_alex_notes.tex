\documentclass[12pt]{report}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%For Importing Pictures
%
\usepackage{graphicx}
%
%%%%%%%%%%%%
%%
%%For Importing Pictures
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\setlength{\topmargin}{.5in}
\setlength{\textheight}{215mm}
\setlength{\textwidth}{142 mm}
\evensidemargin 0.3in
\oddsidemargin 0.3in
\topmargin -.2in
\setcounter{page}{1}
\setcounter{chapter}{-1}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\newtheorem{theorem}{Theorem}[section] 
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary} 
\newtheorem{claim}[theorem]{Claim}
 \newtheorem{prop}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{no}[theorem]{Notation} 
\newtheorem{definition}[theorem]{Definition}
\newtheorem{re}[theorem]{Remark} 
\newtheorem{examp}{Example}[section] 
\newtheorem {exercise}[theorem] {Exercise} 
\newenvironment{de}{\begin{definition}\rm}{\end{definition}}
\newcommand{\Section}[1]{\section{#1}\setcounter{equation}{0}}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\newenvironment{proof}{\noindent {\em Proof:}}{\hspace*{1cm}
        \hspace*{\fill}$\rule{1.2ex}{1.4ex}$\medskip}
\newcommand{\twostack}[2]{\mathrel{\mathop{#1}\limits_{#2}}}
\newcommand{\threestack}[3]{\mathrel{\mathop{\mathop{#1}\limits_{#2}}\limits
_{#3}}}
\newcommand{\twostackup}[2]{\mathrel{\mathop{#1}\limits^{#2}}}
\newcommand{\threestackmix}[3]{\mathrel{\mathop{\mathop{#1}\limits_{#2}}\lim
its^{#3}}}
\def\2stack#1#2{\mathrel{\mathop{#1}\limits_{#2}}}
\def\3stack#1#2#3{\mathrel{\mathop{\mathop{#1}\limits_{#2}}\limits_{#3}}}

\begin{document}

\chapter{Preliminaries}\label{c:1}

\vspace{4mm}
\noindent
Here we shall review some of the material usually presented
in undergraduate Real Analysis courses. These
preliminaries  should  be read together with the
Prologue of Folland's book. Since most proofs are not provided,
we strongly suggest that the reader does the missing proofs
as exercises.  Rudin's  Principles is a good reference book for this
material.



\section{Numbers and Euclidean spaces} \label{s1:1} 
We shall use the notation

\begin{eqnarray*} &\mathbb{N}& = \{1, 2, 3, \dots\} \quad \mbox{
Natural numbers}\\ &\mathbb{Z}& = \{0, \pm 1, \pm 2, \dots \} \quad
\mbox{ Integers}\\ &\mathbb{Q}& = \left \{\frac{m}{n}: m, n \in
\mathbb{Z}, \ n \ne 0, (m, n) = 1\right \} \
\mbox{ Rationals}
\end{eqnarray*} The rationals with the usual addition, +, and
multiplication, $\cdot$, form a field, which is ordered with the usual
``$<$''.  However, the rational are
\textbf{incomplete}.  For example, the set
\[
\{x \in \mathbb{Q}: x^2 < 2\}
\] is bounded above but has no least upper bound in $\mathbb{Q}$. 
The rationals can be completed by the real numbers $\mathbb{R}$,
which is a complete, ordered field.

\bigskip
\noindent
\textbf{Completeness of $\mathbb{R}$}.  For every non-empty set
$E$ of real numbers which is bounded above there exist a unique real
number denoted by
$\sup E$ such that 

\begin{itemize}
\item[1.]  $\sup E$ is an upper bound of $E$.
\item[2.]  If $u$ is any upper bound, then $u \ge \sup E$.
\end{itemize}

\medskip
\noindent
\textbf{Examples} 

\begin{itemize}
\item[1.] If $E = \{x \in \mathbb{Q}: x^2 < 2\}$ then $\sup E = \sqrt{2}$.
\item[2.] If $E = \left \{1 - \frac{1}{n}: n \in \mathbb{N}\right \}$ then
$\sup E = 1$.
\end{itemize}

\bigskip
\noindent
\textbf{$\mathbf{n}$-dimensional Euclidean space $\mathbb{R}^n$.} 
By
$\mathbb{R}^n$ we denote the set of all $n$-vectors of real numbers,
i.e.
\[
\mathbb{R}^n = \left \{x = (x_1, \dots, x_n): \ x_j \in \mathbb{R}
\right \}.
\]
$\mathbb{R}^n$ is a vector space over $\mathbb{R}$ with respect to
the usual addition of vectors, and multiplication by a  scalar.  The
distance between two points $x = (x_1, \dots, x_n)$ and $y = (y_1,
\dots, y_n)$ in $\mathbb{R}^n$ is defined by 
\[ d(x, y) = |x-y| = \sqrt{(x_1 - y_1)^2 + \dots +(x_n - y_n)^2}.
\] This distance makes $\mathbb{R}^n$ a \textbf{metric} space which
is
\textbf{complete} (see next section for the definitions).

\medskip
\noindent
\textbf{Complex Numbers.}  They are the algebraic completion of
the real numbers.  That is, all  polynomial equations with real
coefficients have all their roots in  the set of complex numbers. 
They are denoted by $\mathbb{C}$ and are defined by
\[
\mathbb{C} = \{a + ib:\  a, b \in \mathbb{R}\}.
\] With the usual addition and multiplication, they form a field.  The
distance between two complex number $z = a + ib$ and $w = u + iv$ is
defined by
\[ d(z, w) = |z - w| = \sqrt{(a-u)^2 + (b - v)^2}.
\] The set $\mathbb{C}$ together with $d$ forms a complete metric
space.

\bigskip
\noindent
\textbf{The space $\mathbb{C}^{\mathbf{n}}$.}  It is defined by 
\[
\mathbb{C}^n = \left \{z = (z_1, \dots, z_n): z_j \in \mathbb{C} \right
\}.
\] 
With the usual addition and multiplication by a scalar in
$\mathbb{C}$ it becomes a vector space.  With the distance
\[ d(z, w) = \sqrt{|z_1 - w_1|^2 + \dots+ |z_n - w_n|^2}
\] it becomes a complete metric space.

\section{Metric Spaces}
A \textbf{metric} (or distance) on a non-empty set $X$ is a function $d: X
\times X
\longrightarrow [0, \infty)$ such that

\begin{itemize}
\item[1.]  $d(x, y) \ge 0$ with equality iff $x = y$ (positivity)
\item[2.]  $d(x, y) = d(y, x)$ (symmetry)
\item[3.]  $d(x, z) \le d(x, y) + d(y, z)$ (triangle inequality)
\end{itemize} A set $X$ equipped with a metric $d$ is called a
\textbf{metric space} denoted by
$(X, d)$, or simply $X$.

\medskip
\noindent
\textbf{Examples of Metric Spaces}

\begin{itemize}
\item[1. ] The space $\mathbb{R}^n$ with the Euclidean distance $d(x,
y) = |x-y|$.
\item[2. ] The set $C([0,1])$ of the continuous $ f: [0, 1]
\longrightarrow
\mathbb{C}$ with distance defined by
\[ 
d(f, g) = \max_{x \in [0,1]}  |f(x) - g(x)|.
\]
\item[3. ]  The set $R[0,1]$ of the Riemann integrable functions $f: [0,
1]
\longrightarrow \mathbb{R}$, with metric defined by
\[ 
d(f, g) = \int^1_0 |f(x) - g(x)|dx.
\]
\item[4. ] If $(X_1, d_1)$ and $(X_2, d_2)$ are two metric spaces then
the Cartesian  product $X_1 \times X_2$ with metric defined by 
\[ d((x_1, x_2), (y_1, y_2)) = \max\{ d_1 (x_1, y_1), d_2(x_2,
y_2)\}
\] is a metric space.
\end{itemize}

\bigskip
\noindent
\textbf{Definitions.}  Let $(X, d)$ be a metric space.  For $x_0 \in X$
and $r > 0$ the (open)
\textbf{ball} with radius $r > 0$ about $x_0$ is 
\[ B(x_0, r) = \{x \in X: d(x_0, x) < r\}.
\] A subset $E$ of $X$ is \textbf{open} if for every $x \in E$ there
exist $r = r(x) > 0$ such that $B(x, r) \subset E$.  A subset $E$ of $X$
is \textbf{closed} if $E^c = X - E$ is open.  Note that $X$ and the
empty $\emptyset$ are both open and closed.


\begin{theorem} 
\label{th:metric-topo} Let $(X, d)$ be a metric space. 
Then the following hold:
\begin{itemize}
\item[1.]  The union and intersection of a finite family of open
(closed) is open (closed).
\item[2.]  The union of any family of open sets is open.
\item[3.]  The intersection of any family of closed sets is closed.
\end{itemize}
\end{theorem}
The union of all open sets $U \subset E$ is the largest open
set contained in
$E,$ called the \textbf{interior} of $E$ and denoted by 
$\overset{\circ}{E}$.  The intersection of all closed sets containing
$E$ is  the smallest closed set containing
$E$, called the
\textbf{closure} of $E$ and denoted  by $\bar E$.  A subset $E$ of $X$
is called 
\textbf{dense} if $\bar E = X$.  For example, the rationals are dense
in
$\mathbb{R}$.  $E$ is called \textbf{nowhere dense} if $\bar E$ has
empty interior.  For example, $\mathbb{Z}$ is nowhere dense in
$\mathbb{R}$. $X$ is called
\textbf{separable} if it contains a countable dense subset.  For
example, $\mathbb{Q}^n$ is dense in $\mathbb{R}^n$.  Therefore
$\mathbb{R}^n$ is separable.


\section{Convergence, Cauchy Sequences and
Completeness}
Let $(X, d)$ be a metric space.  A sequence $\{x_n\} \subset X$
converges in $X$ if there is $x \in X$ such that $d(x_n, x)
\longrightarrow 0$ as $n \to \infty$; i.e. for any $\varepsilon > 0$
there is $N = N (\varepsilon) > 0$ such that
\[ n > N \Longrightarrow d(x_n, x) < \varepsilon.
\]

\noindent
{\bf Notation.}  $x_n \longrightarrow x$ or $\lim\limits_{n \to
\infty} x_n = x.$

\begin{prop} 
\label{pr:closure}
  Let $(X, d)$ be a metric space  $E
\subset X$ and $x \in X$.  Then the following are equivalent.
\begin{itemize}
\item[1.]  $x \in \bar E$.
\item[2.]  $B(x, r) \cap E \ne \emptyset$ for all $r > 0$.
\item[3.]  There is $\{x_n\} \subset E$ such that $x_n \longrightarrow
x$.
\end{itemize}
\end{prop}
\textit{Proof:}  (See book)

\medskip
\noindent
 Let $(X, d)$ be a metric space.  A sequence  $\{x_n\} \subset
X$ is called
\textbf{Cauchy} if $d(x_n, x_m) \longrightarrow 0$ as $m, n 
\longrightarrow
\infty$.  i.e. If for any $\varepsilon > 0$ there is $ N = N(\varepsilon)$
such that
\[ m, n > N \Longrightarrow d(x_n, x_m) < \varepsilon.
\] A subset $E$ of $X$ is said \textbf{complete} if every Cauchy
sequence $\{x_n\}
\subset E$ converges to a point $x \in E$.

\bigskip
\noindent
\textbf{Example.}  $\mathbb{Q}$ is not a complete subset of
$\mathbb{R}$ since the sequence
\[ x_n = 1 + \frac{1}{1!} + \frac{1}{2!} +\dots + \frac{1}{n!}, 
\] where $n! = 1 \cdot 2\cdot \dots \cdot n$ is Cauchy but it does
not converge to a number in $\mathbb{Q}$.  It converges to the
well-known irrational number $e$.

\begin{prop} 
\label{pr:closed-complete} 
Complete subsets are closed and
closed subsets of a complete metric space are complete.
\end{prop}

\medskip
\noindent
\textbf{Problem 1.}  Show that $C([0,1])$ is complete.

\medskip
\noindent
\textbf{Problem 2.}  Show that $R([0,1])$ is not complete.


\section{Compact Sets and the Heine-Borel
Theorem }
Let $(X, d)$ be a metric space and $E, F \subset X$.  The
\textbf{distance} between $E$ and $F$ is defined  by
\[ d(E, F) = \inf \{d(x, y): \ x \in E, y \in F\}.
\] The \textbf{diameter} of a set $E \subset X$ is defined by
\[
\mbox{diam } E = \sup \{d(x, y): x, y \in E\}.
\] A set $E \subset X$ is said to be \textbf{bounded} if
\[
\mbox{diam } E < \infty.
\] A \textbf{cover} of a set $E \subset X$ is a family of sets
$\{V_\alpha\}_{\alpha
\in A}$ such that

\[ E \subset \bigcup_{\alpha \in A} V_\alpha.
\]
If all $V_\alpha$ are open sets then the cover is said \textbf{open}. 
A \textbf{subcover} is a subset of the family $\{V_\alpha\}_{\alpha
\in A}$ which also covers $E$.
A set $E \subset X$ is called \textbf{totally bounded} if for any
$\varepsilon > 0$ there exist finitely many balls $B_1, \dots, B_k$ of
radius $\varepsilon$ that cover $E$.



\begin{prop}  
\label{pr:totally-bdd}
 Let $(X, d)$ be a metric space.  Then
the following hold.

\begin{itemize}
\item[1.]  \textit{A totally bounded set in $X$ is bounded.}
\item[2.]  \textit{The converse is not true in general.}
\item[3.] \textit{ If $X = \mathbb{R}^n$ then a bounded set is totally
bounded.}
\end{itemize}
\end{prop}
A subset $E$ of a metric space $X$ is called \textbf{compact} if any
open cover of $E$ has a finite subcover of $E$.

\begin{theorem}
\label{th:Heine-Borel}
  Let $(X, d)$ be a metric space and $E
\subset X$.  Then the following are equivalent.
\begin{itemize}
\item[(a)]  $E$ is complete and totally bounded.
\item[(b)]  Every sequence in $E$ has a subsequence which
converges in $E$ (Bolzano-Weirstrass)
\item[(c)]  $E$ is compact (Heine-Borel).
\end{itemize}
\end{theorem}

\bigskip
\noindent
\textbf{Remark.} \textit{ In $\mathbb{R}^n$ (a) is equivalent to $E$
being closed and bounded.  Then $(a) \Longleftrightarrow (c)$ is the
classical Heine-Borel Theorem.}

\medskip
\noindent
\textit{Proof:} \underline{\ $(a) \Longrightarrow (b)$}. 
 Let $\{x_n\}$ be a sequence in
$E$.  Since $E$ is totally  bounded for $\varepsilon = \frac{1}{2}$
there exist finitely many balls with radius $2^{-1}$ covering $E$. 
Since the sequence $\{x_n\}$ has
$\infty$-many terms, there is a ball which contains $\infty$-many
terms of the sequence.  Call  this ball $B_1 = B(2^{-1})$ and $x_{n_1}$
one of the terms of
$\{x_n\}$ inside $B(2^{-1})$.  Again, for $\varepsilon = 2^{-2}$ there
exist finite many balls  covering $E$ and therefore $B_1 \cap E$. 
Since there are $\infty$-many terms of $\{x_n\}$ in $B_1
\cap E$ there exists a ball $B_2 = B(2^{-2})$ such that $B_2 \cap B_1
\cap E$ contains
$\infty$-many terms of $\{x_n\}$.  For $n_2 > n_1$ choose $x_{n_2}
\in B_2 \cap B_1
\cap E$.  Continuing in this way, we construct a subsequence
$\{x_{n_j}\}$ of
$\{x_n\}$ such that
\[ x_{n_j} \in B_1 \cap \dots \cap B_j \cap E, \  B_k = B(2^{-k}).
\] 
By the last relation, we obtain that
\[ d(x_{n_j}, x_{n_k}) \le 2^{1-j} \mbox{ if } k > j.
\] 
Therefore $d(x_{n_j}, x_{n_k}) \longrightarrow 0$ as $j, k \to
\infty$, and
$\{x_{n_j}\}$ is a Cauchy sequence in $E$.  Since $E$ is complete, there
exist $x \in E$ such that
$x_{n_j} \longrightarrow x$.

\medskip
\noindent
\underline{$(b) \Longrightarrow  (a)$ (By contradiction)}.
$\bullet$  Assume $E$ is \textbf{not} complete.  Then there exists a
Cauchy sequence $\{x_n\} \subset E$ which has no limit in $E$.  But
then $\{x_n\}$ can have no convergence subsequence in $E$, because
of the following fact: ``If a Cauchy sequence has a convergent
subsequence then itself converges'' (Homework).

\medskip
\noindent
$\bullet$  Assume $E$ is \textbf{not} totally bounded.  Then there is
$\varepsilon > 0 
$ such that no finite number of balls of radius $\varepsilon$ can
cover $E$.  Choose
$x_1 \in E$.  Since the ball $B(x_1, \varepsilon)$ cannot  cover $E$
there exist $x_2
\in E - B(x_1, \varepsilon)$.  Since $B(x_1, \varepsilon) \cup B(x_2,
\varepsilon)$ cannot cover $E$  there is $x_3 \in E - [B(x_1,
\varepsilon) \cup B(x_2,
\varepsilon)]$.  Continuing this way, we construct a sequence
$\{x_j\}$ and balls $ B(x_j, \varepsilon)$ such that
\[ x_j \in E - \bigcup^{j-1}_{k=1} B(x_k, \varepsilon)
\] Therefore we obtain
\[ d(x_j, x_k) \ge \varepsilon \mbox{ for } j, k.
\] This inequality implies that $\{x_j\}$ does not have a convergent
subsequence  in
$E$, which contradicts the assumption that (b) holds.

\medskip
\noindent
\underline{$(a) +(b) \Longrightarrow (c)$}.  Let $\{V_\alpha\}_{\alpha
\in A}$ be an open family of sets covering $E$.  To show that there is
a finite subcover covering $E$ it suffices to show the following:
``There exists an $\varepsilon > 0 $ such that every ball of radius
$\varepsilon$ and intersecting $E$ is contained in some $V_\alpha$''. 
Since then by the total boundedness of $E$, we would have finitely
many ball covering $E$ and therefore finitely  many $V_\alpha$
covering $E$.  Assume the contrary.  Then for any $n \in \mathbb{N}$
there is a ball $B_n = B(2^{-n})$ such that
$B_n \cap E \ne
\emptyset$ and $B_n$ is contained in no $V_\alpha$.  Choose $x_n \in
B_n \cap E$.  By (b)  there is a subsequence
$\{x_{n_j}\}$ such that $x_{n_j}
\longrightarrow x \in E$.  Since $x \in V_\alpha $ for some $\alpha$
and $V_\alpha$ is open there is $\varepsilon > 0$ such that
\[ B(x, \varepsilon) \subset V_\alpha.
\] Since $x_{n_j} \to x$ for $j$ large enough, we have $d(x_{n_j}, x) <
\frac{\varepsilon}{3}$.  If we choose $j$ large enough so that $2^{-n_j}
<
\frac{\varepsilon}{3}$ then we obtain $B_{n_j} \subset V_\alpha$,
which contradict
$B_{n_j} \not\subset V_\alpha$.

\medskip
\noindent
\underline{$(c) \Longrightarrow (b)$ (by contradiction)}.  Assume
that there is $\{x_n\}
\subset E$ with no convergent subsequence in $E$.  Then for any $x
\in E$ there is a ball $B(x, r_x)$ which contains finitely many terms of
$\{x_n\}$.  Then the family of balls
$\{B(x, r_x) \}_{x \in E}$ cover $E$ and has no finite  subcover of $E$. 
This contradicts (c).
 This completes the proof of Theorem \ref{th:Heine-Borel}.


\medskip
\noindent
\textbf{Remark.} \textit{ The above proof is not different than the
one we would give if $ X =
\mathbb{R}^n$.}

\begin{corollary}
\label{cor:Heine-Borel}
 A subset in $\mathbb{R}^n$ is compact iff
it is closed and bounded.
\end{corollary}

\section{Limit and Continuity of Functions}
 Let $(X, d)$ and $(Y, \rho)$ be two metric spaces, $ x_0 \in
X$ and $y_0 \in Y$.  A function $f: X \longrightarrow Y$ is said to have
{\bf limit $y_0$ as $x$ goes to $x_0$} if for any $\varepsilon > 0$,
there is $\delta = \delta (\varepsilon) > 0 $  such that
\[ 0 < d (x, x_0) < \delta \Longrightarrow \rho(f(x),y_0) <
\varepsilon.
\]
\textit{Notation:}  $\displaystyle{\lim_{x\to x_0} f(x) = y_0}$

\bigskip
\noindent
\textbf{Remark.}  For the definition to make good sense, $x_0$ must
be a
\textbf{limit (accumulation) point}.   That is, any ball $B(x_0, r), \ r >
0$, should contain a point
$x \ne x_0$.

\bigskip
\noindent
\textbf{Examples} 
\begin{itemize}
\item[1. ] Let $f: \mathbb{R} \longrightarrow \mathbb{R}$ with
$f(x) = (\sin x)/x$.  Then $\lim\limits_{x \to 0} f(x) = 1$.
\item[2. ] Let $f: \mathbb{R} \longrightarrow \mathbb{R}$ with $f(x) =
\sin
\frac{1}{x}, \ x \ne 0$ and $f(0) = 0$.  Then $f$ has no limit at $x_0=0$.
\end{itemize}

\begin{prop}
\label{pr:seq-cont0}
  $\displaystyle{\lim_{x \to x_0} f(x) = y_0}$
iff for any sequence
$\{x_n\} \subset X$ with $x_n \ne x_0$ and such that $x_n \to x_0$ we
have that
$f(x_n) \longrightarrow y_0$.
\end{prop}

\begin{prop}
\label{pr:uniq-lim}  If $f$ has a limit, then it is unique.
\end{prop}

\begin{prop}
\label{pr:alg}  If $Y =\mathbb{C}$ and $f, g$ have
limits at $x_0 \in X$ then
\begin{itemize}
\item[1.]  $\displaystyle{\lim_{x \to x_0} (f + g)(x) = \lim_{x \to
x_0} f(x) +
\lim_{x \to x_0} g (x)}$
\item[2.]  $\displaystyle{\lim_{x \to x_0} (fg)(x) = \left ( \lim_{x \to
x_0} f(x)
\right ) \left ( \lim_{x \to x_0} g(x) \right )}$
\item[3.]  $\displaystyle{\lim_{x \to x_0} \left ( \frac{f}{g} \right )
(x) = \frac{\lim_{x \to x_0} f(x)}{\lim_{x \to x_0} g(x)}}$
provided that $\lim_{x \to x_0} g (x) \ne 0$.
\end{itemize}
\end{prop}
A function $f: X \longrightarrow Y$ is said to be
{\textbf{continuous at
$x_0
\in X$}} iff  $\lim\limits_{x\to x_0} f(x) = f(x_0);$ That is for any
$\varepsilon > 0$ there is
$\delta = \delta (\varepsilon) > 0$ such that
\[ d(x, x_0) < \delta \Longrightarrow \rho(f(x), f(x_0)) < \varepsilon.
\]
\begin{prop}
\label{pr:seq-cont1}
 $f$ is continuous at $x_0$ iff is
sequentially continuous at
$x_0$. That is,  for any sequence $x_n \longrightarrow x_0$ in $X$ the
sequence
$f(x_n)
\longrightarrow f(x_0)$ in $Y$.
\end{prop}
\textit{Proof:} (Exercise)

\medskip
\noindent
A function $f: X \longrightarrow Y$ is said to be \textbf{continuous
in} $X$ if it is continuous at every point in $x \in X$.

\begin{prop} 
\label{pr:equiv-cont}
The following are equivalent.
\begin{itemize}
\item[(1)] $f: X \longrightarrow Y$  is continuous in  $X$.
\item[(2)]  for all sequences $\{x_n\} \subset X$, if $x_n
\longrightarrow x$ then 
$f(x_n) \to f(x).$
\item[(3)]  for every open set $U$ in $Y$ we have $f^{-1} (U)$
is open in $X$.
\end{itemize}
\end{prop}
\textit{Proof:} \underline{$(1) \Longrightarrow (3)$}.  
Let $U$ be an open
set in $Y$.  We want to show that $f^{-1} (U)$ is open in $X$.
Let $x_0 \in f^{-1}(U)$, and define $y_0=f(x_0)$.
Since $y_0 \in U$ and $U$ is open there is $B(y_0,\varepsilon_{y_0})
\subset U$.
   Since $f$ is continuous at $x_0$
there is $\delta > 0$ such that
$ 
d(x_0, x) < \delta \Longrightarrow \rho (f(x), y_0) <
\varepsilon_{y_0}
$,
or
$
f(B(x_0, \delta)) \subset B(y_0, \varepsilon_{y_0}) 
$,
 or
$
 B(x_0, \delta) \subset f^{-1} (B (y_0, \varepsilon_{y_0})) \subset
f^{-1} (U) .
$
Therefore $f^{-1} (U)$ is open.
The proof of the other directions are left as an exercise.

\begin{prop} 
\label{pr:cont-alg}
 Let $X, Y, Z$ be metric spaces, and $f, g:
X\longrightarrow Y $,  $\,  h: Y
\longrightarrow Z$ continuous functions.  Then
\[ f + g, \ f \cdot  g, \ \frac{f}{g},\  h\circ f
\] are continuous functions whenever they are well defined.
\end{prop}
\textit{Proof:} (Exercise)

\medskip
\noindent
\textbf{Problem 1.}  Let  $(X, d)$ be a metric space.  A function $f: X
\longrightarrow
\mathbb{R}^n$ is continuous iff each component  of $x \longmapsto
(f_1(x), \dots, f_n(x))$ is continuous.

\medskip
\noindent
\textbf{Problem 2.}  A function $f: \mathbb{R}^n \longrightarrow
\mathbb{R}$ is
\textbf{separately} continuous if for all $j = 1, \dots, n$ and for
all fixed value of all  $x_k$, $k \ne j$, the function $g(x_j) = f(x_1,
\dots, x_j, \dots, x_n)$ is continuous.  Show that continuity implies
separate continuity, and that the converse is not true.


\section{Continuity and Compactness}
Let $(X, d)$ and $(Y, \rho)$ be metric spaces.  A function $f: X
\longrightarrow Y$ is
\textbf{uniformly} continuous if for any $\varepsilon > 0 $ there is
$\delta =
\delta (\varepsilon) > 0$ such that
\[ d(x, y) < \delta \Longrightarrow \rho(f(x), f(y)) < \varepsilon.
\]
\textbf{Examples} 
\begin{itemize}

\item[1. ] $f:[0,1] \longrightarrow \mathbb{R}$ with $f(x)=x^2$.
Since $|x^2-y^2|=|x+y||x-y|\le 2|x-y|$, for given  $\varepsilon > 0 $,
we can choose  $\delta=\varepsilon/2$ to show that $f$ is uniformly
continuous.
\item[2. ] $f:[0,1] \longrightarrow \mathbb{R}$ with $f(x) $ =
polynomial,  is uniformly  continuous function in [0,1].
\item[3. ]  $f: (0,1) \longrightarrow \mathbb{R}$ with $f(x) =
\frac{1}{x}$ is not  uniformly continuous on (0,1).
\end{itemize}

\begin{theorem} 
\label{th:cont-comp}
 Let $X$ be a compact metric space, and
let $f: X
\longrightarrow Y$ be a continuous function.  Then $f$ is uniformly
continuous.
\end{theorem}
\textit{Proof:}  
Let $\varepsilon > 0$. Since $f$ is continuous at every
point $x \in X$ there exist $\delta_x > 0 $ such that
\[ d(x, y) < \delta_x \Longrightarrow \rho(f(x), f(y)) <
\frac{\varepsilon}{2}.
\] Now the family of the balls $\left \{B\left ( x, \frac{1}{2}
\delta_x\right )\right \}$ cover
$X$.   Since $X$ is compact there exist finitely many balls $B\left
(x_1, \frac{1}{2}
\delta_{x_1}\right ), \dots, B\left (x_m, \frac{1}{2}
\delta_{x_m}\right )$ covering
$X$.  Now let
\[
\delta = \delta_\varepsilon = \frac{1}{2} \min \{\delta_{x_1}, \dots,
\delta_{x_m}\} > 0.
\] If $x, y \in X$ such that $d(x, y) < \delta$ then $x \in B \left (x_j,
\frac{1}{2}
\delta_{x_j}\right )$ for some $j$.  Since
\begin{eqnarray*} d(x_j, y) &\le& d(x_j, x) + d(x, y) \\ &\le&
\frac{1}{2} \delta_{x_j} + \frac{1}{2} \delta_{x_j} =\delta_{x_j}
\end{eqnarray*} 
we have
\begin{eqnarray*}
\rho(f(x), f(y)) &\le& \rho(f(x), f(x_j)) + \rho(f(x_j), f(y))\\
&<&\frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon.
\end{eqnarray*}

\begin{theorem} 
\label{th:cont-maxmin}
Let $f: X \longrightarrow \mathbb{R}$ be
a continuous function and $X$ be compact.  Then $f$ achieves its
maximum and its minimum in $X$. i.e. there exist points $x_{\min}$
and $x_{\max}$ in $X$ such that
\[ f(x_{\min}) \le f(x) \le f(x_{\max}), \ \forall x \in X.
\]
\end{theorem}
\textit{Proof:}  Let $x_n$ be a sequence such that $f(x_n)
\longrightarrow sup\ f(x)$.  Since $X$ is compact there exists a
subsequence $\{x_{n_j}\}$ such that
$x_{n_j}
\longrightarrow x_{\max} \in X$.  Since $f$ is continuous, we have 
that $f(x_{n_j})
\longrightarrow f(x_{\max})$.  Therefore
\[ f(x) \le \sup_{x \in X} f(x) = f(x_{\max}), \ \forall
x \in X.
\]
 Similarly we show the inequality for the minimum.

\begin{theorem} 
\label{th:image-compact}
The image of a compact set under a
continuous function between metric spaces is compact.
\end{theorem}
\textit{Proof:}  Let $f: X \longrightarrow Y$ be continuous and $K$ be
a compact subset of $X$.  We want to show  that $f(K)$ is compact in
$Y$.  Let $\{V_\alpha\}$ be an open cover of $f(K)$.  Then $f^{-1}
(V_\alpha)$ are open  and cover $K$.  Since $K$ is compact there are
finitely many $V_\alpha$ say $V_{\alpha_1}, \dots, V_{\alpha_k}$
such that
\[
 K \subset \overset{k}{\underset{j=1}{\cup}} f^{-1} (V_{\alpha_j}).
\]

Then $ f(K) \subset \overset{k}{\underset{j=1}{\cup}}  
V_{\alpha_j}.$ Thus $f(K)$ is compact in $Y$.  


\section{Sequences and Series of Functions}
 Let $(X, d), (Y, \rho)$ be metric spaces and  $f_n: X
\longrightarrow Y$ be a sequence of functions.  The sequence
$\{f_n\}$ converges \textbf{pointwise} to a function $f: X
\longrightarrow Y$ if for any fixed $x \in X$ we have that $f_n(x)
\longrightarrow f(x)$; i.e.  for any $\varepsilon > 0 $ there exists $N =
N(x, \varepsilon)$ such that
\[ n \ge N \Longrightarrow d(f_n(x), f(x)) < \varepsilon.
\] If in this definition $N$ can  be chosen independently of $x$, i.e. $N
= N(\varepsilon)$, then $f_n$ is said to converge \textbf{uniformly}
to $f$.


\medskip
\noindent
\textbf{Examples}
\begin{itemize}
\item[1. ] If $f_n: [0,1] \longrightarrow \mathbb{R}$ with $f_n(x) =
x^n$ and
\[f(x) = \left \{ \begin{array}{ll} 0 &\mbox{if } 0 \le x < 1\\ 1 &\mbox{
if } x = 1 ,\end{array}\right .
\] then $f_n \longrightarrow f$ pointwise in [0,1], but not uniformly.

\item[2. ]  $f_n: \left [0,\frac{1}{2} \right ]\longrightarrow
\mathbb{R}$ with $f_n(x) = x^n$ and
$f(x) = 0$, then $f_n \longrightarrow f$ in $\left [ 0, \frac{1}{2} \right
]$ uniformly.
\end{itemize}

\begin{theorem}
\label{th:unif-conv-cont}  
Let $f_n: X \longrightarrow Y$ be a
sequence of functions converging uniformly to a function $ f: X
\longrightarrow Y$.  Then the following hold.
\begin{itemize}
\item[(1)]  If all $f_n$ are continuous at a point $x_0,$ then
$f$ is continuous at
$x_0$.
\item[(2)]  If all $f_n$ are continuous in $X,$ then $f$ is
continuous.
\item[(3)]  If all $f_n$ are uniformly continuous in $X,$ then $f$
is uniformly continuous in $X$.
\end{itemize}
\end{theorem}
\textit{Proof:} (Exercise)

\medskip
\noindent
If $f_n: X \longrightarrow \mathbb{C}$ is a sequence of
continuous functions, then the series $\sum\limits^\infty_{n=1} f_n$
is said to {\bf converge pointwise (uniformly)} if the partial sums
\[ S_N (x) = \sum^N_{n=1} f_n (x)
\]  converge pointwise (uniformly) to a function $ s: X
\longrightarrow
\mathbb{C}$.


\begin{theorem}
\label{th:Cauchy-Criterion} (Cauchy's Criterion) 
 Let $f_n: X
\longrightarrow
\mathbb{C}$ be a sequence of functions.  Then the following hold:
\begin{itemize}
\item[(1)]   $f_n$ converges uniformly on $X$ if for any
$\varepsilon > 0 $ there is $N > 0$ such that
\[ n, m > N \Longrightarrow |f_n(x) - f_m(x) | < \varepsilon.
\]
\item[(2)]  The series $\sum f_n$ converges uniformly on $X$
if for any
$\varepsilon > 0 $ there is $N > 0$ such that
\[ m \ge N, k \ge 0 \Longrightarrow \left | \sum^{m+k}_{n=m} f_n (x)
\right | <
\varepsilon.
\]
\end{itemize}
\end{theorem}
\textit{Proof:} (Exercise) 

\begin{theorem}
\label{th:Weierstrass-Mtest}
(Weierstrass $M$-test)     
If $f_n: X
\longrightarrow
\mathbb{C}$ is a sequence of function such that

\begin{itemize}
\item[(a)]  $|f_n(x)| \le M_n, \ x \in X, \ n = 1, 2, \dots$
\item[(b)]  $\sum\limits^\infty_{n=1} M_n < \infty,$
\end{itemize}
then $\sum f_n$ converges uniformly on $X$.
\end{theorem}
\textit{Proof:} (Exercise) 

\medskip
\noindent
{\bf Example.} $\sum\limits^\infty_{n=1} n^{-2}\sin (nx)$
converges uniformly since $|n^{-2}\sin (nx)|\le n^{-2}$, and
$\sum\limits^\infty_{n=1}  n^{-2} < \infty$.

 

\section{The Riemann Integral}
 Let $I$ be a closed interval in $\mathbb{R}^n$, i.e.
\[ I = [a_1, b_1]\times \dots \times [a_n, b_n] = \{(x_1, \dots, x_n): \
a_j \le x_j \le  b_j\}
\] and $f: I \longrightarrow \mathbb{R}$ be a bounded function.  We
partition $I$ into a finite collection $P$ of non-overlapping intervals:
i.e. $P = \{I_k\}^N_{k = 1}$ we define
\[|P| = \max _k (\mbox{diam } I_k).
\]
%\centerline{\textbf{ INSERT GRAPH}}

For each $I_k$ we select a point $x^k$ and form the sum

\[ R_P (f) = R_P (x^1, \dots, x^N) f= \sum^N_{k=1} f(x^k) vol (I_k),
\]
called a Riemann sum.  Then $f$ is said Riemann integrable if  there is
a number, denote by $\int_I f(x) dx,$ such that
\[
\lim_{|P| \to 0} R_P(f) = \int_I f(x) dx.
\]
I.e. for any $\varepsilon > 0$ there is $\delta = \delta (\varepsilon) >
0$ such that

\[ |P| < \delta \Longrightarrow \left |R_P(f) - \int_I f(x) dx \right | <
\varepsilon.
\]
If for a partition $P$ define

\begin{eqnarray*} U_P(f) &\doteq& \sum\limits^N_{k=1} \left [
\sup_{x \in I_k} f(x)
\right ]
\cdot vol (I_k)\\ L_P (f) &\doteq& \sum\limits^N_{k=1} \left [
\inf_{x \in I_k} f(x) \right ] \cdot vol (I_k)
\end{eqnarray*} then we have
\[ L_P(f) \le R_P(f)\le U_P(f),
\] and $f$ is Riemann integrable if 
\[
\sup_{P} L_P(f) = \inf_P U_P(f)
\] and then
\[
\int_I f(x) dx =\sup_{P} L_P(f) = \inf_P U_P(f)
\]

\begin{theorem}
\label{th:cont-int}
Every continuous function $ f: I
\longrightarrow
\mathbb{R}$ is Riemann integrable.
\end{theorem}
\textbf{Example.}  For the Dirichlet function on [0,1]

\[f(x) = \left \{\begin{array}{ll} 1 &\mbox{ if } x \in Q \cap [0,1]\\
\\ 0 &\mbox{ if }x \in Q^c \cap [0,1] \end{array} \right .
\]
and for any partition $P$ we have
$ L_P(f) = 0$  and  $U_P (f) = 1$.
Therefore $f$ is not Riemann integrable.

\medskip Notice that the Dirichlet's function is discontinuous
everywhere.

\begin{theorem}
\label{th:almost-cont-int}  A bounded function $ f: I
\longrightarrow
\mathbb{R}$ is Riemann integrable iff the set of its discontinuities
\[
\{x \in I: f \mbox{\textit{ is not continuous at }} x \}
\]
has ``measure'' zero.
\end{theorem}

\medskip
\noindent
\textbf{Remark.} It is interesting that the condition for 
Riemann integrability involves the notion of ``Lebesgue measure"
which we shall consider in the next chapter.

\medskip
\noindent
\textbf{Fact.}  Countable sets in $\mathbb{R}^n$ have ``measure''
zero.  Thus a bounded function with countable discontinuities is
Riemann integrable.

\begin{theorem}
\label{th:unif-cont-int} If $f_k: I \longrightarrow \mathbb{R}$ is a
sequence of Riemann integrable functions converging uniformly to a
function on $I$, then
\[
\lim_{k \to \infty} \int_I f_k(x) dx = \int_I \lim_{k \to \infty} f_k (x)
dx.
\]
\end{theorem}
 \medskip
\noindent
\textbf{Example.}  $\displaystyle{\lim_{k \to \infty}
\int^{\frac{1}{2}}_0 x^k dx =
\int^{\frac{1}{2}}_0 \lim_{k \to \infty} x^k = \int^{\frac{1}{2}}_0 0 \
dx = 0}$

\medskip
\noindent
{\bf Remark.} Theorem \ref{th:unif-cont-int} provides a condition
(uniform convergence) for passing the limit inside the integral
sign in the Riemann integral. However as we shall see later
there is a need for a more general such theorem, and it will be
accomplished in the framework of the ``Lebesgue integral".



\section{Contraction Mapping Theorem and
Applications}
Here we shall discuss the contraction  mapping principle and we shall
apply it to show  the fundamental theorem on local existence and
uniqueness of a solution to the initial value problem for  ordinary
differential equations (ODE).

\medskip
\noindent
Let $(X, d)$ be a  metric space.  A mapping $A:X \to X$ is said
a {\bf contraction} if there exists an  $\alpha$, $0 < \alpha < 1$, such
that
\[
d(Ax, Ay) \le \alpha d(x, y), \ \forall x, y \in X.
\]
{\bf Remark.} Observe that a contraction mapping is always
continuous.

\begin{theorem}
\label{th:contr-map} (Contracting Mapping Theorem)
Let $(X, d)$ be a complete metric space, and $A:X \to X$ be a
contraction mapping.
Then $A$ has a unique fixed point in $X$.
That is,  there is a unique point $ x^\ast\in X$ such
that $Ax^\ast = x^\ast$. Furthermore, if $x_0$ is any point in $X$,
and define the sequence $x_{n+1}=Ax_n$, then
$x_n\to x^\ast$ as $n\to\infty$.
\end{theorem}
{\bf Proof.}  First we show uniqueness.
 If $x^{\ast}$ and
$x^{\ast\ast}$ are two fixed points, then
\[
d(x^{\ast}, x^{\ast\ast}) = d(Ax^{\ast},  Ax^{\ast\ast}) \le \alpha
d(x^{\ast}, x^{\ast\ast}) \Longrightarrow d(x^{\ast}, x^{\ast\ast}) = 0
\Longrightarrow x^{\ast} = x^{\ast \ast}.
\]
To show that there is a fixed point $x^{\ast}$
it suffices to show that $\{x_n\}$ converges to $x^{\ast}\in X$.
Since $X$ is complete it suffices to show
that $\{x_n\}$ is a Cauchy sequence.  
For this we shall need the following computation.  For $n = 1, 2,
\dots$, by a repeated application of the contraction
inequality we obtain 
\begin{eqnarray*}
d(x_{n+1}, x_n) &=& d(Ax_n, Ax_{n-1})\\
&\le& \alpha d(x_n, x_{n-1})\\
&\le& \alpha^2 d(x_{n-1}, x_{n-2})\\
&\dots& \\
&\le& \alpha^n d(x_1, x_0).
\end{eqnarray*}
Next let $n, k \in \mathbb{N}$. The last inequality
together with the triangle inequality give
\begin{eqnarray*}
d(x_{n+k}, x_n) &\le& \sum^k_{j=1} d(x_{n+j}, x_{n+j-1})\\
 &\le& \sum^k_{j=1} \alpha^{n+j-1} d(x_0, x_1)\\
&\le& \frac{\alpha^n}{1 - \alpha} d(x_0, x_1) \to 0 \mbox{ as } n \to
\infty.
\end{eqnarray*}
Therefore $x_n$ is a Cauchy sequence. This completes the proof
of the theorem.

\vspace{2mm}
\noindent
{\bf Fundamental local existence theorem for  ODE.}
We want to solve the initial value problem
\[
\frac{dx}{dt} = f(t, x), \ x(t_0) = \xi,
\]
where $f$ is  a given continuous function in two variables,
and $\xi$ a real number such that the point $(\xi, t_0)$
is in the domain of $f$. 

\medskip
\noindent
First observe that our initial
value problem is equivalent to the following integral
equation
\[
x(t) =\xi + \int^t_{t_0} f(s, x(s)) ds.
\]
Next assume that $f$ is continuous on 
$[t_0-a, t_0+a]\times[\xi -b, \xi + b]$, and
 for $h \in [0, a] $ define the space
\[
X = \{x \in C[t_0-h, t_0+ h]:  \underset{|t-t_0| \le h}{\max} |x(t) -
\xi|
\le b\},
\]
where $h$ is to be chosen later. If on $X$ consider the distance
defined by 
\[
d(x,y)=\underset{|t-t_0| \le h}{\max}|x(t)-y(t)|,
\]
then $(X, d)$ is a complete metric space ( in fact is a Banach
space).  In addition   if we let
  \[
Ax(t) = \xi + \int^t_{t_0}f(s, x(s)) ds
\]
and define  $M = \underset{|t-t_0| \le a; |x-\xi| \le b}{\sup}
|f(t, x)|$ then  we have
\[
|Ax(t) - \xi| = |\int^t_{t_0} f(s, x(s)) dx | \le Mh \le b,
 \]
where $h$  is chosen such that $Mh \le b$. Therefore 
 $A$ is a mapping from $X$ into $X$. 

\medskip
\noindent
$\bullet$ 
  $A$ is a Contraction :  Let $x, y \in X$.  Then
\[
|Ax(t) - Ay(t)|  \le \text{sign} (t-t_0) \int^t_{t_0}|f(s, x(s)) - f(s,
y(s))|ds.
\]
If we assume that $f$ satisfies the following Lipschitz condition
\[
|f(t, x) - f(t, y) | \le L|x-y|, \ \forall x, y \in [\xi - b, \xi +
b],
\]
then 
\begin{eqnarray*}
|Ax(t)-Ay(t)| &\le
 L \text{sign} (t-t_0)\int^t_{t_0} |x(s) - y(s)|ds\\
&\le L|t-t_0| \underset{|s-t_0| \le h}{ \max } |x(s) - y(s)|\\
&\le Lh d(x,y) 
\end{eqnarray*}
Thus, if we choose $h\le a$ and small enough so that 
$Mh \le b$ and  $Lh < 1$, then $A: X\to X$ and is a
contraction.   So $A$ has a unique fixed point in $X$ which is the
unique solution to the integral equation, which gives a unique
solution to our initial value problem. This proves the following 
theorem.

\begin{theorem}\label{th:ODE} (Local existence) Let $f(t, x)$ be a
continuous function on $[t_0-a, t_0+a]\times [\xi -b, \xi + b]$ 
such that  
\[
|f(t, x) - f(t, y) | \le L|x-y|, \ \forall x, y \in [\xi - b, \xi +
b],\, \forall t\in  [t_0-a, t_0+a].
\]
Then the initial value problem
\[
\frac{dx}{dt} = f(t, x), \ x(t_0) = \xi,
\]
has a unique solution $x(t) $ in $[t_0-h, t_0+ h]$ for $0<h\le a$ and
satisfying the conditions $Mh \le b$ and  $Lh < 1$,
where  $M = \underset{|t-t_0| \le a; |x-\xi| \le b}{\sup}
|f(t, x)|$.
Furthermore, this
solution is in $ C^1[t_0-h, t_0+ h]$.
\end{theorem}



\section{Arzela-Ascoli Theorem}
In the space $C[0, 1]$ with the usual max-distance
a bounded sequence may not have a convergence subsequence.
The following theorem states that this is true if the sequence
is uniformly equicontinuous.


\begin{theorem}[Arzela-Ascoli Theorem] \label{th:Arz-Asc}
Let $K$ be compact set in $\mathbb{R}^n$ and $f_j : K \to
\mathbb{C} $ be a sequence of functions such that:
\begin{itemize}
\item[1.]  $\{f_j\}$ is uniformly bounded,
 i.e. $|f_j(x)| \le C,\,  \forall j,x$.
\item[2.]  $\{f_j\}$ is uniformly equicontinuous, i.e.
for every $\varepsilon > 0$ there is $\delta = \delta
(\varepsilon) > 0$  such that
\end{itemize}
\[
 |x-y| < \delta \Longrightarrow |f_j(x) - f_j(y)| <
\varepsilon,\, \forall j=1, 2,\cdots
\]
Then there is a uniformly convergent subsequence of $\{f_j\}$.
\end{theorem}
{\bf Proof. } It will be done in two steps. First we shall
   obtain a subsequence converging on a countable
dense subset of $K$, and then
we shall show that it converges uniformly on $K$.

\vspace{2mm}
\noindent
{\bf Step 1:} (Here we will use only uniform boundedness).
We  can find  a countable set $D = \{x_1, x_2, \dots\}\subset K$ 
such  that $\bar D = K$ (Choose a point from each one of the
non-empty intersections of K with the balls $B(r,1/j)$, where 
$r\in\mathbb{Q}^n$  and $j=1, 2, 3\cdots$). 
 Since $\{f_j(x_1)\}$ is a bounded sequence in $\mathbb{C}$, there
is a subsequence $\{f_{1j}\}$ such that $\{f_{1j}(x_1)\}$
converges.  Since
$\{f_{1j}(x_2)\}$ is bounded in $\mathbb{C}$, there is a
subsequence $\{f_{2j}\}$ of  $\{f_{1j}\}$  such
that $\{f_{2j}(x_2)\}$ converges. Keep going to obtain
\[\begin{array}{rrrcl}
f_{11} &f_{12} &f_{13} &\cdots &\mbox{ converging on } \{x_1\}\\
f_{21} &f_{22} &f_{23} &\cdots &\mbox{ converging on } \{x_1,
x_2\}\\ 
f_{31} &f_{32} &f_{33} &\cdots &\mbox{ converging on  } 
\{x_1, x_2, x_3\}\\
\cdots &&&&\\
f_{m1} &f_{m2} &f_{m3} &\cdots &\mbox{ converging on } \{x_1,
x_2,
\dots, x_m\}\\
\cdots &&&&
\end{array}
\]
Now if we let $g_m = f_{mm}$ (the diagonal) then
$\{g_m\}$ converges on $D$.

\vspace{2mm}
\noindent
{\bf Step 2.} (We will use the equicontinuity of $\{f_j\}).$  We will
be done if we prove the uniform Cauchy criterion. That is  for any
$\varepsilon > 0$ there exists $N = N(\varepsilon)$ such that 
\[
m,\ell \ge N
\Longrightarrow |g_m(x) - g_\ell(x)| < \varepsilon, \
\forall x \in K.
\]
\noindent
Let $\varepsilon > 0$.  By equicontinuity for this $\varepsilon $
there exists $\delta > 0$, such that
\[ x, y\in K,\,
 |x-y| < \delta \Longrightarrow |g_m(x) - g_m(y)| <
\varepsilon,\,  m=1,2,\cdots 
\]
 Since $D$ is dense  we have
$\overset{\infty}{\underset{j=1}{\cup}} B(x_j, \delta) \supset K$.
  By the Heine-Borel theorem there exist 
 $\{x_1, \dots, x_\nu\}$ 
such that
$\overset{\nu}{\underset{j=1}{\cup}}  B(x_j, \delta) \supset K$.
Since
 $\{g_m(x_j)\}$ converges on  $\{x_1, \dots, x_\nu\}$
there exists $N=N(\varepsilon)$ such that
 \[
m, \ell \ge N \Longrightarrow |g_m(x_j) - g_\ell (x_j)| <
\varepsilon,\, \forall j =1,\dots, \nu.
\]
Now for any $x \in K$ there exists $x_{j(x)}\in
\{x_1, \dots, x_\nu\}$ such that 
$x\in B(x_{j(x)}, \delta)$.  Then for  $m, \ell \ge N$ we have
\begin{eqnarray*}
|g_m(x) - g_\ell(x)| &\le& |g_m(x) - g_m(x_{j(x)})| + |g_m(x_{j(x)})
- g_\ell (x_{j(x)})|\\ &+& |g_\ell (x_{j(x)}) - g_\ell(x)|\\
&<& \varepsilon + \varepsilon + \varepsilon = 3 \varepsilon.
\end{eqnarray*}
Thus $\{g_m\}$ is a Cauchy sequence. This completes the proof
of the theorem.
\noindent
{\bf Typical Situation for Equicontinuity:}
If a sequence $\{f_j\}$ satisfies $|f^\prime_j(x)| \le C, \ \forall x,
j$,  then by the mean value theorem we obtain
\[
|f_j(x) - f_j(y)| \le C|x-y|, \ \forall j, x, y,
\]
which implies equicontinuity.


\section{Weierstrass Approximation Theorem}
\begin{theorem}\label{th:W-Approx.}  If $f: [a, b] \to \mathbb{C}$ is a
continuous function then
for any  $\varepsilon > 0$ there exists  a polynomial
$P(x)$ depending  on $\varepsilon$ such that 
\[
|f(x) - P(x)| <
\varepsilon, \ \forall x \in [a, b]\quad (\Longleftrightarrow
\|f-P\|_\infty <
\varepsilon).
\]
\end{theorem}
{\bf Proof.}  By a change of the variable $x$ we may assume 
$a=0$ and $ b=1$. For $n \in \{1, 2, \dots\}$, define the $n$-th
Bernstein polynomial.  
\[
B_n(f, x) = \sum^n_{k=0} \frac{n!}{k!(n-k)!}  
f\left ( \frac{k}{n} \right ) x^k (1-x)^{n-k}.
\]
We will show that $f_n\to f$ uniformly. First observe that
\[
\displaystyle{1=(x+(1-x))^n = \sum^n_{k=0}
\binom nk x^k (1-x)^{n-k}.}
\]
Let $\varepsilon > 0 $.  Since $f$ is uniformly continuous on
 $[0,1]$ there exists $\delta = \delta (\varepsilon)$, such that
\[
|x-y| < \delta
\Longrightarrow |f(x) - f(y)| < \frac{\varepsilon}{2}.
\]
 Now we have
\begin{eqnarray*}
|f(x) - B_n(f, x)| &=&  |f_n(x)\cdot 1 - B_n(f, x)|\\
&=& \left | \sum^n_{k=0}\left  [f(x) - f\left ( \frac{k}{n}\right
)\right ]
\binom nk x^k (1-x)^{n-k}\right |\\
&\le & \sum^n_{k=0} \left |f(x) - f\left ( \frac{k}{n} \right ) \right |
\binom nk x^k(1-x)^{n-k}\\
&=& \sum^n_{k=0,\text{ and } |x - \frac{k}{n}| < \delta} \left |
f(x) - f
\left (
\frac{k}{n}\right )\right |
\binom nk x^k (1-x)^{n-k} \\
&+&\sum^n_{k=0, \text{ and } |x-\frac{k}{n}| \ge
\delta}\left |f(x) - f\left (
\frac{k}{n}\right ) \right | \binom nk x^k (1-x)^{n-k}.
\end{eqnarray*}
Using the fact that  $f$ is  uniformly continuous we obtain

\begin{eqnarray*}
|f(x) - B_n(f, x)| 
&\le & \frac{\varepsilon}{2} \cdot  1+ \sum_{0 \le k \le n, \text{ and }
\frac{(nx-k)^2}{n^2\delta^2} \ge 1} \left ( \left \|f\right \|_\infty +
\|f\|_\infty \right ) \binom nk x^k(1-x)^{n-k}\\
&\le & \frac{\varepsilon}{2} + 2\|f\|_\infty \sum^n_{k=0}
\frac{1}{n^2\delta^2} (nx-k)^2 \binom nk x^k (1-x)^{n-k}\\
&\le& \frac{\varepsilon}{2} + \frac{2\|f\|_\infty}{n^2\delta^2} n \cdot
\frac{1}{4} < \varepsilon\, \mbox{ for large enough } n,
\end{eqnarray*}
where in the last step we used the identity
\[
\sum^n_{k=0} (nx-k)^2 \binom nk x^k (1-x)^{n-k}= nx(1-x),
\]
and the fact that the maximum of the quantity $x(1-x)$  over $[0, 1]$
is 1/4.
To show the above identity we use Newton's binomial
formula
 \[
(x+y)^n = \displaystyle{\sum^n_{k=0} \binom nk x^k
y^{n-k}}.
\] 
Differentiating it with respect to $x$ and  multiplying  by $x$
gives
\[
nx(x+y)^{n-1} = \sum^n_{k=0} k \binom nk x^k y^{n-k}
\]
Also differentiating twice with respect to $x$ and  multiplying  
  by $x^2$ gives
\[
n(n-1) x^2(x+y)^{n-2} = \sum^n_{k=0} k(k-1) \binom nk x^k y^{n-k}
\]
Then we let  $y = (1-x)$ to obtain
\[
nx= \sum^n_{k=0} k \binom nk x^k (1-x)^{n-k}
\]
and
\[
n(n-1) x^2 = \sum^n_{k=0} k(k-1) \binom nk x^k (1-x)^{n-k}
\]
Therefore we have
\begin{eqnarray*}
\sum^n_{k=0} (nx-k)^2 \binom nk x^k (1-x)^{n-k} &=& \sum^n_{k=0}
[k^2 - 2nxk + n^2 x^2] \binom nk x^k (1-x)^{n-k}\\
&=& nx + n(n-1)x^2 - (2nx) nx + n^2x^2 \\
&=& nx + n^2x^2 - nx^2 - 2n^2x^2 + n^2 x^2\\
&=& nx(1-x).
\end{eqnarray*}
This completes the proof of the theorem.




\section{The Baire Category Theorem}
\begin{theorem}\label{th:Baire-Cat}  
In a complete metric space $X$ the following hold.
\begin{itemize}
\item[(a)]  If $\{V_n\}^\infty_{n=1} $ is a sequence of open dense
subset of $X$, then $\overset{\infty}{\underset{n=1}{\cap}}V_n
$ is dense.
\item[(b)]  $X$ is not countable union of nowhere dense subsets of
$X$.
\end{itemize}
\end{theorem}
{\bf proof.} (a)  Let $W$ be a non-empty open set in $X$.  We must show
that
$W
\cap \left (\overset{\infty}{\underset{j=1}{\cap}} V_j\right ) \ne
\emptyset.$
Since $V_1$ is dense and open there exists $B(x_1, r_1)$, $ r_1 < 2^{-1}$,
such that  
\[
\overline{B(x_1, r_1)} \subset V_1 \cap W \ne \emptyset.
\]
Since $V_2 \cap B(x_1, r_1) \ne \emptyset$ and open,there exists 
$B(x_2, r_2), r_2 < 2^{-2}$, such that
\[
\overline{B(x_2, r_2)} \subset V_2 \cap B_1.
\]
Since $V_3 \cap B(x_2, r_2) \ne \emptyset$ and open,
there exists $ B (x_3, r_3), r_3 < 2^{-3}$, such that
\[
\overline{B(x_3, r_3)} \subset V_3 \cap B_2.
\]
Continuing, we find $B (x_n, r_n), r_n < 2^{-n}$, and 
\[
\overline{B(x_n, r_n)} \subset V_n \cap B(x_{n-1}, r_{n-1})
\subset V_n \cap W.
\]
If $m, n \ge N$, then  $x_n, x_m \in B(x_N, r_N)$. That is
$\{x_n\}$ is a Cauchy sequence.  Then, by
 the completeness of $X$ we have  $ x_n \to x \in X $. And since
$x \in \overline{B(x_N, r_N)} \subset V_N \cap W$, for all $N$,  we
conclude that
\[
\left ( \overset{\infty}{\underset{n=1}{\cap}} V_n\right ) \cap W \ne
\emptyset.
\]
(b)\,  If $\{E_n\}^\infty_{n=1} $ is a countable collection of 
nowhere dense sets then
 $\{(\bar  E_n)^c\}$ is a countable collection of open dense sets in $X$.
Now applying (a) gives
\[
\overset{\infty}{\underset{n=1}{\cap}}(\bar E_n)^c \ne \emptyset
\Longrightarrow \overset{\infty}{\underset{n=1}{\cup}}  E_n
\subseteq \overset{\infty}{\underset{n=1}{\cup}} \bar E_n \ne X.
\]
This completes the proof of the theorem.






%%%%%%%%%%%%%%%%%%%%%%
%
%  Chapter 1
%
%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Measures}

Here we follow closely  Folland's Real Analysis.


\section{Introduction} {\large\bf A Motivation:} Let $\{r_1, r_2,\cdots\}
=[0, 1]\cap\mathbb{Q}$ , and define the sequence of functions
$f_j:[0, 1]\to\mathbb{R}$ by 
\[ f_j(x) = \left \{\begin{array}{ll} 1 &\mbox{ if } x \in 
\{r_1, r_2,\cdots, r_j\}\\
\\ 0 &\mbox{ otherwise } \end{array} \right .
\] Then $f_j$ converges to the Dirichlet function on [0,1]

\[ f(x) = \left \{\begin{array}{ll} 1 &\mbox{ if } x  \in
\mathbb{Q} \cap [0,1]\\
\\
 0 &\mbox{ if } x \in \mathbb{Q}^c \cap [0,1] \end{array}
\right .
\] Although every $f_j$ is Riemann integrable, increasing,  bounded, and
$\int_0^1f_j(x) dx$$=0$,
 the limit function $f$ is not Riemann integrable.  Therefore we can {\bf
not} pass the limit inside the integral. In the following  we shall extend the
notion of integral so that we  have a  general theorem which allows
passage of the limit inside the integral. The theorem we are after is called
Lebesgue's Dominated Convergence  theorem  which states that if a
sequence of ``measurable functions" $f_j$
 on a ``measurable set"  $E$ converges, and 
$|f_j|$ is bounded by an ``integrable function", then
\[
\lim_{j\to\infty}\int_E f_j(x)dx=
\int_E \lim_{j\to\infty} f_j(x)dx.
\]


\vspace{.15cm}
\noindent  For this we need the notion of measure, which generalizes the
concept of volume of intervals  in
$\mathbb{R}^n$.  Therefore a measure $\mu(E) $ of a set $E \subset
\mathbb{R}^n$ must be a number in $[0, \infty]$ and should satisfy the
properties
\begin{itemize}
\item[1.]  If $\{E_j\}$ is a sequence of disjoint sets then
\[
\mu(E_1 \cup E_2 \cup \dots ) = \mu(E_1) + \mu(E_2)+ \dots 
\]
\item[2.]  If $E_2$ is obtained from $E_1$ by a translation,  or rotation or
reflection, then 
\[
\mu(E_2) = \mu(E_1).
\]
\item[3.]  If $Q_1$ is the unit cube then
\[
\mu(Q_1) = 1.
\]
\end{itemize}

\vspace{.15cm}
\noindent {\bf Question.}  In $\mathbb{R}^n$ is there a measure
\[
\mu: \mathcal{P}(\mathbb{R}^n) \longrightarrow [0, \infty]
\] satisfying properties (1) - (3)?

\vspace{.15cm}
 The answer to this question is no and we shall prove it for $n = 1$.

\begin{prop}  In $\mathbb{R}$ there is no measure
$\mu: \mathcal{P} (\mathbb{R})  \longrightarrow [0,
\infty]$ satisfying properties (1) - (3).
\end{prop}

\noindent {\em Proof:}  On the interval [0,1) we define the relation
``$\sim$'' by
\[ x \sim y \mbox{ iff } x - y \in Q.
\] Then by the axiom of choice, we choose an element from each
equivalence class to form the set $N$.  And for each
$r \in Q \cap [0,1)$ we let
\[ N_r = \left (r+N \cap [0, 1 - r)\right ) \cup \left ( r -1 + N \cap [1 - r,
1)\right )
\]
\begin{equation*}  [0,1) =\underset{r \in Q \cap [0,1)}{\cup} N_r\tag{$\ast$}
\end{equation*}
 and
\begin{equation*}
 N_r \cap N_s = \emptyset \mbox{ for } r \ne s  \mbox{ in } Q
\cap [0,1)\tag{$\ast\ast$}
\end{equation*}
 In fact, if $x \in [0,1)$ then there is $y \in N$ such that $x \in [y]$.  Now if
we let
\[
 r = \left \{ \begin{array}{ll} x - y &\mbox{ if } x \ge y\\ x - y + 1 &\mbox{
if } x < y
\end{array} \right.
\]  then $x = r + y$ or $ x = r - 1 + y$.  Thus $x \in N_r$, and ($\ast$) hold. 
Next if for some $ x \in [0, 1)$ we  have $ x \in N_r
\cap N_s, \ r
\ne s \in Q \cap [0, 1)$ then $x = r$ (or $r-1) + y = s$ (or $s-1) + y^\prime, \
y \ne y^\prime \in N$ which implies that $y - y^\prime \in Q$ or $[y] =
[y^\prime]$ which is impossible.  Therefore by (1) we have
\[  1 = \mu([0,1)) = \sum_{r \in Q \cap [0, 1]} \mu(N_r)
\] But
\begin{eqnarray*}
\mu(N_r) &\stackrel{(1)}{=}& \mu(r + N \cap [0, 1 -r)) + \mu(r-1 + N
\cap [1-r, 1))\\ &\stackrel{(2)}{=}& \mu(N \cap [0,1-r)) + \mu(N \cap [1-r,
1))\\ &\stackrel{(1)}{=}& \mu \{ ( N \cap [0, 1-r))
 \cup (N \cap [1-r,1)\}\\ &= & \mu (N \cap ([0, 1-r) \cup [1 -r, 1)))=
\mu(N)
\end{eqnarray*} Therefore we have
\[ 1 = \sum_{r \in Q \cap [0, 1]} \mu(N),
\] which is impossible.


\medskip Since it is not possible to define a measure on all subsets of
$\mathbb{R}^n,
\mathcal{P} (\mathbb{R}^n)$, satisfying (1) - (3) we shall try to construct
such a  measure on a subclass of $\mathcal{P} (\mathbb{R}^n)$.  In fact, we
shall discuss more general measures which arise in other situations
(different than volumes) like in probability theory.


\section{$\sigma$-Algebras.}
 Let $X \ne \emptyset$ be a set.  An {\bf algebra} (field) of sets on $X$
is a nonempty collection
$\mathcal{A} \subset \mathcal{P}(X)$ which is closed under complements
and finite unions (or intersections); i.e.
\begin{eqnarray*}
 &(1) \qquad &E \in \mathcal{A} \Longrightarrow E^c \in \mathcal{A}\\ 
&(2) \qquad &\{E_j\}^k_{j=1} \subset
\mathcal{A} \Longrightarrow
\overset{k}{\underset{j=1}{\cup}}  E_j\in\mathcal{A}.
\end{eqnarray*}
$\mathcal{A}$ is a  $\sigma$-algebra ($\sigma$-field) if in addition is
closed under countable unions; (i.e.)
\begin{equation*} (2')\qquad
 \{E_j\}^\infty_{j=1} \subset \mathcal{A} \Longrightarrow
\overset{\infty}{\underset{j=1}{\cup}}  E_j
\in \mathcal{A}.
\end{equation*}
 
\begin{no}  $\emptyset, X \in \mathcal{A}$
\end{no}

\begin{lemma}  If $\{E_j\}^\infty_{j=1}$ is a collection of sets in an algebra
$\mathcal{A}$ then there exist $\{F_j\}^\infty_{j=1} \subset
\mathcal{A}$ and disjoint such that
\[
\overset{\infty}{\underset{j=1}{\cup}} E_j =
\overset{\infty}{\underset{j=1}{\cup}}
 F_j
\]
\end{lemma}
\noindent {\em Proof:}  Let
\begin{eqnarray*}
 F_1 &=& E_1 \in \mathcal{A}\\  F_2 &=& E_2 - E_1 = E_2 \cap E^c_1 \in
\mathcal{A}\\
 F_3 &=& E_3 - (E_1 \cup E_2) =E_3 \cap (E_1 \cup E_2)^c \in \mathcal{A}\\
 &\ &\quad \dots\\ F_k &=& E_k - \overset{k-1}{\underset{j=1}{\cup}}  E_j
\in
\mathcal{A}.
\end{eqnarray*} Therefore an algebra $\mathcal{A}$ is a $\sigma$-algebra
iff is closed under {\bf disjoint} countable unions.


\begin{examp}  Let $X \ne \emptyset$ be a set.  Then

\begin{itemize}
\item[1.]  $\mathcal{A} = \{\emptyset, X\}$ is a $\sigma$-algebra.
\item[2.]  $\mathcal{A} =\mathcal{P}(X)$ is a $\sigma$-algebra.
\item[3.] $\mathcal{A} = \{E \subset X: E $ or $E^c$ countable \} is
$\sigma$-algebra.
\item[4.] $\mathcal{A} = \{E \subset X: E $ or $E^c$ finite \} is an algebra,
and if $X$ is an infinite set then $\mathcal{A}$ is not a $\sigma$-algebra.
\end{itemize}
\end{examp}


\begin{lemma}
Let $\{\mathcal{A}_\alpha\}_{\alpha \in A}$ be a family
of
$\sigma$-algebras on a set $X$.  Then 
$
\bigcap\limits_{\alpha \in A} \mathcal{A}_\alpha 
$
  is a $\sigma$-algebra.
\end{lemma}

\vspace{.15cm}
\noindent {\em Proof:} (Exercise)

\vspace{.15cm} 
\noindent If $\mathcal{E} \subset \mathcal{P}(X)$ then $\mathcal{P}(X)$
is a $\sigma$-algebra containing $X$.  The intersection of all
$\sigma$-algebras containing $\mathcal{E}$ is called  the
$\sigma$-algebra {\bf generated} by $\mathcal{E}$, and is denoted by
$\mathcal{M}(\mathcal{E})$.

\begin{lemma}  If $\mathcal{E} \subset \mathcal{M} (\mathcal{F})$ then
$\mathcal{M} (\mathcal{E}) \subset \mathcal{M} (\mathcal{F}).$
\end{lemma} {\em Proof:} (Exercise).

\begin{definition}
 A family $\mathcal{T}\subset \mathcal{P}(X) $ is a topology on $X$  if
\begin{itemize}
\item[1.]  $\emptyset, X \in \mathcal{T}$
\item[2.]  $\{U_\alpha\}_{\alpha \in A} \subset \mathcal{T} $ then $
U_{\alpha \in A} U_\alpha \in \mathcal{T}.$
\item[3.] $U_1, \dots, U_k \in \mathcal{T}\Longrightarrow U_1 \cap
\dots \cap U_k\in\mathcal{T}$
\end{itemize}
\end{definition} A set $X$ with a topology $\mathcal{T}$ is called a
topological  space. $\mathcal{T}$ is the collection of the {\bf open}
sets.  Metric spaces are special cases of topological spaces.

\vspace{.15cm} 
\noindent If $X$ is a metric or topological space, then the
$\sigma$-algebra generated by the open sets is called the  {\bf Borel}
$\sigma$-algebra on $X$ denoted by
$\mathcal{B}_X$.

\begin{itemize}
\item  Countable intersections of open set are in
$\mathcal{B}_X$ and are called
$G_\delta$ sets.
\item  Countable unions of closed sets  are in
$\mathcal{B}_X$ and are called $F_\sigma$ sets.
\item Countable union of $G_\delta$ sets are in
$\mathcal{B}_X$ and are called $G_{\delta
\sigma}$ sets.
\item  and so on.
\end{itemize}

\begin{prop} $\mathcal{B}_{\mathbb{R}}$ is generated by each of the
following collections.
\begin{itemize}
\item[1.]  $\mathcal{E}_1 = \{(a, b) : a< b \}$
\item[2.]  $\mathcal{E}_2 = \{[a, b]: a < b\}$
\item[3.] $\mathcal{E}_3 = \{(a, b]: a \in \mathbb{R}\}$ or
$\mathcal{E}^\prime_3 =
\{[a, b): a \in \mathbb{R}\}.$
\item[4.]  $\mathcal{E}_4 = \{(a, \infty): \alpha \in \mathbb{R}\}$ or
$\mathcal{E}^\prime_4 = \{(- \infty, a): a \in \mathbb{R}\}$
\item[5.]  $\mathcal{E}_5 = \{[a, \infty): a \in \mathbb{R}\}$ or
$\mathcal{E}^\prime_5 = \{(- \infty, a]: a \in \mathbb{R}\}.$
\end{itemize}
\end{prop}

\vspace{.25cm}
\noindent {\em Proof:}  (Exercise) see book.


\vspace{.25cm}
\noindent {\bf Cartesian Product:}  If $X_1, \dots X_n$ are sets then 
their {\bf Cartesian} product is 

\begin{eqnarray*}
\Pi^n_{j=1} X_j &=& X_1 \times \dots \times X_n \doteq \{(x_1,
\dots, x_n): x_j
\in X_j\}\\  &=& \{f: \{1, \dots, n\} \longrightarrow
\overset{n}{\underset{j=1}{\cup}}
 X_j\ \text{such that}\  f(j) \in X_j \}
\end{eqnarray*}


The first definition generalizes to countable $\{X_j\}$, and the second
definition generalizes to any family of $\{X_\alpha\}_{\alpha
\in A}$.  That is
\[  X = {\Pi}_{\alpha \in A} X_\alpha = \left \{f: A
\longrightarrow
\underset{\alpha\in A}{\cup}  X_\alpha: f(\alpha) \in X_\alpha.
\right \}
\]
 The $\alpha^{th}$-{\bf projection} or
 {\bf coordinate} map
$\pi_\alpha: {\Pi}_{\alpha \in A}X_\alpha \longrightarrow X_\alpha$ is
defined by
\[
\pi_\alpha (f) = f(\alpha).
\]  If $\mathcal{M}_\alpha$ is a $\sigma$-algebra on $X_\alpha$ then the
{\bf product} $\sigma$-algebra on
 ${\Pi}_{\alpha \in A}X_\alpha$ is the
$\sigma$-algebra generated by
\[
\mathcal{F}_1 = \{\pi^{-1}_\alpha (E_\alpha): E_\alpha \in
\mathcal{M}_\alpha, \
\alpha \in A\}.
\]  It is denoted by ${\otimes}_{\alpha \in A}
\mathcal{M}_\alpha $.

\begin{prop}  If $A$ is countable then
${\otimes}_{\alpha \in A}
\mathcal{M}_\alpha $ is generated by
\[
\mathcal{F}_2 = \left \{{\Pi}_{\alpha \in A} E_\alpha: E_\alpha \in
\mathcal{M}_\alpha \right \}
\]
\end{prop}

\vspace{.25cm}
\noindent {\em Proof:}  $\mathcal{F}_1 \subset \mathcal{F}_2$ since

\[
\pi^{-1}_\alpha (E_\alpha) = {\Pi}_{\beta \in A} E_\beta
\mbox{ where } E_\beta = X_\beta \mbox{ if } \beta \ne \alpha.
\]  And $\mathcal{F}_2 \subset
\mathcal{M}(\mathcal{F}_1)$ since
\[ {\Pi}_{\alpha \in A} E_\alpha = \bigcap_{\alpha \in A}
\Pi^{-1}_\alpha (E_\alpha).
\]


\begin{prop}   Assume that $\mathcal{M}_\alpha =
\mathcal{M} (\mathcal{E}_\alpha)$. 
 Then ${\otimes}_{\alpha
\in A} \mathcal{M}_\alpha$ is generated by
\[
\mathcal{F}_3 = \{\pi^{-1}_\alpha (E_\alpha): E_\alpha \in
\mathcal{E}_\alpha, \
\alpha \in A\}
\] If $A$ is countable and $X_\alpha \in \mathcal{E}_\alpha$ then
${\otimes}_{\alpha \in A} \mathcal{M}_\alpha$ is generated
\[
\mathcal{F}_4 = \{ {\Pi}_{\alpha \in A} E_\alpha:  E_\alpha \in
\mathcal{E}_\alpha\}.
\]
\end{prop}

\vspace{.15cm}
\noindent {\em Proof:}  Since  $\mathcal{F}_3 \subset \mathcal{F}_1$ we
have
$\mathcal{M}(\mathcal{F}_3) \subset \mathcal{M} (\mathcal{F}_1)$.  To
show that
$\mathcal{M} (\mathcal{F}_1) \subset \mathcal{M} (\mathcal{F}_3)$ it
suffices to show that

\[
\pi^{-1}_\alpha (E_\alpha) \subset \mathcal{M} (\mathcal{F}_3), \
\forall E_\alpha
\in \mathcal{M}_\alpha, \ \alpha \in A.
\]  Let
\[
\mathcal{M}^\prime_\alpha \doteq \{ E \subset X_\alpha:
\pi^{-1}_\alpha (E) \in
\mathcal{M} (F_3)\}.
\]
 Then $\mathcal{M}^\prime_\alpha$ is a $\sigma$-algebra on
$X_\alpha$ such that
\[
\pi^{-1}_\alpha (E) \in \mathcal{M}(F_3), \ \forall E \in
\mathcal{M}_\alpha.
\]  Thus
\[
\mathcal{M}(\mathcal{F}_1) \subset \mathcal{M} (F_3).
\]
 Now assume that $A$ is countable.  Then the last proposition we have
$\mathcal{M}(\mathcal{F}_4) \subset \mathcal{M} (F_1)$.   If
$E_\alpha
\in
\mathcal{E}_\alpha$ then
\[
\pi^{-1}_\alpha (E_\alpha) = {\Pi}_{\beta \in A} E_\beta
\mbox{ with } E_\beta = X_\beta \mbox{ for } \beta \ne \alpha.
\]  Therefore $\mathcal{M} (\mathcal{F}_1) =  \mathcal{M}
(\mathcal{F}_3) \subset
\mathcal{M}(\mathcal{F}_4).$



\begin{prop}  Let $X_1, \dots, X_n$ be metric spaces, and $X_1 \times
\dots \times  X_n$ be equipped with the product metric, say
$ d(x, y) = [d_1(x_1, y_1)^2 + \dots + d_n(x_n, y_n)^2]^{\frac{1}{2}}.
$ Then
\begin{itemize}
\item[1.]  $\mathcal{B}_{X_1} \times \dots \times
\mathcal{B}_{X_n}
\subseteq	 \mathcal{B}_{X_1\times \dots\times X_n}$.
\item[2.]   Equality holds if all $X_j$ are separable.
\end{itemize}
\end{prop}

\vspace{.25cm}
\noindent {\em Proof:}  Let $\mathcal{O}_j$ be the open set in $X_j$ and
$\mathcal{O}$  be the open set in $X_1
\times
\dots \times X_n$.  Then we have

\begin{eqnarray*} &\mathcal{F} \doteq \{\pi_j^{-1} (U_j):
 U_j \in \mathcal{O}_j\}
\subset \mathcal{O}\Longrightarrow
\mathcal{M} (\mathcal{F}) 
 \subseteq \mathcal{M}(\mathcal{O})
\\  &\mathcal{B}_{X_1} \times \dots
\times  \mathcal{B}_{X_n} = \mathcal{M} (\mathcal{O}_1)
\times \dots\times \mathcal{M} (\mathcal{O}_j)\\
 &\mathcal{B}_{X_1 \times  \dots \times  X_n} =
\mathcal{M} (\mathcal{O})
\end{eqnarray*}
 Therefore 
\[ B_{X_1}\times  \dots \times  B_{X_n}\stackrel{\mbox{Propo.}}{=}
\mathcal{M} (\mathcal{F}) 
 \subseteq \mathcal{M}(\mathcal{O}) = \mathcal{B}_{X_1
\times  \dots\times  X_n}
\] {\bf 2.}  Let $D_j = \{x^k_j\}^\infty_{k=1} \subset X_j$ be dense in
$X_j$ and let
\[
\mathcal{E}_j = \{B(x, r): x \in D_j, r \in Q^+\}
\]  Then $\mathcal{E}_j$ is countable and 
\[
\mathcal{M} (\mathcal{E}_{j}) = \mathcal{B}_{X_j}.
\]  Let
\[
\mathcal{E} = \{E_1 \times  \dots \times  E_n:  E_j \in \mathcal{E}_j\}.
\]  
Then $\mathcal{E}\subset
\mathcal{M}(\mathcal{E}_1)\times\cdots
\mathcal{M}(\mathcal{E}_n)$.
Since any open set in $X_1 \times  \dots \times  X_n$ is a countable union
of elements in
$\mathcal{E}$ we have
\[
\mathcal{M} (\mathcal{E}) = \mathcal{B}_{X_1 \times  \dots
\times  X_n}
\]  Therefore
\begin{eqnarray*} 
\mathcal{B}_{X_1\times  \dots \times  X_n} &=
\mathcal{M}(\mathcal{E}) \subset\mathcal{M}({\mathcal{E}_1})
\times \dots \times  \mathcal{M}(\mathcal{E}_n)\\ &=\mathcal{B}_{X_1}
\times  \dots
\times  \mathcal{B}_{X_n}.
\end{eqnarray*}


A family $\mathcal{E}$ on $X$ is called an {\bf elementary} family if
\begin{itemize}
\item[1.]  $\emptyset \in \mathcal{E}$
\item[2.]  $E, F \in \mathcal{E} \Longrightarrow E \cap F \in
\mathcal{E}$
\item[3.]  $E \in \mathcal{E} \Longrightarrow E^c = E_1 \cup \dots
\cup E_k$ where $E_j \in \mathcal{E}$ and disjoint.
\end{itemize}


\begin{prop}  The collection of all finite union of disjoint member of
$\mathcal{E}$ is an algebra.
\end{prop}

\section{Measures}

\vspace{.15cm}
\noindent Let $X$ be a set and $\mathcal{M}$ a $\sigma$- algebra on $X$.  A
measure on $\mathcal{M}$ is a function
\[
\mu: \mathcal{M} \longrightarrow [0, \infty]
\] such that

\begin{itemize}
\item[1.]  $\mu (\emptyset) = 0$
\item[2.]  $\{E_j\}^\infty_{j=1} \subset \mathcal{M}$, disjoint
$\Longrightarrow \mu \left ( \overset{\infty}{\underset{j=1}{\cup}}
 E_j\right ) =
\sum^\infty_{j=1} \mu (E_j).$
\end{itemize} Property two is called {\bf countable additivity}.  $\mu$
is called {\bf finitely additive} if
\[ 
\mathbf{2}.') \qquad E_1, \dots , E_n\in M \mbox{ and disjoint }
\Longrightarrow \mu \left (\overset{n}{\underset{j=1}{\cup}}  E_j
\right ) = \sum^n_{j=1}
\mu(E_j) \] The sets $E \in \mathcal{M}$ are called $(\mu)$ {\bf
measurable sets}.  $(X,
\mathcal{M})$ is called {\bf measurable space} and $(X, \mathcal{M},
\mu)$ {\bf measure space}.


\vspace{.25cm}
\noindent {\bf Probability Terminology.}  Experiments involving
randomness are modeled by a triplet $(\Omega, \mathcal{F}, P)$, where
\begin{itemize}
\item  $\Omega$ is the {\bf sample space} $ \omega \in
\Omega$ are called {\bf sample points}.
\item $\mathcal{F}$ is a $\sigma$-algebra called family of {\bf
events};  i.e. event = measurable set.
\item$P: \mathcal{F} \longrightarrow [0, 1]$ is a measure with
\[ P(\Omega) = 1
\]
\end{itemize} called {\bf Probability measure.}

\begin{example}  For the experiment of tossing a coin twice, we can take
\begin{eqnarray*}
\Omega &=& \{HH, HT, TH, TT\}\\
\mathcal{F} &=& \mathcal{P} (\Omega)\\ P \{HH\} &=& P\{HT\} = P\{TH\} =
P\{TT\} = \frac{1}{4}.
\end{eqnarray*}
\end{example} Back to general measures.  If $\mu(X) < \infty$ then $\mu$ is
called {\bf finite}. If there  exist $\{E_j\}^\infty_{j=1}$ such that
\[
\cup E_j = X \mbox{ and } \mu (E_j) < \infty
\] then $\mu$ is called $ \sigma$ - {\bf finite}.  If for any $E
\in \mathcal{M}$ with $\mu(E) = \infty$ there exists $F \subset E $ such
that $0 < \mu(F) < \infty$ then $\mu$ is said {\bf semifinite}.

\begin{examp}  Let $f: X \longrightarrow [0, \infty)$ be a function. And
define
$\mu: \mathcal{P}(X) \longrightarrow [0, \infty]$ by
\[
\mu(E) = \sum_{x \in E} f(x) \doteq  \sup \{\sum_{x \in F} f(x): F \mbox{
finite}
\subset E\}.
\] Then

\begin{itemize}
\item[0.]  If $E = \{x_j \}^\infty_{j=1}$ then $\mu(E) = \sum^\infty_{j=1} f
(x_j)$
 \item[1.]  $\mu$ is a measure.
\item[2.] $\mu$ is semifinite iff $f(x) < \infty, \ \forall x \in X$.
\item[3.]  $\mu$ is $\sigma-$finite  if  $\mu$  is semifinite and $\{x: f(x) >
0\}$ is countable.
\end{itemize}
\end{examp} If $f(x) = 1, \ \forall x \in X$ then $\mu$ is called {\bf
counting} measure.  If there exist $x_0 \in X$ such that
\[  f(x) = \left \{ \begin{array}{ll} 1, &\mbox{ if } x = x_0\\ 0, &\mbox{ if } x
\ne x_0,
\end{array} \right .
\] then $\mu $ is called {\bf point mass} or {\bf Dirac} measure and we
have
\[
\mu(E) = \left \{ \begin{array}{ll} 1, &\mbox{ if } x_0 \in E\\ 0, &\mbox{ if }
x_0 \notin E.
\end{array} \right .
\]
\begin{example}  Let $X$ be uncountable set,
\[
\mathcal{M} = \{E \subset X: E \mbox{ or } E^c \mbox{ is countable}\}.
\] and
\[
\mu(E) = \left \{ \begin{array}{ll} 1, &\mbox{if }  E^c \mbox{ is countable.}\\
0, &\mbox{if } E \mbox{ is countable.}
\end{array} \right .
\] Then $\mu$ is a measure.
\end{example}

\begin{example}  Let $X$ be infinite, $\mathcal{M} =
\mathcal{P} (X)$ and
 \[
\mu(E) = \left \{ \begin{array}{ll} 0, &\mbox{if } E \mbox{ is finite.}\\
\infty, &\mbox{if } E \mbox{ is infinite.}
\end{array}
\right .
\] Then $\mu$ is a finitely additive measure but not a measure.
\end{example}

\begin{theorem} [Properties of Measures]   Let $(X,
\mathcal{M}, \mu)$ a measure space and $E_j \in \mathcal{M}$. Then the
following hold.
\begin{itemize}
\item[1.]{\bf Monotonicity.}  $E_1 \subset E_2 \Longrightarrow
\mu(E_1) \le \mu(E_2)$.  If in addition  $\mu(E_1)<\infty$ then
 $\mu(E_2 -E_1) =\mu(E_2) -\mu(E_1)
$
\item[2.] {\bf Subadditivity:}  $\displaystyle{\mu
\left (\overset{\infty}{\underset{j=1}{\cup}}  E_j\right ) \le
\sum^\infty_{j=1}
\mu(E_j)}$
\item[3.]  Continuity from below.
\[ E_j \subset E_{j+1} \Longrightarrow \mu \left (
\overset{\infty}{\underset{j=1}{\cup}}  E_j\right ) = \lim_{j \to \infty}
\mu(E_j)
\]
\item[4.] {\bf Continuity from above.}
\[ E_{j+1} \subset E_j, \mu(E_{j_0}) < \infty \mbox{ for some } j_0
\Longrightarrow\mu\left (
\overset{\infty}{\underset{j=1}{\cap}}  E_j
\right ) = \lim_{j \to \infty} \mu(E_j)
\]
\end{itemize}
\end{theorem}

\vspace{.15cm}
\noindent {\em Proof:}
\begin{itemize}
\item[1.]  We have $E_2 = E_1 \cup (E_2 - E_1)$ and $E_1, E_2 - E_1$
disjoint.  Therefore
\[
\mu(E_2) = \mu(E_1) + \mu(E_2 - E_1) \ge \mu (E_1)
\]
\item[2.]  Let
\begin{eqnarray*} F_1 &=& E_1\\ F_2 &=& E_2 - E_1 \subset E_2\\ &\
&\dots\\ F_j &=& E_j - \overset{j-1}{\underset{k=1}{\cup}}  E_k \subset
E_j\\ &\ &\dots
\end{eqnarray*} Then $\{F_j\}$ are disjoint with $\cup F_j = \cup E_j$.
Therefore
\[
\mu(\cup E_j) = \mu(\cup F_j) = \sum \mu (F_j) \le \sum \mu (E_j)
\]
\item[3.]  We have
\[
\cup E_j = E_1 \cup (E_2 - E_1) \cup (E_3 - E_2) \cup \dots
\cup (E_j - E_{j-1}) \cup \dots \cup
\] and the second union is disjoint.  Thus
\begin{eqnarray*}
\mu(\cup E_j) &=& \sum^\infty_{j=1} \mu(E_j - E_{j-1}),
\mbox{ with } E_0 = \emptyset\\ &=& \lim_{j \to \infty} \sum^j_{k=1}
\mu(E_k - E_{k-1})\\ &=& \lim_{j \to \infty} \sum^j_{k=1} \mu (E_k) -
\mu(E_{k-1}) = \lim_{j \to \infty} \mu(E_j)
\end{eqnarray*}
\item[4.]  We have \[ E_{j_0} - \overset{\infty}{\underset{j=1}{\cap}}  E_j =
\left (E_{j_0} - E_{j_0 + 1}
\right ) \cup \left ( E_{j_0} - E_{j_0 + 2}\right ) \cup \dots
\] If we let
\[ F_k = E_{j_0} - E_{j_0 + k}
\] then we have $F_1 \subset F_2 \subset \dots$ and
\[ E_{j_0} - \overset{\infty}{\underset{j=1}{\cap}}  E_j = \cup F_k
\] therefore applying (3) we obtain
\begin{eqnarray*}
\mu(E_{j_0} - \cap E_j) &=& \lim_{k \to \infty} \mu(F_k)\\ &=& \lim_{k
\to \infty} \mu (E_{j_0} - E_{j_0 + k})\\ &=& \lim_{k \to \infty}
[\mu(E_{j_0}) - \mu(E_{j_0 + k}]\end{eqnarray*} or
\[\mu(E_{j_0}) - \mu(\cap E_j) = \mu(E_{j_0}) - \lim_{j \to
\infty} \mu(E_j)
\]
\end{itemize}

If $E \in \mathcal{M}$ is such that $\mu(E) = 0$ then $E$ is said {\bf
null} (or negligible).  A statement about points $x \in X$ holds {\bf 
almost everywhere} (a.e.) if it holds for all $x \in X - E$, where $E$ is some
null set.  A measure $\mu$ is {\bf complete} if $\mathcal{M}$ contains
all subsets of all null sets.  If $E \in \mathcal{M}$ with $\mu(E) = 0$ then
\[ F \subset E \Longrightarrow F \in \mathcal{M}.
\]
\begin{example}  If $\mathcal{M} = \{\emptyset, X\}$ with
$\mu(X) = 0$ then
$\mu$ is not complete.
\end{example}

\begin{example} (later) ``Lebesgue measure'' on Borel sets is not
complete.
\end{example}
\begin{theorem}  Let $(X, \mathcal{M}, \mu)$ be  a measure space, and
\[
\mathcal{N} = \{N \in \mathcal{M}: \mu(N) = 0\} \mbox{ \textit{null sets}}.
\] Define
\[
\bar{\mathcal{M}} = \{E \cup F: E \in \mathcal{M}, F \subset N\,
\mbox{\textit{  for some }} N \in \mathcal{N}\}
\] and
$
\bar{\mu}: \bar{\mathcal{M}} \longrightarrow [0, \infty]
$ 
with
\[
\bar \mu(E \cup F) = \mu(E).
\] Then $\bar{ \mathcal{M}}$ is a $\sigma$-algebra and $\bar \mu$ is a well
defined complete measure on $\bar{\mathcal{M}}$ extending $\mu$.  In
addition, $\bar \mu$ is the unique complete extension of $\mu$ on 
$\bar{\mathcal{M}}$.
\end{theorem}
\textit{Proof:}  Let $\{E_j \cup F_j \} \in \bar{\mathcal{M}}$.  Then
\[
\cup(E_j \cup F_j) = (\cup E_j) \cup (\cup F_j) \in \bar{\mathcal{M}},
\] since $\cup E_j \in \mathcal{M}$ and $\cup F_j \subset \cup N_j$ with
$\mu(\cup N_j) \le \sum \mu(N_j) = 0.$
  Next let $E \cup F \in \bar{\mathcal{M}}$.  Since
\[ E \cup F = E \cup (F -E)
\] with
\[ F - E \subset N - E
\] we may assume that $E \cap N = \emptyset$.  Then
\[ E \cup F = (E \cup N) \cap (F \cup N^c).
\] Therefore
\[ (E \cap F)^c = (E \cup N)^c \cup (F \cup N^c)^c = (E^c \cap N^c) \cup (N -
F).
\] Thus $\bar{\mathcal{M}}$ is a $\sigma$-algebra.


\bigskip
\noindent
\underline{{\bf  $\bar \mu$ is well defined.}}  Let $E_1 \cup F_1 = E_2
\cup F_2$ with $E_j \in \mathcal{M}$ and $F_j \subset N_j, N_j
\in \mathcal{M}$ with $\mu(N_j)=0$.  Then since $E_1 \subset E_2 \cup
N_2$ we have $\mu(E_1) \le \mu(E_2)$.  Similarly
$\mu(E_2) \le \mu(E_1)$.  Therefore $\bar \mu(E_1 \cup F_1) = \bar
\mu(E_2 \cup F_2).$

\bigskip
\noindent
{\bf Countable additivity.}  Let $\{E_j \cup F_j\}$ disjoint sets in 
$\bar{\mathcal{M}}$.  Then
\begin{eqnarray*}
\bar \mu(\cup (E_j \cup F_j)) &=& \bar \mu((\cup E_j) \cup (\cup F_j))\\
&=& \mu(\cup E_j) = \sum \mu(E_j) = \sum \bar\mu(E_j \cup F_j)
\end{eqnarray*}

\bigskip
\noindent
{\bf Completeness.}  Let $G \subset E \cup F$, with $\bar \mu(E
\cup F) = \mu(E)=0$.  Then $G \subset E \cup N$ with
$\mu(E\cup N) \le \mu(E) + \mu(N) = 0$.  Thus  
$G \in \bar{\mathcal{M}}$ and $\bar \mu$ is complete.

\bigskip
\noindent
{\bf Uniqueness.}  Assume that $\nu$ is another complete extension of
$\mu$ to $\bar{\mathcal{M}}$.  Then for $E \cup F
\in \bar{\mathcal{M}}$ we have
\[
\nu(E \cup F) \le \nu (E \cup N) \le \nu(E) + \nu(N) = \nu(E) =
\mu(E) = \bar \mu(E \cup F)
\] and
\[
\bar \mu(E \cup F) = \mu(E) = \nu(E) \le \nu(E \cup F).
\]



\section{Outer Measures} {\large\bf Motivation.}  The natural way of
generalizing the notion of the length from intervals to more general
subsets of $\mathbb{R}$ is the following.  Let
$\mathcal{E}$ be the family of all intervals in $\mathbb{R}$; i.e.
\[
\mathcal{E} = \{E = (a, b): a, b \in \mathbb{R}\}
\] The ``generalized length'' of a set $A \subset \mathbb{R}$ should be
defined by
\[
\ell^\ast (A) = \inf \left \{\sum^\infty_{j=1} \ell (E_j): E_j \in \mathcal{E}
\mbox{ and } A \subset \cup E_j \right \}
\] Then one would try to characterize the family $\mathcal{L}$ of sets in
$\mathbb{R}$ on which $\ell^\ast$ is additive.  i.e.
\[ A_j \in \mathcal{L} \mbox{ and disjoint } \Longrightarrow \ell^\ast (\cup
A_j) =
\sum \ell^\ast (A_j). 
\] In this way one obtains the Lebesque measurable sets $\mathcal{L}$ in
$\mathbb{R}$ and the Lebesque measure on $m$ on $\mathcal{L}$. Since for
this construction the rich structure of $\mathbb{R}$ is not needed,  we
shall proceed at a more abstract level to construct more abstract
measures that are useful in probability and other situations.

\medskip
\noindent
\begin{definition}  A function $\mu^*: \mathcal{P}(X) \longrightarrow [0,
\infty]$  is  called an {\bf  outer measure} if
\begin{itemize}
\item[1.]  $\mu^\ast (\emptyset ) = 0$
\item[2.]  $A \subset B \Longrightarrow \mu^* (A) \le \mu^* (B)$
\item[3.]  $\mu^*
\left ( \overset{\infty}{\underset{j=1}{\cup}}  A_j \right ) \le
\sum\limits^\infty_{j=1} \mu^* (A_j), \ A_j
\in \mathcal{P} (X).$
\end{itemize}
\end{definition}  The starting point is a set $X$, a family $\mathcal{E}
 \subset \mathcal{P}(X)$, and a function
$\rho: \mathcal{E} \longrightarrow [0, \infty]$ such that

\begin{itemize}
\item[1.] $\emptyset, X \in \mathcal{E}$
\item[2.]  $\rho (\emptyset) = 0.$
\end{itemize} Then for any $A \subset X$ we define $\mu^* (A)$ by
\[
\mu^*(A) = \inf \left \{\sum^\infty_{j=1} \rho(E_j):  E_j \in \mathcal{E}
\mbox{ and } A \subset \cup E_j\right \}
\]

\begin{prop} $\mu^*: \mathcal{P}(X) \longrightarrow [0,
\infty]$ is an outer measure
\end{prop} {\bf Proof:}   (1.)  $\mu^* (\emptyset) = 0$ since $\emptyset
\subset \cup A_j, \ A_j =
\emptyset$.

\smallskip
\noindent
 (2.) If $A \subset B$ then
\[
\left \{ \sum \rho(E_j): E_j \in \mathcal{E} \mbox{ and } A \subset \cup E_j
\right \}
\supseteq
\left \{ \sum \rho(E_j): E_j \in \mathcal{E} \mbox{ and } B \subset \cup E_j
\right \}.
\] Therefore $\mu^*(A) \le \mu^*(B)$.

\smallskip
\noindent
 (3.) Let $A_j \in \mathcal{P}(X)$.  If $\mu^*(A_j) = \infty$ for some $j$
then (3) holds with equality of the form $\infty = \infty$.  Thus we assume
$\mu^*(A_j) <
\infty$ for all $j$.  Then by the definition of $\inf$ for any fixed 
$\varepsilon>0$ there exist
$\{E_j^k\} \subset \mathcal{E}$,  
$A_j\subset{\underset{k=1}{\cup}}  E_j^k $  such that

\[
\sum^\infty_{k=1} \rho(E_j^k) \le \mu^* (A_j) + \frac{\varepsilon}{2^j}.
\] Since $\{E^k_j\}^\infty_{j, k=1} $ countable $\subset \mathcal{E}$ and 
$\overset{\infty}{\underset{j=1}{\cup}} A_j \subset
\overset{\infty}{\underset{j, k=1}{\cup}}  E^k_j $ we have that 
\begin{eqnarray*}
\mu^*(\cup A_j) &\le & \sum^\infty_{j, k =1} \rho(E_j^k) =
\sum^\infty_{j=1}
\sum^\infty_{k =1} \rho(E^k_j)\\ &\le & \sum^\infty_{j=1} \left [ \mu^*
(A_j) + \frac{\varepsilon}{2^j} \right ] =
\sum^\infty_{j=1} \mu^*(A_j) + \varepsilon.
\end{eqnarray*} Thus (3) follows by letting $\varepsilon \longrightarrow
0$.

\begin{definition} (Caratheodory)  A set $A \subset X$ is said
$\mu^*$-measurable if
\[ (\ast) \hskip 1. in\mu^* (E) = \mu^*(E \cap A) + \mu^*(E \cap A^c), \
\forall E
\subset X
\]
\end{definition} This definition is {\bf brilliant} since it is not intuitive
at first (deep idea). It is very applicable. And extremely useful since the  
following theorem holds.

\medskip
\noindent
\begin{theorem} (Caratheodory)  If $\mu^*$ is an outer measure then the
following  hold.
\begin{itemize} 
\item[a.]  The $\mu^*-$ measurable sets,
$\mathcal{M},  $ is a $\sigma-$algebra.
\item[b.] $\mu^*: \mathcal{M} \longrightarrow [0, \infty] $ is a complete
measure.
\end{itemize}
\end{theorem}
{\bf  Intuition for real line.}  Take $E = (a, b)$ and $A \subset (a, b)$.
Then Caratheodory's  definition gives
\[
 b - a = \mu^*(A) + \mu^*(A^c).
\] which says that $A$ is not a ``very complicated" set.

\medskip
\noindent
\textit{Remarks.}
\begin{itemize}
\item[1.]  Since $E \subset (E \cap A) \cup (E \cap A^c)$ we have
\[
\mu^* (E) \le \mu^* (E \cap A) + \mu^*(E \cap A^c).
\] Therefore to show that $A$ is measurable it suffices to show only that 
\[
\mu^* (E) \ge \mu^* (E \cap A) + \mu^*(E \cap A^c)
\]
\item[2.]  If $\mu^* (E) = \infty$ then the last  inequality holds. Thus it
suffices to show
\[
\mu^* (E) \ge \mu^* (E \cap A) + \mu^*(E \cap A^c), \ \forall E \subset X, \
\mu^* (E) < \infty.
\]
\end{itemize}

\medskip
\noindent
\textit{Proof of Theorem.}  We give it in several steps.

\medskip
\noindent
$\bullet \ \ \mu^* (A) = 0 \Longrightarrow A \in \mathcal{M}$.   (Thus
$\mu^*$ is complete).  In fact, for any $E \subset X$ we have

\begin{eqnarray*} &E \supset E \cap A^c \Longrightarrow \mu^* (E) \ge
\mu^* (E \cap A^c)\\ &A \supset E \cap A \Longrightarrow 0 = \mu^* (A)
\ge \mu^*(E
\cap A) = 0.
\end{eqnarray*} Therefore
\[
\mu^* (E) \ge \mu^*(E \cap A^c) + \mu^*(E \cap A),
\] which implies that $A \in \mathcal{M}$.


\smallskip
\noindent
$\bullet \ \ A \in \mathcal{M} \Longrightarrow A^c \in \mathcal{M}$ since
Caratheodory's definition is symmetric with respect to $A$ and $A^c$.

\smallskip
\noindent
$\bullet$ \ \ Also observe that both $\emptyset$ and $X$ are in
$\mathcal{M}$

\smallskip
\noindent
$\bullet \ \ A, B \in \mathcal{M} \Longrightarrow A \cup B \in \mathcal{M}$
and therefore $A_1, \dots, A_k \in \mathcal{M} \Longrightarrow A_1 \cup
\dots \cup A_K \in \mathcal{M}$.

\smallskip
\noindent
 In fact, for any $E \subset X$ we have
\begin{eqnarray*}
\mu^* (E) &\overset{A \in \mathcal{M}}{=}&  \mu^*(E \cap A) + \mu^*(E
\cap A^c)
\\
&\overset{ B \in \mathcal{M}}{=}& \mu^* (E \cap A \cap B) + \mu^* (E
\cap A \cap B^c)\\ 
&+& \mu^*(E \cap A^c \cap B) + \mu^* (E \cap A^c \cap
B^c)\\
&\ge & \mu^*(E \cap (A \cup B)) + \mu^* (E \cap (A \cup B)^c),
\end{eqnarray*} 
since 
$$ (E \cap A \cap B)\cup (E \cap A \cap B^c) \cup (E \cap
A^c \cap B) =$$
$$ E \cap [(A \cap B) \cup (A \cap B^c) \cup (A^c \cap B)]
=E \cap (A \cup B).
$$
Therefore $A \cup B \in \mathcal{M}$. This shows that
$\mathcal{M}$  is an algebra.


\smallskip
\noindent
$\bullet \ \ A, B \in \mathcal{M}, A \cap B = \emptyset \Longrightarrow 
\mu^* (A
\cup B) = \mu^*(A) + \mu^*(B).$  Thus $A_1, \dots A_n \in \mathcal{M}$ and
disjoint
$\Longrightarrow \mu^* \left (\overset{n}{\underset{j=1}{\cup}} 
A_j\right )=
\sum\limits^n_{j=1}
\mu(A_j)$.  If we apply the Caratheodory's definition with $E = A \cup B$,
and $A
\in
\mathcal{M}$ we obtain
\[
\mu^* (A \cup B) = \mu^*((A \cup B) \cap A) + \mu^*((A \cup B) \cap A^c)
= \mu^* (A) + \mu^* (B).
\] Thus $\mathcal{M}$ is an {\bf  algebra} and $\mu: \mathcal{M}
\longrightarrow [0,
\infty)$ is {\bf  finitely} additive.

\smallskip
\noindent
$\bullet$  Next let $\{A_j\}^\infty_{j=1} \subset \mathcal{M}$ and disjoint.
We shall show that
$\overset{\infty}{\underset{j=1}{\cup}} A_j \in \mathcal{M}$ and additivity.
Let $B_n = \overset{n}{\underset{j=1}{\cup}} A_j$ and 
$B = \overset{\infty}{\underset{j=1}{\cup}}A_j$. 
 For any $E \subset X$ we
have
\begin{eqnarray*}
\mu^* (E \cap B_n) &\overset{A_n \in \mathcal{M}}{=}& \mu^* (E \cap B_n
\cap A_n) + \mu^* (E \cap B_n \cap A^c_n)\\ &=& \mu^* (E \cap A_n) +
\mu^*(E \cap B_{n-1})\\ &=& \mu^* (E \cap A_n) + \mu^*(E \cap A_{n-1}) +
\mu^* (E \cap B_{n-2})\\ &\dots&\\
\end{eqnarray*} Thus
\begin{equation*}
 \qquad \mu^* (E\cap B_n) = \sum^n_{j=1} \mu^* (E \cap A_j), \ A_j \in
\mathcal{M}, E \subset X.\tag{$\ast\ast$}
\end{equation*}
Using the last equality we obtain
\begin{eqnarray*}
\mu^* (E) &\overset{B_n \in \mathcal{M}}{=}& \mu^* (E \cap B_n) + \mu^*
(E \cap B^c_n)\\
 &\overset{(\ast\ast)}{=}& \sum^n_{j=1} \mu^* (E \cap A_j) + \mu^* (E
\cap B_n^c)\\ &\ge& \sum^n_{j=1} \mu^* (E \cap A_j) + \mu^* (E \cap B^c)
\end{eqnarray*} By letting $n \to \infty$ we obtain
\[\mu^* (E) \ge \sum^\infty_{j=1} \mu^* (E \cap A_j) + \mu^* (E
\cap  B^c)
\]



\[
\overset{ \mbox{subad.}}{\ge} \mu^* \left (
\overset{\infty}{\underset{j=1}{\cup}}  (E \cap A_j)\right ) + \mu^* (E \cap
B^c)
\]
\[ = \mu^* \left ( E \cap \left [\overset{\infty}{\underset{j=1}{\cup}} 
A_j\right ]
\right ) + \mu^*
\left ( E \cap \left [ \overset{\infty}{\underset{j=1}{\cup}}  A_j\right ]^c
\right )
\ge \mu^*(E).
\] 
Therefore $\cup A_j \in \mathcal{M}$.  If we apply the last equality with
$E = \overset{\infty}{\underset{j=1}{\cup}}A_j$ we obtain
\[
\mu^* (\overset{\infty}{\underset{j=1}{\cup}}A_j) = \sum^\infty_{j=1}
\mu^*(A_j).
\]  
This completes the proof of the theorem.

\medskip
\noindent 
Next we shall apply the above construction of a measure
 with the first building block being a premeasure. Given
 an algebra $\mathcal{A}$ on $X$, a {\bf  premeasure} is a function  
$\mu_o:
\mathcal{A} \longrightarrow [0, \infty]$  such that
\begin{itemize}
\item[1. ] $\mu_o(\emptyset) = 0$
\item[2. ] $\displaystyle{ A_j \in \mathcal{A}, \mbox{disjoint, and }
\overset{\infty}{\underset{j=1}{\cup}}  A_j \in \mathcal{A}
\Longrightarrow
\mu_o(\overset{\infty}{\underset{j=1}{\cup}}  A_j) =\sum^\infty_{j=1}
\mu_o(A_j).} $
\end{itemize} By using the premeasure $\mu_o$ we define $\mu^*:
\mathcal{P}(X)
\longrightarrow [0,
\infty]$, as before, by
\begin{equation*}
\mu^* (E) = \inf \left \{ \sum^\infty_{j =1} \mu_o(A_j): A_j \in
\mathcal{A}, \ E
\subset \overset{\infty}{\underset{j=1}{\cup}}  A_j \right \}.
\end{equation*} Then by the last proposition $\mu^*$ is an outer measure. 
In addition we have:


\begin{prop}  If $\mu_o$ is a premeasure on an algebra
$\mathcal{A}$ and
$\mu^*$ is the outer measure defined by $\mu_o$ then:
\begin{itemize}
\item[(a)]  $\mu^*\vert_{\mathcal{A}} = \mu_o$
\item[(b)] $A \in \mathcal{A} \Longrightarrow A $ is $\mu^*$-measurable.
\end{itemize}
\end{prop}
\noindent 
{\bf Proof.}  (a.)  Let $A \in \mathcal{A}$.  If we let $A_1 = A$ and
$A_2 = A_3 =
\dots = \emptyset$ then $A \subset \cup A_j$ and $\sum\mu_o(A_j) =
\mu_o(A)$.   Therefore

\begin{equation*}
 \mu^* (A) \le \mu_o(A)
\end{equation*} To prove the reverse inequality, we let $\{A_j\} \subset
\mathcal{A}$ such that $ A
\subset \overset{\infty}{\underset{j=1}{\cup}}  A_j$.  If we define
\[ B_j = A \cap \left ( A_j - \overset{j-1}{\underset{k=1}{\cup}} A_k
\right )
\subset A_j
\] then $\{B_j\} \subset \mathcal{A}$, disjoint and $A =
\overset{\infty}{\underset{j=1}{\cup}}  B_j$.  Therefore

\[
\mu_o (A) = \mu \left (\overset{\infty}{\underset{j=1}{\cup}}  B_j \right )
=
\sum^\infty_{j =1} \mu_o(B_j) \le \sum^\infty_{j=1} \mu_o (A_j).
\] By taking infimum we obtain

\begin{equation*} \mu_o(A) \le \mu^* (A)
\end{equation*}
 which proves (a).

\medskip
\noindent
 (b.)  Let $A \in \mathcal{A}, E \subset X$, with  $\mu^* (E)<\infty$.  Then for
any
$\varepsilon > 0$ there exist $\{A_j\} \subset \mathcal{A}$ such that $E
\subset
\overset{\infty}{\underset{j=1}{\cup}}  A_j$ and

\begin{eqnarray*}
\mu^* (E) + \varepsilon &\ge& \sum^\infty_{j =1} \mu_o(A_j)\\
\mbox{(finite addit.)} &=& \sum^\infty_{j=1} \mu_o(A_j \cap A) +
\sum^\infty_{j =1}
\mu_o(A_j \cap A^c)\\ &\ge& \mu^*[(\cup A_j) \cap A] + \mu^*[(\cup
A_j)
\cap A^c].\\ &\ge& \mu^*(E \cap A) + \mu^* (E \cap A^c)
\end{eqnarray*} By letting $\varepsilon \longrightarrow 0 $ we obtain
$\mu^* (E) \ge
\mu^* (E \cap A) + \mu^*(E \cap A^c)$ which implies that $A$ is a
$\mu^*$-measurable set.  This completes the  proof the proposition.


\begin{theorem}  Let $\mathcal{A}$ be an algebra on $X$, and
$\mu_o$ a premeasure on $\mathcal{A}$.  Denote by $\mathcal{M}$ the
$\sigma$-algebra generated by
$\mathcal{A}$, and by $\mathcal{M}^*$ the $\sigma$- algebra of the
$\mu^*$-measurable sets.  Then the following hold.
\begin{itemize}
\item[a)]  $\mathcal{M} \subset \mathcal{M}^*$
\item[b)]  $\mu = \mu^*\vert_\mathcal{M} $ is a measure
extending
$\mu_o$ from $\mathcal{A}$ into $\mathcal{M} = \mathcal{M}
(\mathcal{A}).$
\item[c)]  If $\nu$ is another extension of $\mu_o$ from
$\mathcal{A}$ to
$\mathcal{M}$ then
\[\nu(E) \le \mu(E), \ \forall E \in \mathcal{M},\]
with ``='' valid if $\mu (E) < \infty$.
\item[d)]  If $\mu_o$ is $\sigma$-finite then $\mu$ is the unique
extension of the premeasure $\mu_o$ on $\mathcal{A}$ as measure on
$\mathcal{M}$.
\end{itemize}
\end{theorem}
{\bf Proof.}  Parts (a) and (b) follow from last proposition and
Caratheodory's theorem.

\smallskip
\noindent (c). 
 Let
$E \in \mathcal{M}$ and $\{A_j\} \subset \mathcal{A}$ such that $E \subset
\overset{\infty}{\underset{j=1}{\cup}}  A_j$.  Then we have

\[
\nu(E) \le \sum^\infty_{j=1}\nu(A_j) = \sum^\infty_{j=1} \mu_o (A_j).
\] Therefore
\begin{equation*}
\nu (E) \le \mu(E), \ \forall E \in \mathcal{M}.
\end{equation*} Next observe that if $A =
\overset{\infty}{\underset{j=1}{\cup}} A_j, A_j \in
\mathcal{A}$ then
\[\nu(A) = \lim_{k \to \infty} \nu \left ( \overset{k}{\underset{j=1}{\cup}} 
A_j
\right ) =
\lim_{k \to
\infty} \mu \left ( \overset{k}{\underset{j=1}{\cup}}  A_j \right ) =
\mu(A)
\] Therefore if $\mu (E) < \infty$ then for any $\varepsilon > 0 $ there
are
$\{A_j\}
\subset \mathcal{A}$ such that
\[ E \subset \overset{\infty}{\underset{j=1}{\cup}}  A_j \mbox{ and }
\sum^\infty_{j=1} \mu_o(A_j) \le
\mu(E) + \varepsilon.
\] If we let $A = \overset{\infty}{\underset{j=1}{\cup}}A_j$ then we
obtain
\[ E \subset A \mbox{ and } \mu(A) \le  \mu(E) + \varepsilon
\] which implies
\[
\mu(A - E) \le \varepsilon.
\] Now we have
\begin{eqnarray*}
\mu(E) \le  \mu(A) &=& \nu(A)\\ &=& \nu(E) + \nu(A - E)\\ &\le&
\nu(E) + \mu(A - E)\\ &\le& \nu (E) + \varepsilon
\end{eqnarray*} By letting $\varepsilon \longrightarrow 0$ we obtain
\[
\mu (E) \le \nu(E) \mbox{ if } \mu(E) < \infty.
\] This completes the proof of (c).

\medskip
\noindent (d).  Assume that $\mu_o$ is $\sigma$-finite; i.e.
\[ X = \overset{\infty}{\underset{j=1}{\cup}} A_j, \ A_j \in \mathcal{A},
\mbox{ and }
\mu_o(A_j) < \infty.
\] We may also assume that $A_j$ are disjoint.Then for any $E \in
\mathcal{M}$ we have
\begin{eqnarray*}
 \mu(E) &=& \mu \left ( \overset{\infty}{\underset{j=1}{\cup}}  (E
\cap A_j) \right ) =
\sum^\infty_{j=1} \mu(E \cap A_j)\\ &=& \sum^\infty_{j=1} \nu(E
\cap A_j) = \nu \left (
\overset{\infty}{\underset{j=1}{\cup}}  (E
\cap A_j)
\right ) = \nu(E).
\end{eqnarray*} This completes the proof of the Theorem.

\section{Borel Measures}
 In the last theorem, we have seen how measures are
constructed from premeasures.  Next we shall construct measures on
$\mathbb{R}$.  We start with the family of all the left-open, right-closed
intervals (half-open)
\[
\mathcal{E}_h = \left \{ (a, b]: a, b \in \mathbb{R}, a < b \right \} \cup
\left \{ (-\infty, b], (a, \infty); a, b \in \mathbb{R} \right \} \cup \left \{
\emptyset\right \}.
\] Then $\mathcal{E}_h$ is an elementary family in $\mathbb{R}$.  Recall
that a collection
$\mathcal{E}$ of subsets of a set $X$ is said an
{\bf  elementary family} if

\begin{itemize}
\item[1)]  $\emptyset \in \mathcal{E}$
\item[2)]  $E, F \in \mathcal{E} \Longrightarrow E \cap F \in \mathcal{E}$
\item[3)]  $E \in \mathcal{E} \Longrightarrow E^c = \bigcup\limits^k_{j=1}
E_j, \ E_j \in\mathcal{E}$ and disjoint
\end{itemize}

In fact (1) and (2) hold, and (3) also holds since
\begin{eqnarray*} (a, b]^c &=& (- \infty, a] \cup (b, \infty)\\ (- \infty, b]^c
&=& (b, \infty)\\ (a, \infty)^c &=& (- \infty, a]\\
\emptyset^c &=& (-\infty, \infty) = (- \infty, a] \cup (a, \infty)
\end{eqnarray*}
 Next we shall need the following:

\medskip
\noindent
\begin{prop} Let $\mathcal{A}_h$ be defined by
\[
\mathcal{A}_h = \left \{A \subset \mathbb{R}: A =
\underset{\mbox{finite}}{\cup} E_j, E_j \in
\mathcal{E}_h \mbox{ and disjoint. }\right \}.
\]
Then $\mathcal{A}_h$ is an algebra in $\mathbb{R}$.
\end{prop}
 {\em Proof.}  (Special case of Proposition 1.7 of the Book)

\medskip
\noindent 
Next on $\mathcal{A}=\mathcal{A}_h$ we shall define a
premeasure which is a generalization of the length.  For this we start with
an increasing  ("$\le$") function $F:
\mathbb{R} \to\mathbb{R}$.  Therefore $F$ has both right and left limits at
every point $a\in \mathbb{R}$; i.e.

\begin{eqnarray*} F(a+) &=& \lim_{\stackrel{x \to a}{x > a}} F(x) 
= \inf_{x> a} F(x)\\ F(a-) &=& \lim_{\stackrel{x \to a}{x < a}} F(x) =
\sup_{x < a} F(x)
\\ F(\infty) &=& \sup_{x \in \mathbb{R}} F(x) \in \mathbb{R} \cup
\{\infty\}, \ F (-\infty) =
\inf_{x \in \mathbb{R}} F(x) \in \mathbb{R} \cup \{- \infty\}.
\end{eqnarray*} We shall assume that $F$ is right continuous i.e.
\[F(a+) = F(a), \ \forall a \in \mathbb{R}.
\] A natural definition of a premeasure related to $\mathcal{A}$ and $F$ is
given by the following:


\begin{prop} Let $F: \mathbb{R} \longrightarrow
\mathbb{R}$
  be an increasing right continuous function.   For finite collection
$\{(a_j, b_j]\}_1^n$ of disjoint $h$-intervals let
\[
\mu_F \left ( \overset{n}{\underset{j=1}{\cup}}  (a_j, b_j] \right ) \doteq
\sum^n_{j=1} [F(b_j) - F(a_j)].
\]
Also define $\mu_F(\emptyset) = 0$.  Then $\mu_F$ is a premeasure
on $\mathcal{A}$. (Here we use the notation $\mu_F$ instead of 
$\mu_o$ to make clear that this premeasure is made out of the
function $F$.) 
\end{prop}

\medskip
\noindent {\bf Remarks. }\begin{itemize}
\item[1.] If one of the $a_j$ is $- \infty$ we let $F(a_j) = F(- \infty)$.  If one
of the $b_j$ is $\infty$ then we let $F (b_j) = F(\infty)$.
\item[2.]  If $F(x) = x$ then $\mu =$ length.
\item[3.]  Instead of left-open, right-closed intervals and $F$ right
continuous we could have chosen left closed-right open intervals and left
continuous $F$.
\end{itemize}


\begin{theorem}  Let $F: \mathbb{R} \longrightarrow
\mathbb{R}$ be an increasing and right continuous function.  For  $E
\subset \mathbb{R} $ define
\[
\mu^*_F (E) = \inf \left\{ \sum^\infty_{j=1} [F(b_j) - F(a_j)]: \ E \subset
\overset{\infty}{\underset{j=1}{\cup}} (a_j, b_j] \right \}.
\]
Then the following hold:
\begin{itemize}
\item[(a)]  $\mu^*_F$   is a complete measure on $\mathcal{M}_F
\doteq
\mu^*_F$-measurable sets.
\item[(b)]  $\mu^*_F |_{B_{\mathbb{R}}}$   is the unique measure
extending
$\mu_F$ from $\mathcal{A}$ to the Borel $\sigma$-algebra
$B_\mathbb{R}$.
\item[(c)]    If $G: \mathbb{R} \longrightarrow \mathbb{R}$ is
another increasing and right continuous function, then
\[\mu_F = \mu_G \left ( \Leftrightarrow \mu^*_F = \mu^*_G \right )
\Leftrightarrow F - G =\mbox{\textit{constant}}.
\]
\item[d)]    Let $\mu$ be a measure on $B_{\mathbb{R}}$ which is
finite on bounded Borel sets, and let
\[
 F(x) = \left \{ \begin{array}{ll}
\mu((0, x]), &x > 0\\ 0, &x = 0\\ - \mu((x, 0]), &x < 0. \end{array} \right .
\]
 Then $F$ is increasing, right continuous, and
\[
\mu = \mu^*_F \mbox{ on } B_{\mathbb{R}}.
\]
\item[e)]   If $F(x) = x$ then $B_{\mathbb{R}}$ is strictly
contained in $\mathcal{M}_F$.
\end{itemize}
\end{theorem}
{\bf  Remarks.}
\begin{itemize}
\item[1)]  The complete measure $\mu^*_F$ on $\mathcal{M}_F$, the
$\mu^*_F $- measurable set, is called the {\bf  Lebesgue-Stieltjes
measure} associated to $F$, and is usually denoted by $\mu_F.$
\item[2)]  The {\bf  Lebesgue measure} is the Lebesgue-Stieltjes
measure associated to the function $F(x) = x$.  Then
\[\mu_F((a, b]) = \ell((a, b]) = b -a.
\]
\item[3)]  If $\mu$ is a finite measure on $B_{\mathbb{R}}$ then
\[F(x) \doteq \mu((- \infty, x])
\] is called the {\bf  cumulative distribution function} of $\mu$.  We
have that
$\mu = \mu_F$.
\end{itemize}
{\bf Proof of Proposition} 
 Denote $\mu_F$ by $\mu$.  We shall give
the proof in several steps:

\medskip
\noindent
{\bf  $\bullet \ \mu $ is well defined:}  We need to show that if
$\{I_i\}^n_{i = 1}$ and $\{J_j\}^m_{j=1}$ are two finite families of disjoint
$h$-intervals such that
\[
\overset{n}{\underset{i=1}{\cup}}  I_i =
 \overset{m}{\underset{j=1}{\cup}} J_j
\] 
then
\begin{equation} 
\label{eq:fin-int} 
 \sum^n_{i = 1} \mu(I_i) = \sum^m_{j=1} \mu(J_j).
\end{equation}
For this let us first assume that
\[(a, b] = \overset{n}{\underset{j=1}{\cup}}  (a_j, b_j] \mbox{ disjoint}.
\]
 We must show that
\begin{equation*}
 F(b) - F(a) = \sum^n_{j=1} [F(b_j) - F(a_j)].
\end{equation*} 
By renumbering the $h$-intervals we may assume
\[a = a_1 < b_1 = a_2 < b_2 = \dots < b_n = b.
\] 
But then the last equality is true (cancellations).
 To prove (\ref{eq:fin-int}) we form the disjoint $h$-intervals
\[\left \{ I_i \cap J_j \right \}_{\stackrel{1 \le i \le n}{1 \le j \le m}}.
\] We have
\[\cup I_i = \cup (I_i \cap J_j) = \cup J_j
\] 
Using $I_i = \cup_j (I_i \cap J_j), \ J_j = \cup_i (I_i \cap J_j)$
we have
\begin{eqnarray*}
\mu(\cup I_i) &=& \sum_i \mu(I_i)\\
&=& \sum_{i,j} \mu(I_i \cap J_j)\\ &=& \sum
\mu(J_j) = \mu(\cup J_j).
\end{eqnarray*}
$\bullet \mu$ is {\bf  finitely additive} on $\mathcal{A}$ by its
definition.

\medskip
\noindent
{\bf  $\bullet \ \mu$ is additive on $\mathcal{A}:$} It suffices to show
that if
$\{I_j\}^\infty_{j=1}$ is a sequence of disjoint $h$-intervals such that
$\overset{\infty}{\underset{j=1}{\cup}}  I_j \in \mathcal{A}$ then

\begin{equation}\label{eq:A-addit}
 \mu \left ( \overset{\infty}{\underset{j=1}{\cup}}  I_j\right ) =
\sum^\infty_{j=1}
\mu(I_j).
\end{equation} 
Since
\[\overset{\infty}{\underset{j=1}{\cup}}I_j  =
\overset{n}{\underset{i=1}{\cup}}  (a_i, b_i] \mbox{ disjoint}
\] we write
\[(a_i, b_i] = \overset{\infty}{\underset{j=1}{\cup}}  [(a_i, b_i] \cap I_j]
\] and it suffices to prove (\ref{eq:A-addit}) when
\[ I = \overset{\infty}{\underset{j=1}{\cup}} I_j = (a, b], I_j = (a_j, b_j].
\]
\underline{{\bf  Case $-\infty < a < b < \infty$:}}   Then we write
\[ I = \left ( \overset{n}{\underset{j=1}{\cup}} I_j \right ) \cup \left ( I -
\overset{n}{\underset{j=1}{\cup}}  I_j \right )\] Since $I -
\overset{n}{\underset{j=1}{\cup}}  I_j$ is a finite union of disjoint
$h$-intervals we have
\begin{eqnarray*}
\mu(I) &=& \mu\left ( \overset{n}{\underset{j=1}{\cup}}  I_j \right ) + \mu
\left ( I -
\overset{n}{\underset{j=1}{\cup}}  I_j\right )\\ &\ge& \mu \left (
\overset{n}{\underset{j=1}{\cup}}  I_j \right ) = \sum^n_{j=1}
\mu(I_j)
\end{eqnarray*} By letting $n \to \infty$ we obtain
\begin{equation}\label{eq:A-addit.1}
\mu(I) \ge \sum^\infty_{j=1} \mu(I_j) 
\end{equation} 
To prove the reverse inequality, we shall use the
Heine-Borel Theorem.  The idea is to replace the interval $(a, b]$ by $[a +
\delta, b]$ when both $a, b$ are finite. Thus assume first that $- \infty < a <
b < \infty$.  And fix $\varepsilon > 0$.  Since
$F$ is right continuous there is $\delta = \delta (\varepsilon) > 0$ such that
\[ F(a + \delta) - F(a) < \varepsilon
\] and $\delta_j = \delta_j (\varepsilon) > 0 $ such that
\[ F(b_j + \delta_j) - F(b_j) < \varepsilon 2^{-j}
\]
\noindent 
Since the family $\{(a_j, b_j + \delta_j) \}^\infty_{j=1}$ covers
$[a +
\delta, b]$ by the Heine-Borel Theorem there are finitely many intervals
$(a_j, b_j +
\delta_j)$ covering $[a + \delta, b]$.  After discarding any interval
contained in a larger one and relabeling them, we may assume that $\{(a_j,
b_j+\delta_j)\}^N_{j=1}$ 
satisfy
\begin{eqnarray*} (1) &\qquad&\overset{N}{\underset{j=1}{\cup}}  (a_j,
b_j + \delta_j) \supset [a +
\delta, b]\\ (2) &\qquad& a_1 < a_2 < \dots < a_N\\ (3) &\qquad& b_j +
\delta_j \in (a_{j+1}, b_{j+1} + \delta_{j+1})
\end{eqnarray*} Then we have
\begin{eqnarray*}
\mu(I) &=& F(b) - F(a) = F(b) - F(a + \delta) + (F(a + \delta) - F(a))\\ &\le&
F(b) - F(a + \delta) + \varepsilon\\ &\le& f(b_N + \delta_N) - F(a_1) +
\varepsilon\end{eqnarray*}
\[ = F(b_N + \delta_N) +   [F(a_2) - F(a_1) ]  +\dots + [F(a_N) -
F(a_{N-1})]  - F(a_N) + \varepsilon
\]
\[ = F(b_N + \delta_N) - F(a_N) + \sum^{N-1}_{j=1} [F(a_{j+1}) - F(a_j)] +
\varepsilon
\]
\[
\le F(b_N + \delta_N) - F(a_N) + \sum^{N-1}_{j=1} [F(b_j + \delta_j) -
F(a_j)] +
\varepsilon
\]
\[ = \sum^N_{j=1} \{[F(b_j) - F(a_j)] + [F(b_j + \delta_j) - F(b_j)]
\} +
\varepsilon
\]
\[
\le \sum^N_{j=1} \mu ((a_j, b_j]) + \sum_{j=1}^N \varepsilon 2^{-j} +
\varepsilon
\]
\[
\le \sum^\infty_{j=1} \mu(I_j) + \varepsilon + \varepsilon
\] Therefore for any $\varepsilon > 0 $ we have
\[
\mu(I) \le \sum^\infty_{j=1} \mu(I_j) + 2 \varepsilon
\] By letting $\varepsilon \to 0 $ we obtain
\begin{equation}\label{eq:A-addit.2}
\mu(I) \le \sum^\infty_{j=1} \mu(I_j).
\end{equation}
 By (\ref{eq:A-addit.1}) and (\ref{eq:A-addit.2}) obtain the
additivity when
$-
\infty < a < b <
\infty$.

\medskip
\noindent
$\bullet $  If $a = - \infty$ then we replace the interval $(- \infty, b]$ by
$(-M, b]$ for $M > 0 $ and large, i.e.

\[I^M = (- \infty, b] \cap (-M, b] = \overset{\infty}{\underset{j=1}{\cup}} 
(I_j \cap (-M, b])
\] By replacing $I$ with $I^M$ and $I_j$ with $I^M_j = I_j \cap (-M, b]$ and
applying (2), we obtain
\[ F(b) - F(-M) = \mu(I^M) = \sum^\infty_{j=1} \mu\left ( I^M_j \right ) \le
\sum^\infty_{j=1} \mu (I_j)
\] By letting $M \to \infty$ we obtain (2) with $a = - \infty$.  

\medskip
\noindent
$\bullet $  
If $b =
\infty$ then we write
\[ I^M = (a, \infty) \cap (a, M] = \overset{\infty}{\underset{j=1}{\cup}}  (I_j
\cap (a, M])
\] and by applying (2), we obtain
\[ F(M) - F(a) = \mu (I^M) = \sum^\infty_{j=1} \mu(I^M_j) \le
\sum^\infty_{j=1}
\mu(I_j).
\] Then by letting $M \to \infty$ we obtain ((\ref{eq:A-addit})).  This
completes the proof of the proposition.

\bigskip
\noindent Next we show that the half-open intervals can be replaced with
open intervals.

\begin{lemma} Let $\mu \doteq \mu^\ast_F$, and $\mathcal{M}
\doteq
\mathcal{M}_F$.  Then for any $E \subset \mathcal{M}$
\begin{equation}\label{eq:open-int}
\mu(E) = \inf \left \{ \sum^\infty_{j=1} \mu((a_j, b_j)): \ E \subset
\overset{\infty}{\underset{j=1}{\cup}}  (a_j, b_j) \right \}.
\end{equation}
\end{lemma}
\textit{Proof:}  Denote by $\nu (E)$ the right-hand-side of 
(\ref{eq:open-int}).  
Let $E\subset
\overset{\infty}{\underset{j=1}{\cup}}  (a_j, b_j)$.  Then
by the subadditivity of $\mu$ we have
\[
\mu(E)\le\sum^\infty_{j=1} \mu((a_j, b_j)) .
\] 
Taking infimum gives
\begin{equation}\label{eq:open-int.1}
\mu(E)\le \nu (E) .
\end{equation}
Next we prove the reverse of (\ref{eq:open-int.1}).  Let $\varepsilon >
0$.   We may assume
$\mu(E)<\infty$, since otherwise we have equality. Then by the definition of
$\mu(E)$ as $\inf$ there are $\{(a_j, b_j] \}^\infty_{j=1}$ such that
\begin{equation}\label{eq:h-open.ineq}
 E \subset \overset{\infty}{\underset{j=1}{\cup}}  (a_j, b_j] \mbox{ and
}\sum^\infty_{j=1}
\mu((a_j, b_j]) \le \mu(E) + \varepsilon
\end{equation} 
Since $F$ is right continuous for each $j$, there is
$\delta_j > 0$ such that
\begin{equation}\label{eq:F-ineq}
 F(b_j + \delta_j) - F(b_j) < \varepsilon 2^{-j}
\end{equation} 
Then we have $E \subset
\overset{\infty}{\underset{j=1}{\cup}}  (a_j, b_j +
\delta_j)$ and

\begin{eqnarray*} \nu(E) \le \sum^\infty_{j=1} \mu ((a_j, b_j + \delta_j))
&\le & \sum^\infty_{j=1} \left [ \mu((a_j, b_j]) +
\mu((b_j, b_j + \delta_j] )\right ]\\ &\overset{(\ref{eq:h-open.ineq}) +
(\ref{eq:F-ineq})}{\le}&
\mu(E) +
\varepsilon  + \varepsilon \le \mu(E) + 2
\varepsilon.
\end{eqnarray*} Thus for any $\varepsilon > 0 $ we have

\[\nu(E) \le \mu(E) + 2 \varepsilon.
\] By letting $\varepsilon \to 0$ we obtain

\begin{equation}\label{eq:open-int.2}
\nu(E) \le \mu(E)
\end{equation} 
By (\ref{eq:open-int.1}) and (\ref{eq:open-int.2}) we
obtain (\ref{eq:open-int}). $\square$


\begin{theorem}  If $E \in \mathcal{M}$ then the following hold
\begin{equation}\label{eq:open-approx} 
\mu(E) = \inf \{\mu(U):\ U \mbox{\textit{ open} } \supset
E\}
\end{equation}
and
\begin{equation}\label{eq:compact-approx} 
\mu (E) = \sup \{\mu (K): \ K \mbox{\textit{compact} } \subset E\}.
\end{equation}
\end{theorem}
\textit{Remarks.}  
If $\mu$ is a general measure on $\Bbb R$ then when
(\ref{eq:open-approx}) holds
$\mu$ is called {\bf  outer regular}, and when (\ref{eq:compact-approx}) 
holds $\mu$ is called
{\bf  inner regular}.

\bigskip
\noindent
\textit{Proof:}  If $U$ open $\supset E$ then $\mu(U) \ge \mu(E)$ and
therefore
\begin{equation}\label{eq:open-approx.1}
\mu(E) \le \inf \{\mu(U):  U\mbox{ open  } \supset E \}
\end{equation} 
To prove the reverse inequality, let $\{(a_j,
b_j)\}^\infty_{j=1}$ such that $E
\subset\overset{\infty}{\underset{j=1}{\cup}}  (a_j, b_j)$.  Then $V =
\overset{\infty}{\underset{j=1}{\cup}}  (a_j, b_j)$ is an open set containing
$E$ and
\[
\inf\{\mu(U): \ U \mbox{ open } \supset E\} \le \mu(V) \le
\sum^\infty_{j=1}
\mu((a_j, b_j)).
\] By taking $\inf$ over $\{(a_j, b_j)\}^\infty_{j=1}$ we obtain
\begin{equation}\label{eq:open-approx.2}
\inf \{\mu(U): \ U \mbox{ open } \supset E\} \le \mu(E).
\end{equation}
Then  (\ref{eq:open-approx}) follows from (\ref{eq:open-approx.1}) and
(\ref{eq:open-approx.2}).

\bigskip
\noindent Next we prove (\ref{eq:compact-approx}).  Since $K \subset
E$ we have
\begin{equation}\label{eq:compact-approx.1}
\mbox{ RHS of (\ref{eq:compact-approx})} \le \mu(E).
\end{equation} 
To prove the reverse inequality, we first assume that $E$
is bounded.  If $E$ is closed i.e. $\bar E = E$ then $E$ is compact and by
letting $K = E$ we obtain
$\mu(E)
\le $ RHS of (\ref{eq:compact-approx}).  Otherwise we consider the set
$\bar E - E$ which is in
$\mathcal{M}$.  Since $\mu(\bar E - E) < \infty$ for any $\varepsilon > 0$
there is open set $U$ such that $U \supset \bar E - E$ and 
\[
\mu(U) \le \mu(\bar E - E) + \varepsilon
\] Now the set $K = \bar E - U$ is compact, $ K \subset \bar E - (
\bar E - E) = E$, and
\[
\mu(K) = \mu(E) - \mu(E - K)
\] since $E - K = E \cap K^c = E \cap (\bar E \cap  U^c)^c = E \cap \left (
\bar E^c \cup U\right ) = E \cap U$ the last equality gives

\begin{eqnarray*}
\mu(K) &=& \mu(E) - \mu(E \cap U)\\ &=& \mu(E) - [\mu(U) - \mu(U -
E)]\\ &\ge& \mu(E) - \mu(U) + \mu(\bar E - E)\\ &\ge& \mu(E) -
\varepsilon
\end{eqnarray*} Therefore $\mu(E) - \varepsilon \le$ RHS of
(\ref{eq:compact-approx}).  By letting $\varepsilon \to 0 $ we obtain
\begin{equation}\label{eq:compact-approx.2}
\mu(E) \le \mbox{ RHS of (\ref{eq:compact-approx})}.
\end{equation} 
This proves (\ref{eq:compact-approx}) in the bounded case.
If $E$ is unbounded, then we write
\[  E = \overset{\infty}{\underset{j=1}{\cup}} E_j \mbox{ with } E_j = E \cap
(j, j + 1].
\] Since $E_j$ is bounded,  there exist $K_j$ compact $\subset E_j$ such
that
\[
\mu(E_j) - \varepsilon 2^{-j} \le \mu(K_j)
\] If we let $H_n =\overset{n}{\underset{j=-1}{\cup}} K_j$ then $H_n$ is
compact $\subset E$ and since
$\{E_j\}^\infty_{j=1}$ are disjoint, we have
\[
\mu \left ( \overset{n}{\underset{j=-n}{\cup}} E_j \right ) - \varepsilon \le
\mu(H_n).
\] Now, if $\mu (E) = \infty$ then $\mu \left
(\overset{n}{\underset{j=-n}{\cup}} E_j
\right )
\longrightarrow \mu(E) = \infty$ as $n \to \infty$ and therefore $\mu(H_n)
\longrightarrow \infty$ as $n \to \infty$, which shows that (6) holds.  If
$\mu (E) <
\infty$ then there is $N$ such that
\[
\mu(E) - \varepsilon \le \mu \left ( \overset{N}{\underset{j=-N}{\cup}} E_j
\right )
\] therefore
\[
\mu(E) - 2\varepsilon \le \mu(H_N)
\] which gives $\mu(E) - 2 \varepsilon \le $ RHS of
(\ref{eq:compact-approx}).  And by letting
$\varepsilon
\to 0$ we obtain the desired inequality.




\begin{theorem}
If $E \in \mathcal{M}$ then for any $\varepsilon
> 0$ there is a closed set $F$ and an open set $V$ such that $F \subset E
\subset V$,
\[
\mu(V - E) < \varepsilon \mbox{ and } \mu(E - F) < \varepsilon.
\]
\end{theorem}
\textit{Proof:}  Let $E_j = E \cap (j, j + 1]$.   Then by the first part of the
last theorem for each $j$ there exists $V_j$ open $\supset E_j$ such that
\[
\mu(V_j - E_j) < 2^{-j} \varepsilon.
\] Then $V = \overset{\infty}{\underset{j=1}{\cup}} V_j$ is open $\supset
\overset{\infty}{\underset{j=1}{\cup}} E_j = E$ and
\[
\mu (V - E) \le \sum^\infty_{j=1} \mu(V_j - E_j) < \varepsilon
\sum^\infty_{j=1} 2^{-j}  = \varepsilon.
\] To prove the second inequality we consider $E^c$ and find $U$ open
$\supset E^c$ such that $\mu(U - E^c) < \varepsilon$.  Since $U - E^c = E -
U^c$ by letting $U^c = F$ we obtain $\mu(E-F) < \varepsilon$, which
completes the proof of the theorem.


\begin{theorem}
 Let $E \subset \mathbb{R}$.  Then the following
are equivalent.
\begin{itemize}
\item[(a)]  $E \in \mathcal{M}$.
\item[(b)]  $E = V - N_1$, where $V$ is $G_\delta$ set and
$\mu(N_1) = 0$.
\item[(c)]  $E = F \cup N_2$, where $F$ is $F_\sigma$ set and
$\mu(N_2) = 0$.
\end{itemize}
\end{theorem}
{\em Proof:}  Since $B_{\mathbb{R}} \subset \mathcal{M},$ and
since $\mu$ is complete,  we have that each of (b) and (c) imply (a).  To
show that (a) implies both (b) and (c), for each $j \in \mathbb{N}$ we apply
the last theorem to find an open set $V_j$ and a closed set $F_j$ such that
$F_j \subset E \subset V_j$ and $\mu(V_j - F_j) < \frac{1}{j}$.  Now if we let
$V = \overset{\infty}{\underset{j=1}{\cap}} V_j$ and
$F =
\overset{\infty}{\underset{j=1}{\cup}}  F_j$ then we have 
\[ F_j \subset F \subset E \subset V \subset V_j \mbox{ and } \mu(V- F) <
\frac{1}{j}
\] for all $j$.   Therefore $\mu(V-F) = 0$, which completes the proof of the
theorem.

\medskip
\noindent 
Recall that the {\bf  Lebesgue measure} is the measure
$\mu^*_F$ associated to the function 
\[ F(x) = x.
\] It is denoted by $m$, and the $\sigma$-algebra of the Lebesgue
measurable sets is denoted by $\mathcal{L}$.


\begin{theorem} Let $E \in \mathcal{L}$ and $r \in \mathbb{R}$. 
Then
\begin{itemize}
\item[(a)] $r + E = \{r + x: x \in E \} \in \mathcal{L} $ and $ m(r + E)
= m(E)$.
\item[(b)] $r \cdot E = \{r \cdot x: x \in E\} \in \mathcal{L}$ and
$m(r \cdot E) = |r|m(E).$
\end{itemize}
\end{theorem}
 {\em Proof:}  Since the collection of all intervals are invariant
under translation by $r$ and dilation by $r$, we see that the Borel
$\sigma$-algebra $B_{\mathbb{R}}$ is also invariant under translations and
dilations.  Also the collection of the sets of measure zero is invariant
under translations and dilations.  Thus $\mathcal{L}$ is invariant under
translation and dilations.  Since for an interval $I$ we have $m(r + I) =
m(I)$ and $m(rI) = |r|m(I),$ we see that $m$ is invariant under translations
and dilations.

\bigskip
\noindent
{\large\bf   The Cantor Set}
\noindent Consider the initial closed interval [0,1].  In the first step
subdivide [0,1] into three equal subintervals and remove the middle open
subinterval $\left (
\frac{1}{3}, \frac{2}{3} \right )$ .

\smallskip
\noindent
\centerline{{\bf  INSERT GRAPH}}

 Let
\[ C_1 = \left  [0, \frac{1}{3} \right ] \cup \left [ \frac{2}{3}, 1 \right ]
\] In the second step subdivide each of the two left subintervals again in
three equal subintervals and remove the middle open subinterval to
obtain
\[ C_2 = \left [ 0, \frac{1}{3^2} \right ] \cup \left [ \frac{2}{3^2}, \frac{1}{3}
\right ]
\cup \left [ \frac{2}{3}, \frac{7}{3^2} \right ] \cup \left [ \frac{8}{3^2}, 1
\right ]
\] We continue this construction for each of the remaining subintervals. 
At the
$k$-th step we will obtain $C_k \supset C_{k-1}$ as a union of $2^k$ closed
subintervals, each of length $3^{-k}$.  The Cantor set is 
\[ C = \overset{\infty}{\underset{k=1}{\cap}} C_k.
\]
\noindent 
Next we construct the {\bf  Cantor}(-Lebesgue) function.  By
using $C_1$ we define the function $f_1(x)$ to be the continuous function
with $f_1(0) = 0,\  f(1) = 1,\  f(x) = \frac{1}{2}, \ x \in \left [ \frac{1}{3}, \
\frac{2}{3} \right ]$ and on $C_1$ to be linear.

\smallskip
\noindent
\centerline{{\bf  INSERT GRAPH}}

\smallskip
\noindent
$f_2$ is the continuous function defined by $f_2(0) = 0, f_2(1) = 1$

\[ f_2 (x) = \left \{ \begin{array}{ll}
\frac{1}{2^2}, &x \in \left [ \frac{1}{3^2}, \frac{2}{3^2} \right ]\\
\\ 2 \cdot \frac{1}{2^2}, &x \in \left [ \frac{1}{3}, \frac{2}{3} \right ]\\
\\ 3 \cdot \frac{1}{2^2}, &x \in \left [ \frac{7}{3^2}, \frac{8}{3^2} \right
]\end{array}
\right .
\] and linear on $C_2$.  Continuing this way, $f_k$ is defined to be the
continuous functions defined by $f_k(0) = 0, \ f_k (1) = 1, \ f(x) = j2^{-k}$
on
$\overline{I}_{k_j}$, where
$\overline{I}_{k_j}$ is the $j$-th interval removed from [0,1] to get
$C_{k}$, i.e.
\[ [0,1] - C_k = \overset{2^{k-1}}{\underset{j=1}{\cup}}I_{k_j},
\] and $f_k(x)$ is linear on $C_k$.  Then the following hold.
\begin{itemize}
\item[(a)] $f_k$ is increasing
\item[(b)] $f_{k+1} = f_k$ on $[0,1] - C_k$
\item[(c)] $|f_k - f_{k+1}| < 2^{-k}$
\end{itemize} Therefore $\sum\limits^\infty_{k=1} \left ( f_{k+1} - f_k
\right )$ converges uniformly by the Weierstrass $M$-test to an
increasing continuous function $f(x)$ which is the limit of
$f_k(x)$; i.e.
\[ f(x) = \lim_{k \to \infty} f_k(x)
\] 
This is the {\bf  Cantor-Lebesgue} function.

\bigskip
\noindent
{\bf  Exercise}  Let $C$ be the Cantor set and $f$ be the
Cantor-Lebesgue function.  Then the following hold:

\begin{itemize}
\item[1.] $\displaystyle{C = \left \{x = \sum\limits^\infty_{j=1}
\frac{\alpha_j}{3^j}: \ \alpha_j = 0 \mbox{ or } 2 \right \}}$
\item[2.]   If $\displaystyle{x = \sum\limits^\infty_{j=1} \frac{\alpha_j}{3^{j}}\in C
\mbox{ then } f(x) = \sum\limits^\infty_{j=1} \frac{\alpha_j/2}{2^{j}}}$
\item[3.] $C$ is compact
\item[4.] $m(C) = 0$
\item[5.] $C$ contains no open interval; i.e. $\stackrel{0}{C} = \emptyset$.
\item[6.]  $f: C \longrightarrow [0,1]$ is onto.  Thus Card $(C)$ = Card
$(\mathbb{R})$
\item[7.]  $C$ is perfect, i.e.  $C$ is equal to its limit points.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%     Chapter 2
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{Integration }

In this chapter we follow closely  Folland's Real Analysis.


\bigskip
\noindent
{\large\bf Motivation.} Lebesgue's idea for integrating a 
function $f:
\mathbb{R} \to \mathbb{R}$ was to collect the points in its
domain where the function has the same size together, and
then multiply the measure of this set by the common size. 
Then summing over all different size of values gives a good
approximation of the integral  of the given function.  In other
words, Lebesgue's idea was to partition the range of the
function
$f$ in very small intervals and then to sum the products of a 
value in each interval with the measure of its inverse image. 
More precisely, for each $n \in \mathbb{N}$ partition
$\mathbb{R}$ into interval of length $\frac{1}{n}$; i.e. write
\[
\mathbb{R} = {\cup}^\infty_{j= -\infty} \left (
\frac{j}{n},
\frac{j+1}{n}
\right ]
\] 
and for this partition form the ``Lebesgue sum''
\[
 S^{\mathcal{L}}_n (f) = \sum^\infty_{j = - \infty} \frac{j}{n} m
\left ( f^{-1}
\left ( \frac{j}{n}, \frac{j+1}{n} \right ] \right )
\] 
Then if the function is such that the sets
\[ f^{-1} \left ( \left ( \frac{j}{n}, \frac{j+1}{n} \right ] \right )
\]
 are measurable (i.e. are not too complicated) and if the limit of
$S^{\mathcal{L}}_n(f)$ as $n \to \infty$ exists then the function $f$ is
``Lebesque integrable''.

\medskip
\noindent
{\bf Example.} If $f(x)$ is the Dirichlet function on [0,1]
\[
 f(x) = \left \{
 \begin{array}{ll} 1, &x \in Q \cap [0,1]\\ 0, &x \in Q^c
\cap [0,1] 
\end{array}\right .
\]
 then the Lebesque integral for this function is
\[
 1 \cdot m[f^{-1} (1)]+ 0 \cdot m[ f^{-1} (0)] = 1 \cdot m(Q \cap
[0,1]) + 0
\cdot m(Q^c \cap [0,1] = 0.
\]
%
Next we shall define measurable functions.

\section{Measurable Functions}

 \begin{definition} A function $f: \mathbb{R} \longrightarrow
\mathbb{R}$ is said Lebesque measurable if for any Borel set 
$B$ we have that $f^{-1} (B)$ is
\textbf{Lebesque} measurable; i.e.
\[ 
f^{-1} (B) \in \mathcal{L}, \ \forall B \in
\mathcal{B}_{\mathbb{R}}.
\]
 More generally, if $(X, \mathcal{M})$ and $(Y, \mathcal{N})$ are
two
\textbf{measurable} spaces, then a function $f: X \longrightarrow Y$ is
called $(\mathcal{M}, \mathcal{N})$-measurable if 
\[
 f^{-1} (E) \in \mathcal{M}, \ \forall E \in \mathcal{N}.
\]
 A function $f: X \longrightarrow \mathbb{R}$ is said
\textbf{$\mathcal{M}$-measurable} if it is $(\mathcal{M},
\mathcal{B}_{\mathbb{R}})$-measurable.  If $X = \mathbb{R}$ and
$\mathcal{M} = B_{\mathbb{R}}$, then it is called \textbf{Borel}
measurable.
 \end{definition}

\begin{prop} If $\mathcal{N}$ is generated by
$\mathcal{E}$, then 
\[
 f \mbox{ is } (\mathcal{M},
\mathcal{N})-\mbox{ measurable }
\Longleftrightarrow f^{-1} (E)
\in \mathcal{M}, \forall E \in \mathcal{E}.
\]
\end{prop}

\noindent
{\em Proof:}  The direction $``\Longrightarrow''$ is true since
$\mathcal{E} \in
\mathcal{N}$.  To prove $``\Longleftarrow''$ we consider the 
family
\[
\mathcal{N}_1 = \{E \subset Y: f^{-1} (E) \in \mathcal{M} \}.
\]
 By our assumption $\mathcal{E} \subset \mathcal{N}_1$.  If we
prove that
$\mathcal{N}_1$ is a  $\sigma$-algebra, then we would have
$\mathcal{N} \subset \mathcal{N}_1$ which would prove the proposition.
So let $E
\in \mathcal{N}_1$.  Then  $f^{-1} (E) \in \mathcal{M}$.  Thus $[f^{-1}
(E)]^c \in \mathcal{M}$ since $[f^{-1}(E)]^c = f^{-1} (E^c)$ we have  $E^c
\in \mathcal{N}_1$.  Next let $\{E_j\}^\infty_{j=1} \subset
\mathcal{N}_1$.  Then we have
\[
 f^{-1} \left ( {\cup}^\infty_{j=1} E_j\right ) =
{\cup}^\infty_{j=1} f^{-1} (E_j) \in
\mathcal{M}
\] 
Therefore ${\cup}^\infty_{j=1} E_j \in
\mathcal{N}_1$, and this proves that
$\mathcal{N}_1$ is a $\sigma$-algebra.

\begin{corollary}  If $X, Y$ are metric (or topological
spaces) then every continuous function $f: X \longrightarrow Y$
is
$(\mathcal{B}_X,
\mathcal{B}_Y)$- measurable.
\end{corollary}

\begin{prop} \label{pr:R-meas} Let $(X, \mathcal{M})$ be a measurable
space.  Then the following are equivalent

\begin{itemize}
\item[(a)] $f: X \longrightarrow \mathbb{R}$ is
$\mathcal{M}$-measurable.
\item[(b)] $\{x \in X: f(x) > a\} \doteq f^{-1} (a, \infty) \in \mathcal{M}, \
\forall a \in \mathbb{R}$.
\item[(c)] $\{x \in X: f(x) \ge a\} \doteq f^{-1} [a, \infty) \in 
\mathcal{M}, \ \forall a \in \mathbb{R}.$
\item[(d)] $\{x \in X: f(x) < a\} \doteq f^{-1} (- \infty, a) \in 
\mathcal{M}, \ \forall a \in \mathbb{R}.$
\item[(e)] $\{x \in X: f(x) \le a\} \doteq f^{-1} (- \infty, a] \in
\mathcal{M}, \ \forall a
\in \mathbb{R}.$
\end{itemize}
\end{prop}

\noindent
{\em Proof:}  It follows from Proposition 1 since each one
of the families
\[
\left \{ (a, \infty) \right \}_{a \in \mathbb{R}}, \left \{ [a, \infty) 
\right \}_{a \in \mathbb{R}}, \left \{ (- \infty, a) \right \}_{a \in
\mathbb{R}}, \left \{(-
\infty, a]\right \}_{a \in \mathbb{R}}
\]
 generate $\mathcal{B}_{\mathbb{R}}$.


\bigskip
\noindent
\textbf{Composition.} If $(X, \mathcal{M}), (Y,
\mathcal{N})$ and
$(Z, \mathcal{O})$ are measurable spaces,  $f: X \longrightarrow
Y$ is $\left (\mathcal{M}, 
\mathcal{N}
\right )$-measurable and $g: Y \longrightarrow Z$ is $(\mathcal{N},
\mathcal{O})$-measurable, then $g \circ f$ is $(\mathcal{M},
\mathcal{O})$- measurable.

\smallskip
\noindent
\textbf{Warning:}  If $f, g: \mathbb{R} \longrightarrow \mathbb{R}$ are
Lebesque measurable, then $ g \circ f$ may \textbf{not} be measurable,
unless $g$ is Borel measurable.

\medskip
\noindent
 Let $(X, \mathcal{M})$ and $\{(Y_\alpha,
\mathcal{N}_\alpha)\}_{\alpha \in A}$ be measurable spaces and 
$f_\alpha: X \longrightarrow Y_\alpha$ be a family of functions.  The
$\sigma$-algebra generated by
\[
\{f^{-1}_\alpha(E_\alpha): E_\alpha \in \mathcal{N}_\alpha,
\, \alpha\in A\} \subset
\mathcal{P}(X)
\]
 is called the $\sigma$-algebra \textbf{generated} by
$\{f_\alpha\}_{\alpha \in A}$.

\begin{example} If $Y = \Pi_{\alpha \in A} Y_\alpha$ then the
cartesian product 
$\otimes_{\alpha \in A} \mathcal{N}_\alpha$ is the $\sigma$
-algebra generated by the projection $\pi_\alpha: Y
\longrightarrow Y_\alpha$.
\end{example}

\begin{prop}   Let $(X, \mathcal{M}), \{(Y_\alpha,
\mathcal{N}_\alpha)\}_{\alpha \in A}$ be measurable spaces, and $Y =
\Pi_{\alpha \in A} Y_\alpha$ the cartesian product of $Y_\alpha$ with
the product 
$ \sigma$-algebra $\mathcal{N} = \otimes_{\alpha\in A}
\mathcal{N}_\alpha$.  Then a function
 $f: X\longrightarrow Y$ is $(\mathcal{M}, \mathcal{N})$-measurable iff 
$f_\alpha =
\pi_\alpha \circ f$  is $ (\mathcal{M}, \mathcal{N}_\alpha)$-measurable
for all
$\alpha \in A$.  Here $\pi_\alpha: \Pi_{\alpha \in A} Y_\alpha
\longrightarrow Y_\alpha$ are the projections.
\end{prop}


\bigskip
\noindent
{\em Proof.}  Recall that by its definition $\mathcal{N}$ is
generated by
\[
\left \{ \pi^{-1}_\alpha (E_\alpha): \ E_\alpha \in \mathcal{N}_\alpha, \
\alpha \in
\mathcal{A} \right \}
\]
or equivalently by the projections $\{\pi_\alpha \}_{\alpha \in
A}$.  Therefore
$\pi_\alpha$ is $(\mathcal{N}, \mathcal{N}_\alpha)$-measurable. 
Therefore if $f$ is
$(\mathcal{M}, \mathcal{N})$-measurable, then $f_\alpha = \pi_\alpha
\circ f$ is
$(\mathcal{M}, \mathcal{N}_\alpha)$-measurable.  Conversely if
$f_\alpha$ is
$(\mathcal{M}, \mathcal{N}_\alpha)$-measurable, then
\[ f^{-1}_\alpha (E_\alpha) = f^{-1} (\pi^{-1}_\alpha (E_\alpha)) \in
\mathcal{M}
\] which means that $f$ is $(\mathcal{M}, \mathcal{N})$ measurable.

\begin{corollary}  $f: X\longrightarrow \mathbb{C}$ is
measurable if both $\Re f$ and $\Im f$ are measurable.
\end{corollary}

\begin{prop}  Let $(X, \mathcal{M})$ be a
measurable space and $f, g: X\longrightarrow \mathbb{C}$ be $
\mathcal{M}$-measurable.
Then
\begin{itemize}
\item[(a)]  $f + g$ and $f \cdot g$ are
$\mathcal{M}$-measurable.
\item[(b)] $\frac{1}{f}$ is $\mathcal{M}$ measurable if $f
\ne 0$ on $X$.
\end{itemize}
\end{prop}

\medskip
\noindent
{\em Proof:}
\begin{itemize}
\item[(a).]  Define the functions $F: X \longrightarrow \mathbb{C}
\times \mathbb{C}$ by $F(x) = (f(x), g(x))$,
$\varphi: \mathbb{C} \times \mathbb{C}\to \mathbb{C}$ by 
$\varphi(z, w) = z + w$, and $\psi: \mathbb{C} \times \mathbb{C}
\longrightarrow
\mathbb{C}$ by $\psi (z, w) = z \cdot w$.  If $\pi_j, j = 1, 2$ 
denotes the  projection from $\mathbb{C}
\times \mathbb{C} \longrightarrow \mathbb{C}$ then $f = \pi_1 \circ F$
and $g =
\pi_2 \circ F$.  Since both $f$ and $g$ are measurable, by the last
proposition $F$ is measurable.  And since both $\varphi $ and $\psi$ are
continuous, we have that
\[ f + g = \varphi \circ F \mbox{ and } f \cdot g = \psi \circ F
\] are measurable.  This completes the proof of (a).
\item[(b).]  Let $f = f_1 + if_2$.  Since
\[
\frac{1}{f} = \frac{1}{f_1 + if_2} = \frac{f_1 - if_2}{f^2_1 + f^2_2}
\]
 it suffices to prove (b) when $f: X \longrightarrow
\mathbb{R}$.  In this case, for any
$\alpha \in \mathbb{R}$ we have
\end{itemize}


\[
\left \{x \in X: \frac{1}{f(x)} < a \right \} = \left \{ 
\begin{array}{ll} f^{-1} \left (\frac{1}{a},  0 \right ),\qquad 
&\mbox{ if } a < 0\\ f^{-1} \left ( - \infty, 0 \right ),  \quad &\mbox{ if } a =
0 \\ f^{-1} (- \infty, 0) \cup f^{-1} \left ( \frac{1}{a}, \infty \right ), 
&\mbox{ if } a > 0.
\end{array} \right .
\]
 Therefore $\frac{1}{f}$ is measurable. 

\medskip
\noindent
 There are situations where we have to consider
functions with values of the extended real number
$\overline{\mathbb{R}} =
\mathbb{R} \cup \{\pm \infty \}.$  For example, when we take the $\sup$
in a sequence of real valued functions, we may obtain a function valued
on $\overline{\mathbb{R}}$.  A function $ f: X
\longrightarrow
\overline{\mathbb{R}}$ is measurable if again one of the equivalent
conditions (b) - (e) of  Proposition \ref{pr:R-meas} holds.  In other words,
$f: X
\longrightarrow
\overline{\mathbb{R}}$ is measurable if $f|_{\mathbb{R}}$ is
measurable and the sets $f^{-1} (\infty)$ and
$f^{-1} (- \infty)$ are also measurable.  Moreover, if $f, g: X \to
\overline{\mathbb{R}}$ are measurable then $f+g, fg$ are measurable if
we define
$\infty - \infty$ to be always the same element in
$\overline{\mathbb{R}}$.

\begin{prop} Let $(X, \mathcal{M})$ be a
measurable space.  Then
\begin{itemize}
\item[(a)]  If the functions $f_k: X \longrightarrow
\overline{\mathbb{R}}$ are
$\mathcal{M}$-measurable, then the functions
\[
\inf_k f_k(x),\  \sup_k f_k(x), \ \lim_{k \to \infty}\inf \
f_k(x), 
\ \lim_{k \to \infty}\sup f_k (x) 
\]
are $\mathcal{M}$-measurable.  In particular, if $f_k$
converges in $X$, then the limit is measurable.
\item[(b)]  If $f: X \longrightarrow
\overline{\mathbb{R}}$ is
$\mathcal{M}$-measurable, then the functions
\[ 
f^+(x) \doteq \max \{f(x), 0\}, \ f^-(x) = \max \{-f(x), 0\}
\]
are $\mathcal{M}$-measurable. $f^+$ and $f^-$ are called
the
\textbf{positive} and \textbf{negative} part of $f$ respectively.  And
$f = f^+ - f^-$.
\item[(c)]  If $f: X \longrightarrow \mathbb{C}$ is
$\mathcal{M}$-measurable then
\[ 
|f| = \sqrt{(\mbox{Ref})^2 + (\mbox{Imf)}^2} \mbox{ and } sgn f
\]
are $\mathcal{M}$-measurable.  Recall that for 
$z \in \mathbb{C}\  sgn z =
\frac{z}{|z|}$ if $ z \ne 0$ and $sgn 0 = 0$.
\end{itemize}
Thus $f$ can be written in a measurable way in the form
\[
 f = (sgn f) |f|
\]
 which is called the \textbf{polar} decomposition of $f$.
\end{prop}

\medskip
\noindent
{\em Proof:}  For any $a \in \mathbb{R}$ we have
\[
\left \{ x \in X: \inf_k f_k(x) < a \right \} =
{\cup}^\infty_{k=1}\left \{x \in X: f_k(x) < a \right \}
\] and
\[
\left \{ x \in X: \sup_k f_k(x) > a \right\} =
{\cup}^\infty_{k=1}\left \{ x \in X: f_k (x) > a
\right
\}.
\]
 Therefore by Corollary 1 $\inf f_k$ and $\sup f_k$ are
$\mathcal{M}$-measurable.  Now define
\[
 h_k (x) = \sup_{j \ge k} f_k (x).
\]
 Then $h_k$ are $\mathcal{M}$-measurable.  Since
\[
\lim_{k \to \infty} \sup f_k = \inf_k h_k(x).
\]
 we have that $\lim\sup f_k$ is measurable.  Similarly, we show
that
$\lim \inf f_k$ is measurable.  The proof of the rest is left as an
exercise.
 
\medskip
\noindent
{\large \bf Simple Functions Approximation}
 Let $(X,
\mathcal{M})$ be a measurable space.  The simplest type of a
measurable function is the
\textbf{characteristic} function of measurable set
$E$.  That is the function defined by
\[
\chi_E (x) = \left \{ \begin{array}{ll} 1, &x \in E\\ 0, &x \notin E
\end{array} \right .
\]
 Observe that for any $a \in \mathbb{R}$
\[
\left \{x \in X: \chi_E (x) > a \right \} = \left \{ \begin{array}{ll} X,
&\mbox{ if } a < 0\\ E, &\mbox{ if } a = 0\\
\emptyset, &\mbox{ if } a \ge 1 \end{array} \right .
\] Therefore $\chi_E$ is measurable iff $E$ is measurable.

The next simplest type of a measurable function is a linear
combination of characteristic functions of measurable sets.  That is
functions of the form
\[
\varphi(x) = \sum^k_{j=1} c_j \chi_{E_j}, \ c_j \in \mathbb{C}, \ E_j \in
\mathcal{M}.
\] Such functions are called \textbf{simple} functions.  A simple
function may be represented in many different ways.  It's
\textbf{standard} representation is the one for which all $c_j$ are
different and $E_j = f^{-1} (c_j)$, that is all $E_j$ are disjoint.

\bigskip
\noindent
\textbf{Exercise.}  Show that the sum and the product of two simple
functions  are simple functions.


\begin{theorem}
\textbf{(a)}  If $f: X \to [0, \infty]$ is a nonnegative measurable
function, then there exists a sequence of simple measurable functions
$\{\varphi_n\}$ such that
\begin{itemize}
\item[(i)]  $0 \le \varphi_1 \le \varphi_2 \le \dots \le f.$
\item[(ii)]  $\varphi_n \longrightarrow f$ \textit{as} $n \to \infty$.
\item[(iii)]  $\varphi_n \longrightarrow f$ uniformly on any set
where $f$ is bounded.
\end{itemize}
\noindent
\textbf{(b)}  If $f: X \longrightarrow \mathbb{C}$ is measurable,
then there exists a sequence of simple functions $\{\varphi_n\}$ such
that
\begin{itemize}
\item[(i)] $0 \le |\varphi_1| \le |\varphi_2| \le \dots \le |f| .$
\item[(ii)]  $\varphi_n \longrightarrow f.$
\item[(iii)]  $\varphi_n \longrightarrow f$ uniformly on any set
where $f$ is bounded.
\end{itemize}
\end{theorem}
\textit{Proof:}  
Let $n \in \mathbb{N}$ be fixed.  Then

\smallskip
\noindent
\centerline{\textbf{INSERT GRAPH}}
\smallskip
\noindent we subdivide the range $[0, 2^n]$ into $2^{2n}$ equal
subintervals of equal length
$2^{-n}$ and we let
\[ E^k_n = f^{-1} \left ( \frac{k}{2^n}, \frac{k+1}{2^n} \right ], k = 0, 1,
\dots, 2^{2n} -1
\] and
\[ F_n = f^{-1} (2^n, \infty].
\] If we define $\varphi_n$ by 
\[
\varphi_n = \sum_{k=0}^{2^{2n} -1} \frac{k}{2^n} \chi_{E^k_n}+ 2^n
\chi_{F_n}
\] then it is easy to see that $\varphi_n$ satisfies the properties
\begin{itemize}
\item[(1)]  $0 \le \varphi_n \le \varphi_{n+1} \le f$
\item[(2)] $0 \le f - \varphi_n \le 2^{-n} \mbox{ on } \{x: f(x) \le 2^n\}$
\end{itemize} Properties (1) and (2) imply (a).

\bigskip
\noindent
\textbf{(b)}  Write $f = g + ih$, where $g, h$ are real.  Then write $g =
g^+ - g^-$ and
$h = h^+ - h^-$.  And use (a) to approximate $g^+$ with $\{\psi^+_n\}\ 
g^-$with
$\{\psi^-_n\}, h^+$ with $\{\xi^+_n\}$ and $h^-$ with $\{\xi^-_n\}$.  If we
let
$\varphi_n = (\psi^+_n - \psi_n^-) + i (\xi^+_n - \xi_n^-)$ then
$\{\varphi_n\}$ is the desired sequence in (b).


\medskip
\noindent Recall that a sequence of functions $f_n: X \longrightarrow
\mathbb{C}$ converge
\textbf{almost everywhere} (a.e.) to $f: X \longrightarrow \mathbb{C} $
if there is a set $N \in \mathcal{M}$ such that
\begin{itemize}
\item[(i)]  $\mu(N) = 0$
\item[(ii)]  $f_n (x) \longrightarrow f(x), \ \forall x \in X - N$.
\end{itemize}



\begin{prop}  Let $(X, \mathcal{M}, \mu)$ be a measure
space with
$\mu$ complete.  Then
\begin{itemize}
\item[(a)]   If $f$ is measurable and $g = f $ a.e. then $g$ is
measurable.
\item[(b)]   If $f_n$ are measurable and $f_n \longrightarrow f $
a.e.then $f$  is measurable.
\end{itemize}
\end{prop}

\textit{Proof:}  
(a) Let $C = \{x \in X: f(x) = g(x) \}$.  Then $C \in \mathcal{M}$
and $\mu (C^c) = 0$.  Since
\[
\{x: g(x) > a\} = \left ( \left \{x: f(x) > a\right \} \cap C \right ) \cup \left
( \left \{x: g(x) > a \right \} \cap C^c \right )
\]
$g$ is measurable.
(b)  Let $C = \left \{x \in X: f_n (x) \longrightarrow f(x) \right
\}$.  Then $C \in
\mathcal{M}$ and $\mu(C^c) = 0$.  Since $f_n$ are measurable on $C$, we
have that
$f$ is measurable on $C$.
Now define
\[
\bar f(x) = \left \{\begin{array}{ll} f(x), &x \in C\\ 0, &x \in
C^c.\end{array} \right .
\] Since $\mu(C^c) = 0$ we have that $\bar f$ is measurable.  Since $f$
differs from
$\bar f$ on a set of measure zero, we conclude that $f$ is measurable.

\medskip
\noindent
\textbf{Remark.}  Here we will consider complete measures only.



\section{Integration of Nonnegative Functions}
 Lebesgue's idea was to integrate functions by paritioning
the range, not the domain.  This idea is embedded in the representation
of a simple function in standard form.  Therefore it is natural to define
Lebesgue's integral first for such functions.  Let $(X, \mathcal{M}, \mu)$
be a measure space and denote by 
\[ L^+ = \{f: \mathbb{R} \longrightarrow [0, \infty]: f \mbox{ is
measurable}\}.
\] If $\varphi$ is a simple function in $L^+$ written in standard form as
\[
\varphi (x) \doteq \sum^m_{j=1} a_j \chi_{E_j}
\] 
then the \textbf{integral} of $\varphi$ is defined by 
\begin{equation}
\label{eq:step-funct-int}
\int \varphi \left ( = \int \varphi d \mu = \int_X \varphi d\mu = \int
\varphi (x) d\mu(x) \right ) \doteq \sum^m_{j=1} a_j \mu(E_j) \tag{1}
\end{equation}
\textbf{Note:}  By the definition of simple functions $a_j \in
\mathbb{R}$.  However
$\mu(E_j)$ may  be $\infty$. In this case, we use the convention $0
\cdot \infty = 0$.  If $A \in \mathcal{M}$ then \textbf{the integral of
$\varphi$ over $A$} is defined by
\[
\int_A \varphi d\mu =\int \varphi \chi_A = \sum^m _{j=1} a_j \mu(E_j
\cap A).
\]

\begin{prop} (Basic Properties)  Let $\varphi$ and $\psi$ be
simple functions.  Then
\begin{itemize}
\item[(a)] $\int c \varphi = c \int \varphi, \ c \ge 0.$
\item[(b)] $\int (\varphi + \psi) = \int \varphi + \int \psi$.
\item[(c)] $\int \left ( \sum\limits^m_{j=1} a_j \chi_{E_j} \right ) =
\sum\limits^m_{j=1} a_j \mu(E_j), \mbox{(\textit{may not be in
standard form})}$.
\item[(d)]  $\varphi \le \psi \Longrightarrow \int \varphi \le \int \psi$.
\item[(e)] $\mathcal{M} \ni A \longmapsto \nu(A) \doteq \int_A \varphi
d \mu$ is a measure.
\end{itemize}
\end{prop}
\textbf{Proof:}
  Part (a) follows immediately from the definition.

\smallskip
\noindent
(b)  If
\[
\varphi = \sum^m_{j=1} a_j \chi_{E_j}, \ \psi = \sum^\ell_{k=1} b_k
\chi_{F_k}
\] are the standard representations of $\varphi$ and $\psi$ then the
standard representation of $\varphi + \psi$ is
\[
\varphi + \psi = \sum_{j,k} (a_j + b_k) \chi_{E_j \cap F_K}.
\] 
(Here the book is not precise. We need to collect 
the sets where $a_j + b_k$ takes the same value together.)
Therefore
\begin{eqnarray*}
\int(\varphi + \psi) &=& \sum_{j, k} (a_j + b_k) \mu(E_j \cap F_k)\\
&=& \sum_{j, k} a_j \mu(E_j \cap F_k) + \sum_{j, k} b_k \mu(E_j \cap
F_k)\\ &=& \sum^m_{j=1} a_j \mu(E_j) + \sum_k b_k \mu(F_k)\\ &=&
\int \varphi + \int  \psi.
\end{eqnarray*}
\smallskip
\noindent
(c) follows from (a) and (b) applied to the $m$ functions $a_j
\chi_{E_j}$.

\smallskip
\noindent(d)  If $\varphi \le \psi$ then $a_j \le b_k$ on $E_j \cap F_k$. 
Therefore
\[
\int \varphi  = \sum_{j, k} a_j \mu(E_j \cap F_k) \le \sum_{j, k} b_k
\mu(E_j \cap F_k) = \int \psi.
\]

\smallskip
\noindent
(e)  If $\{A_k\} \subset \mathcal{M}$ and disjoint then
\begin{eqnarray*}
\nu \left ( {\cup}^\infty_{k=1} A_k\right ) &=&
\sum^m_{j=1} a_j
\mu \left [ E_j
\cap \left ( {\cup}^\infty_{k=1} A_k \right )
\right ]\\ 
&=& \sum^m_{j=1} a_j \sum^\infty_{k=1} \mu(E_j
\cap A_k)\\
 &=&
\sum^\infty_{k=1} \left [ \sum^m_{j=1} a_j \mu(E_j \cap A_k) \right ] =
\sum^\infty_{k=1} \nu(A_k)
\end{eqnarray*}

\noindent
\textbf{Definition of integral for general nonnegative $f$:} If $f \in L^+$
then the (natural) definition of its integral is
\begin{equation}
\label{eq:pos-funct-int}
\int f d \mu \doteq \sup\left \{\int \varphi d \mu: 0 \le \varphi \le f
\mbox{ and }
\varphi \mbox{ simple}\right \}
\end{equation}


\begin{lemma} The following hold.

\begin{itemize}
\item[(a)]   If $f$ is simple, then definitions
(\ref{eq:step-funct-int}) and (\ref{eq:pos-funct-int}) give the
same number.
\item[(b)]  $0 \le f \le g \Longrightarrow \int f \le \int g.$
\end{itemize} 
\end{lemma}

Next we shall state the first theorem that allows the
passing of the limit inside the integral sign.

\begin{theorem} (The Monotone Convergence Theorem (MCT))  Let $f_n: X
\longrightarrow [0, \infty]$ be in $L^+$ such that $f_1 \le f_2 \le\dots$. 
Then
\begin{equation}\label{eq:MCT}
\lim_{n \to \infty} \int f_n = \int \lim_{n \to \infty} f_n 
\end{equation}
\end{theorem}
\textit{Proof:} 
 Let $f = \lim_{n \to \infty} f_n$.  Since $f_n \le f$ we have
$\int f_n\le \int f$ and 
\begin{equation}\label{eq:MCT.1}
\lim_{n \to \infty} \int f_n \le \int f.
\end{equation} 
To prove the reverse of (4), let $0 < \alpha < 1$.  For a
given simple function
$\varphi, 0 \le \varphi \le f,$ define
\[ E_n = \left \{x \in X: f_n (x) \ge \alpha \varphi(x) \right \}
\] since $f_n \le f_{n+1}$ we have that $E_n \subset E_{n+1}$.  Therefore
\[
\int f_n \ge \int f_n \chi_{E_n} \ge \int \alpha \varphi \chi_{E_n} =
\alpha \int_{E_n}\varphi 
\] 
since $\mathcal{M} \ni A \longmapsto \int_A \alpha \varphi$ is a
measure, $E_n$ is increasing and $X =
{\cup}^\infty_{n=1} E_n$ we have 
\[
\alpha \int \varphi = \int_{\bigcup^\infty_{n=1} E_n} \alpha \varphi =
\lim
\int_{E_n} \alpha \varphi.
\] Therefore
\[
\lim_{n \to \infty} \int f_n \ge \lim_{n \to \infty} \int_{E_n} \alpha
\varphi = \alpha
\int \varphi.
\] Since this inequality is true for any $0 < \alpha < 1$ we have
\[
\lim_{n \to \infty} \int f_n \ge \int \varphi, \ \forall \varphi \mbox{
with } 0 \le
\varphi \le f.
\] Now by taking $\sup$ over $\varphi$ we obtain
\begin{equation}\label{eq:MCT.2}
\lim_{n \to \infty} \int f_n \ge \int f.
\end{equation} 
By (\ref{eq:MCT.1}) and (\ref{eq:MCT.1}) we obtain (\ref{eq:MCT}).


\begin{corollary} 
 If $f \in L^+$ then
\[
\int f = \lim\limits_{n \to \infty} \left ( \sum\limits^{2^{2n} -1}_{k = 0}
\frac{k}{2^n} \mu
\left \{ x: \frac{k}{2^n} < f(x) \le \frac{k+1}{2^n} \right \}  +2^n \mu \left
\{x: f(x) > 2^n\right \}  \right ).
\]
\end{corollary}
\textit{Proof:}  It follows from the simple functions approximation
theorem.

\begin{corollary}  Let $f_n \in L^+$.  Then
\begin{itemize}
\item[(a)] $\displaystyle{\int \left ( \sum^N_{n=1} f_n\right ) =
\sum^N_{n=1}
\int f_n, \ \forall N}$\\
\item[(b)] $\displaystyle{\int \left ( \sum^\infty_{n=1} f_n\right ) =
\sum^\infty_{n=1} \int f_n.}$
\end{itemize}
\end{corollary}
\noindent
\textit{Proof:}  For (a) it suffices to prove for two functions $f_1$ and
$f_2
\in  L^+$.  The general case follows by induction.  Let $\{\varphi_j\}$ and
$\{\psi_j\}$ sequences of simple functions in $L^+$ such that 
\[
\varphi_j \longrightarrow f_1 \mbox{ and } \psi_j \longrightarrow f_2.
\] Then $\varphi_j + \psi_j \longrightarrow f_1 + f_2$ and by the MCT

\begin{eqnarray*}
\int(f_1 + f_2) &=& \lim_{j\to \infty} \int (\varphi_j + \psi_j) =
\lim(\int \varphi_j +\int \psi_j)\\ &= &\lim \int \varphi_j + \lim \int
\psi_j = \int f_1 + \int f_2.
\end{eqnarray*}
\noindent
(b)  We have
\begin{eqnarray*} 
 \int \left ( \sum^\infty_{n=1} f_n \right ) &=& \int \lim_{N \to\infty}
\left (\sum^N_{n=1} f_n \right )\\ &\stackrel{\mbox{MCT}}{=}& \lim_{N
\to \infty} \int \left (\sum^N_{n=1} f_n\right )\\ &\stackrel{(a)}{=}&
\lim_{N \to \infty} \sum^N_{n=1} \int f_n =
\sum^\infty_{n=1} \int f_n
\end{eqnarray*}

\begin{prop} Let $f$ be a nonnegative measurable
function.  Then
$\int f = 0$ if and only if $f = 0$ a.e.
\end{prop}
\textit{Proof:}  
If $f = 0$ a.e. then for any simple function $\varphi$
with  $0 \le
\varphi \le f$ we have $\varphi = 0$ a.e.  Since $\int \varphi = 0$  the
$\sup$ of
$\int \varphi$ over all such $\varphi$ is zero and therefore $\int f = 0$. 
Conversely, let us assume that $\int f = 0$.  And define
\[ E_n (x) = \left \{x \in X: f(x) > \frac{1}{n} \right \}.
\] Since $\{x \in X: f(x) > 0\} = {\cup}^\infty_{n=1}
E_n$ it suffices to show that
$\mu(E_n) = 0$ for all $n$.   Assume there is $n$ such that
$\mu(E_n) > 0$.  Then $f >
\frac{1}{n} \chi_{E_n}$ and therefore $\int f \ge \int \frac{1}{n}
\chi_{E_n} = \frac{1}{n} \mu (E_n) > 0$, which contradicts the
assumption
$\int f = 0$. 

\begin{corollary}  The Monotone Convergence Theorem holds if $f_n (x)
\nearrow f(x)$ a.e. in $X$, instead of $f_n(x) \nearrow f(x)$ everywhere
in $X$.
\end{corollary}

\medskip
\noindent
\textit{Proof:}  Let
\[ E = \{x \in X: f_n (x) \nearrow f(x) \}.
\] Then by the assumption $\mu(E^c) = 0$ and $f_n \chi_E \nearrow f
\chi_E$ in $X$.  And by the MCT we have
\[
\int f = \int f \chi_E + \int f \chi_{E^c} = \int f \chi_E = \lim
\int f_n \chi_E = \lim \int f_n.
\]


We have seen that the MCT gives us the first criterion for
interchanging limits and integrals in nonnegative measurable functions
which are monotone.  The monotonicity is essential  assumption since
if on [0,1] choose  $f_n = n
\chi_{(0, \frac{1}{n})}$ we see that $f_n \longrightarrow 0$ but $1 = \int
f_n
\not\longrightarrow \int f = 0$.  Or if we choose  $f_n = n^2 \chi_{(0,
\frac{1}{2})}$ then $n = \int f_n \not\longrightarrow \int f = 0$. 
Observe that in these examples the limit function has a smaller
integral.  That is in the limit the area under the function may
disappear.  This is expressed in the following more general result of
Fatou.
 \begin{theorem} (Fatou)  If $\{f_n\}$ is a sequence of nonnegative
measurable functions on a measure space $(X, \mathcal{M}, \mu)$ then
\[
\int \lim \inf f_n \le \lim \inf \int f_n. 
\]
\end{theorem}
\textit{Proof:}  By definition we have
\[
\lim \inf f_n = \lim_{k \to \infty} \left ( \inf_{n \ge k} f_n \right )
\] 
Since the new sequence $\{\inf_{n \ge k} f_n
\}^\infty_{k=1}$ 
is increasing by the MCT we have
\[
\int \lim \inf f_n = \lim_{k \to \infty}
\int  {\inf}_{n \ge k} f_n \le \lim \inf \int f_n. 
\] 
The last inequality is true since ${\inf}_{n \ge k} f_n 
\le f_n, \
\forall n
\ge k,$ gives
\[
\int \inf_{n \ge k} f_n \le \int f_n, \ \forall n \ge k \Longrightarrow \int
\inf_{n \ge k} f_n \le \inf_{n \ge k} \int f_n. 
\]


\begin{corollary}  If $f_n \longrightarrow f $ a.e. then $\int f \le \lim
\inf \int f_n$.
\end{corollary} 

\medskip
\noindent
\textit{Proof:}  By defining $f_n, f$ equal to the same value (say 65) on
the set of points $x$ where $f_n (x)$ does not converge, we obtain
$f_n(x) \longrightarrow f(x), \ \forall x \in X$.  Then we apply Fatou's
Lemma and use the fact that  the integrals of $f_n$ and $f$ are not
changed.


\begin{prop} If $f$ is a nonnegative measurable function with $\int f <
\infty$ then 
\begin{itemize}
\item[a)]  $\mu\{x: f(x) = \infty \} = 0$
\item[b)]  $\{x: f(x) > 0\}$\textit{ is } $\sigma$-finite.
\end{itemize}
\end{prop}
\textit{Proof:}
(a)  By definition the set $E_\infty = \{x: f(x) = \infty \}$ is
measurable and 
\[
\infty \cdot \mu E_\infty = \int_{E_\infty} f d\mu \le \int_X f d \mu <
\infty.
\] Therefore $\mu(E_\infty) < \infty$.

\smallskip
\noindent
(b)  Let $E_n = \{x \in E: f(x) > \frac{1}{n} \}$.  Then $\{x: f(x) > 0\} =
{\cup}^\infty_{n=1} E_n$ and 
\[
\mu(E_n) = \int_{E_n} 1 d \mu = \int_{E_n} n \cdot \frac{1}{n} d \mu \le
n \int_{E_n} f d \mu \le n \int_X f d \mu < \infty.
\]



\section{Integration of Complex-valued Functions}
In the last section we defined the integral of a non-negative
measurable function as the limit of the integrals of a sequence of
nonnegative  function.  Next we shall define the integral of more
general functions in terms of the integral of nonnegative functions. 
For this, we assume that $(X, \mathcal{M}, \mu)$ is a measure  space.  If
$f: X \longrightarrow \bar{\mathbb{R}}$ is a measurable function, then
we write  $f = f^+ - f^-$ and define
\begin{equation}\label{eq:general-int}
\int f = \int f^+ - \int f^-
\end{equation}
 provided at least one of $\int f^+$ and $\int f^-$ isfinite.  
Such a function is said
\textbf{integrable} if both $\int f^-$ and $\int f^+$ are finite.  Since  $|f|
= f^+ + f^-$ we see that 
\begin{equation}\label{eq:int-funct}
f \mbox{ is integrable if } \int |f| < \infty.
\end{equation}

If $f: X \longrightarrow \mathbb{C}$ is a complex-valued and
measurable function then we say that $f$ is \textbf{integrable} if both
Re$f$ and Im$f$ are integrable and define
\[
\int f = \int \mbox{Re}f + i \int \mbox{Im} f
\] Since $|f| \le |\mbox{Re}f| + |\mbox{Im}f|$ we see that $f$ is integrable
if $|f|$ is integrable.

\begin{prop}  Let $(X, \mathcal{M}, \mu)$ be a measure
space.  Then
\begin{itemize}
\item[a)] \textit{ The set of integrable real-valued functions is a
vector space over
$\mathbb{R}$, and the integral is a linear functional on it.}
\item[b)]  \textit{The set of integrable complex-valued functions is a
vector space over
$\mathbb{C},$ and the integral is a complex linear functional on it.}
\end{itemize}
\end{prop}
\textit{Proof:} In both cases $f + g$ and $cf$ are integrable
since
\[ |c_1 f + c_2 g| \le |c_1| |f| + |c_2| |g|.
\] 
Also it is easy to check that in both cases
\[
\int c f = c\int f.
\]
Thus, in both cases it suffices to show that  
$\int (f + g)$ $=\int f +\int g$. 
In the real case, we let $h = f + g$ and write 
\[ h^+ - h^- = f^+ - f^- + g^+ - g^-
\] or
\[ h^+ + f^- + g^- = h^- + f^+ + g^+,
\] which gives
\[
\int h^+ + \int f^- + \int g^- = \int h^- + \int f^+ + \int g^+
\] or
\[
\int h^+ - \int h^- = \left ( \int f^+ - \int f^- \right ) + \left ( \int g^+ -
\int g^-\right ),
\] which is the desired result.  In the complex case we have

\begin{eqnarray*}
\int(f+g) &=& \int \mbox{Re}(f+g) + i \int \mbox{Im}(f + g)\\ &=& \int
\mbox{Re} f + \int \mbox{Re} g + i \left ( \int \mbox{Im}f + \int
\mbox{Im}g
\right )\\ &=& \left ( \int \mbox{Re} f + i \int \mbox{Im} f \right ) +
\left ( \int \mbox{Re}g + i\int \mbox{Im} g\right ). 
\end{eqnarray*}


\begin{prop} Let $f, g: X \longrightarrow \mathbb{C}$
integrable.  Then
\begin{itemize}
\item[a. ]  $\left |\int f \right | \le \int |f|$
\item[b. ] $\{x: f(x) \ne 0\} $ \textit{is} $\sigma-$finite
\item[c. ] $f = g$ a.e. $\Longleftrightarrow \int |f-g| = 0
\Longleftrightarrow \int_E f = \int_Eg, \ \forall E \in \mathcal{M}$.
\end{itemize}
\end{prop}
\textit{Proof:}  
(b) follows from the proposition we proved last lecture
applied to
$|f|$.  To prove (a) we notice that it is true if $\int f = 0$, or if $f$ is real
valued.  If
$f$ is complex with $\int f \ne 0$ then we let
\[
\alpha = \frac{\overline{\int f}}{|\int f|} = \overline{\text{sgn} \int
f}
\] and obtain

\begin{eqnarray*}
\left | \int f \right | &=& \frac{\overline{\int f} \int f}{|\int f|} = \alpha
\int f = \int
\alpha f\\ &=&\mbox{Re} \int \alpha f = \int \mbox{Re} (\alpha f)\\ 
&\le& \int |\mbox{Re}(\alpha f)| \le \int |\alpha f| = \int |f|.
\end{eqnarray*} (c)  The first equivalence follows from the
corresponding result for nonnegative functions applied to $|f-g|$.  For
the second, assume  that $\int |f-g| = 0$.  Then for any $E \in
\mathcal{M}$ we have
\[
\left | \int_E f - \int_E g \right | = \left |\int \chi_E (f-g) \right | \le \int
|f-g| = 0.
\] Conversely, assume that $\int_E f = \int_E g$ for any $E \in
\mathcal{M}$.  If $f = g$ a.e. were false, then at least one of Re$(f-g)$ or
Im$(f-g)$ would be  positive (or negative) on a set $E$ of positive
measure.  Then Re
$\left ( \int_E f - \int_E g\right ) \ne 0$, or Im$\left ( \int_E f - \int_E g
\right ) \ne 0$, which imply $\int_E f \ne \int_E g$. 


\bigskip
\noindent
\textbf{Remark.}  Due to part (c), we may change the values of a
function on a set of measure zero without changing the integral of the
function.  Therefore we identify all functions  which differ from each
other on a set of measure zero.  More precisely, we define the
equivalence relation
\[ f \sim g \mbox{ if } f = g \ \mbox{ a.e.}
\] And when we talk about an $f$, we mean its equivalence class $[f]$. 
Then we define the space $L^1(X, \mu)$ by
\[ L^1 (X, \mu) = \left \{f: X \stackrel{\mbox{meas.}}{\longrightarrow}
\mathbb{C}: \int |f| < \infty \right \}.
\] This is a vector space over $\mathbb{R}$ (or $\mathbb{C}$) and with 
metric defined by 
\[ d(f, g) = \int |f-g| \doteq \|f-g\|_1
\] becomes a metric space, which later on we will show that is
complete.


Next we shall state our third theorem that allows commutation
between limit and integral signs.


\begin{theorem} (The Dominated Convergence Theorem (DCT))
Let
$f_n: X
\longrightarrow
\mathbb{C}$ be a sequence in $L^1$ such that
\begin{itemize}
\item[a.]  $f_n \longrightarrow f$ a.e.
\item[b.]  $|f_n| \le g, \ n = 1, 2, \dots, \mbox{\textit{for some} } g \in
L^1$.
\end{itemize}
Then
\[
\lim_{n \to \infty} \int f_n = \int \lim_{n \to \infty} f_n = \int f.
\]
\end{theorem}
\textit{Proof:}  By taking real and imaginary parts we may assume that
$f_n$ are real valued.  Then
\[ |f_n| \le g \Longleftrightarrow - g \le f_n \le g \Longleftrightarrow g
+ f_n \ge 0
\mbox{ and } g - f_n \ge 0.
\] By applying Fatou's lemma to both $g + f_n$ and $g - f_n$ we obtain

\begin{eqnarray*}
\int (g + f) = \int \lim \inf (g + f_n) \le \lim \inf \int (g + f_n) = \int g +
\lim \inf \int f_n
\\
\int (g-f) = \int \lim \inf (g-f_n) \le \lim \inf \int (g - f_n) = \int g -
\lim \sup \int f_n.
\end{eqnarray*} These two inequalities imply that

\[
\lim\sup \int f_n \le \int f \le \lim \inf \int f_n
\] which gives the desired relation. $\square$


\begin{corollary}  If $f_j \in L^1$ with $\sum^\infty_{j=1} \int |f_j| <
\infty$ then
\[
\int \sum^\infty_{j=1} f_j = \sum^\infty_{j=1} \int f_j. 
\]
\end{corollary}
\textit{Proof:}  
By the MCT we have 
\[
\int \sum^\infty_{j=1} |f_j| \le \sum^\infty_{j=1} \int |f_j| < \infty.
\] If we let $g = \sum |f_j|$ and apply DCT then we obtain the desired
result.


\bigskip
\noindent
{\large\bf Application to Fourier Transform.}
If $f \in L^1(\mathbb{R}, m) \doteq L^1 (\mathbb{R})$, then
its \textbf{Fourier Transform} $\widehat f$ is defined by 
\[
\widehat f(\xi) = \int_{\mathbb{R}} e^{-ix \xi} f(x) dx, \ \xi \in
\mathbb{R}.
\]


\begin{theorem}  If $f \in L^1 (\mathbb{R})$ then
\begin{itemize}
\item[1.]  $|\widehat f(\xi)| \le \|f\|_1$
\item[2.] $f$\textit{ is continuous in} $\mathbb{R}$.
\end{itemize}
\end{theorem}
\textit{Proof:}
(a)  we have
\[ |\widehat f(\xi) | = \left | \int_{\mathbb{R}} e^{-ix \xi} f(x) dx \right
| \le \int \left | e^{-ix \xi} f(x) dx \right | = \|f\|_1
\]
(b)  Let $\xi_0 \in \mathbb{R}$ and $\{\xi_n\}$ be a sequence such
that $\xi_n
\longrightarrow \xi_0$.  Define
\[ F_n(x) = e^{-ix \xi_n} f(x), \ n = 0, 1, \dots
\]
 Then we have
\begin{itemize}
\item[a.] $F_n \longrightarrow F_0$
\item[b.]  $|F_n| \le |f|.$
\end{itemize} Thus we can apply the DCT to obtain
\[
\widehat f(\xi_n) \doteq \int_{\mathbb{R}} F_n (x) \longrightarrow
\int_{\mathbb{R}} F_0(x) \doteq \widehat f(\xi_0),
\] which shows that $\widehat f(\xi)$ is continuous.

\medskip
\noindent
Next we shall discuss approximations
of integrable functions by  simple and continuous functions in $L^1$
metric.

\begin{theorem}  Let $(X, \mathcal{M}, \mu)$ be a measure space, $f \in
L^1(X,
\mu)$, and $\varepsilon > 0$.  Then

\begin{itemize}
\item[(a)]  There is a simple function $\varphi \in L^1$ such that
\[
\|f - \varphi\|_{L^1} \doteq \int_X |f - \varphi| d \mu < \varepsilon
\]
\item[(b)]  If $X = \mathbb{R}$ and $\mu$ is a Lebesque-Stieljes
measure, then
$\varphi$ in (a) can be chosen such that

\[
\varphi = \sum^{\infty}_{j=1} a_j \chi_{E_j}
\]
 where each $E_j$ is a finite union of open  intervals.
\item[(c)]  In the Lebesgue - Stieljes case there is a continuous function
$g$ which  vanishes outside a bounded interval such that
\[
\|f-g\|_{L^1} < \varepsilon.
\]
\end{itemize}
\end{theorem}

\noindent {\em Proof:}  

\begin{itemize}
\item[(a)]  By the simple functions approximation theorem there is a
sequence of simple measurable function $\varphi_j$ such that $\varphi_j
\longrightarrow f$ and $|\varphi_j| \le |f|$.  Thus by the DCT
\[
 \int |f - \varphi_j| 
\longrightarrow 0 \mbox{ as } j \to
\infty.
\] If we choose large enough $j$ such that $\|f-\varphi_j\|_{L^1} <
\varepsilon$ and let
$\varphi = \varphi_j$ we obtain the desired result.
\item[(b)]  Let $\psi$ be a $\varphi_j$ from (a) such that then
\[
\psi = \sum^\ell_{j=1} a_j \chi_{E_j} \mbox{ and } \|f - \psi\| <
\frac{\varepsilon}{2},
\] where $a_j \ne 0 $ for all $j, $ and $E_j$ are disjoint.  We have
\[
\mu(E_j) \le \frac{1}{|a_j|} \int |\psi| \le \frac{1}{|a_j|} \int |f| < \infty.
\] Thus there exists a finite union of open intervals $A_j =
\overset{m_j}{\underset{k=1}{\cup}} I_{jk}$ such that
\[
\mu(A_j \Delta E_j) <|a_j|^{-1} 2^{-j-1} \varepsilon.
\] Since
\[
\int |\chi_{A_j} - \chi_{E_j} | = \mu(A_j \Delta E_j),
\] if we let
\[
\varphi = \sum^\ell_{j=1} a_j \chi_{A_j}
\] then we obtain

\begin{eqnarray*}
\int |f - \varphi| &\le& \int |f - \psi| + \int |\psi - \varphi|\\ &\le&
\frac{\varepsilon}{2} + \sum^k_{j=1} |a_j| \int |\chi_{A_j} - \chi_{E_j}|\\
&\le& \frac{\varepsilon}{2} + \sum^k_{j=1} |a_j| |a_j|^{-1} 2^{-j-1}
\varepsilon\\ &\le& \frac{\varepsilon}{2} + \frac{\varepsilon}{2} =
\varepsilon.
\end{eqnarray*}
\item[(c)]  Observe that for any fixed interval $I = (a, b)$ the sequence
of continuous functions $\{h_k\}$ where, $h_k = 1$ in $(a + \frac{1}{k}, b -
\frac{1}{k}), h_j (a) = h_j(b) = 0$, and linear in $(a, a + \frac{1}{n}) \cup (b
- \frac{1}{n}, b)$ converges to $\chi_I$ and 
$|h_j| \le 1$.  Therefore by   DCT$ \int |\chi_I - h_j|
\longrightarrow 0$.
\end{itemize}

\noindent Thus for any given $\varepsilon > 0$ there is $h_I$ continuous
such  that $\|\chi_I - h_I \| < \varepsilon$.  Now apply this observation to
each of the intervals $I_{jk}$ in (6) to obtain a continuous function
$h_{jk}$ such that
\[
\int |\chi_{I_{jk}} - h_{jk} | < |a_j|^{-1} 2^{-j-k-1} \varepsilon.
\] Define
\[ g = \sum^\ell_{j=1} a_j \sum^{m_j}_{k=1} h_{jk}
\] to obtain

\begin{eqnarray*}
\int |f - g| &=& \int |f - \varphi| + \int |\varphi - g|\\ &\le& \varepsilon +
\sum^\ell_{j=1} |a_j| \sum^{m_j}_{k=1} \int |\chi_{I_{jk}} - h_{jk} |\\ &\le&
 \varepsilon + \varepsilon = 2 \varepsilon.
\end{eqnarray*} This completes the proof of the theorem.  $\square$



\begin{lemma}  Let $\mu$ be a Borel-Stieljes measure in
$\mathbb{R}$.  If $E$ is a measurable set with $\mu(E) < \infty$, then for
any $\varepsilon > 0$ there exist a union of finitely  many open intervals
$A =
\overset{m}{\underset{j=1}{\cup}} I_j$ such that
\[
\mu(A \Delta E) < \varepsilon.
\]
\end{lemma}

\vspace{.5cm}
\noindent
\textbf{Solution.}  Since $\mu$ is a Lebesgue - Stieltjes measure  and
$\mu(E) < \infty$ for any $\varepsilon > 0 $ there exists family of open
intervals
$\{I_j\}^\infty_{j=1}$ such that $E \subset
\overset{\infty}{\underset{j=1}{\cup}}  I_j$ and
\[
\sum^\infty_{j=1} \mu(I_j) \le \mu(E) + \frac{\varepsilon}{2} < 
\infty
\] Since $\sum\limits^\infty_{j=1} \mu(I_j) < \infty$ there exist $J$  such
that
\[
\sum^\infty_{j = J+ 1} \mu(I_j) < \frac{\varepsilon}{2}.
\] If we let $A = \overset{N}{\underset{j=1}{\cup}} I_j$ then we have
\begin{eqnarray*}
\mu(A \Delta E) &\le& \mu \left ( \overset{\infty}{\underset{j=1}{\cup}}
I_j - E\right ) + \mu (E-A)\\  &=& \mu \left (
\overset{\infty}{\underset{j=1}{\cup}} I_j\right ) - \mu(E) + \mu(E-A)\\
 &\le& \sum^\infty_{j=1} \mu(I_j) - \mu(E) + \mu \left (
\overset{\infty}{\underset{j= J+1}{\cup}} A_j
\right )\\ &\le& \frac{\varepsilon}{2} + \sum^\infty_{j = J + 1} \mu(I_j) <
\varepsilon
\end{eqnarray*}

Next we shall consider integrals depending on a parameter.

\begin{theorem} Let $f: X \times [a,b] \longrightarrow
\mathbb{C}, - \infty < a < b < \infty$, be such that
\[ f(\cdot, t): X \longrightarrow \mathbb{C} \mbox{ is }  L^1
\mbox{ for all } t
\in [a,b].
\]
\begin{itemize}
\item[(a)]  If $|f(x, t)| \le g(x)$, for all $ x, t$, for some $g \in L^1$, and
\[
\lim_{t \to t_0} f(x, t) = f(x, t_0), \mbox{ all } x.
\] Then
\[
\lim_{t \to t_0} \int f(x, t) dx = \int \lim_{t \to t_0} f(x, t) dx.
\] In particular, if $f(x, \cdot)$ is continuous in $t$ for each fixed $x
\in X,$ then the function
\[ F(t) = \int f(x, t) dx
\] is continuous.
\item[(b)]  If $\frac{\partial f}{\partial t}$ exists and there is a $G
\in L^1$ such that
\[
\left |\frac{\partial f}{\partial t} (x, t) \right | \le G(x), \mbox{ for all } x,
t,
\] then $F(t)$ is differentiable and
\[
\frac{\partial f}{\partial t} \int f(x, t) dx = \int \frac{\partial f}{\partial
t} (x, t) dx.
\]
\end{itemize}
\end{theorem}

\medskip
\noindent {\em Proof:} 
\begin{itemize}
\item[(a)]  Let  $t_n \longrightarrow t_0$.  Then the $f_n(x) = f(x,  t_n)
\longrightarrow f_0 (x) = f(x, t_0)$ and  $\left |f_n(x) \right |
\le g(x)$.  Therefore by DCT we obtain the desired result.
\item[(b)]  Let $t_n \longrightarrow t_0$, and define the sequence
\[ h_n (x) = \frac{f(x, t_n) - f(x, t_0)}{t_n - t_0}.
\]
\end{itemize} Since $\frac{\partial f}{\partial t}$ exists we have that
\[ h_n (x) \longrightarrow h_0(x) = \frac{\partial f}{\partial t} (x, t_0),
\mbox{ all } x.
\] Also, by the mean value theorem, we have
\[ |h_n(x)| \le \left | \frac{\partial f}{\partial t} (x, \xi(x, t_n)) \right | |t -
t_0| \le CG(x).
\] Thus by the DCT we obtain the desired result.


\begin{theorem}  For  a bounded function $f$   on an interval $[a, b], -
\infty < a < b < \infty$ the following hold.
\begin{itemize}
\item[(a)]  If $f$ is Riemann integrable, then $f$ is measurable and
therefore in
$L^1$ with 
\[
\int_{[a,b]} f d m = \int^b_a f(x) dx, 
\] where the integral in the RHS is Riemann.
\item[(b)]  $f$ is Riemann integrable if  the set of its discontinuities
\[D_f = \left \{x: f \mbox{ discontinuous  at } x \right \}
\] has Lebesgue measure zero.
\end{itemize}
\end{theorem}

\medskip
\noindent {\em Proof:} (a)\quad Assume that $f$ is Riemann integrable. 
For each $k$ let 
$P_k$ be a partition defined by $a = x^k_1 < x^k_2 < \dots < x^k_{N_k} =
b$ such that
$\max_j |x^k_{j+1} - x^k_j| \longrightarrow 0$ as $k \to \infty$, and
$\ell_k (x), u_k (x)$ be defined by
\begin{eqnarray*}
\ell_k (x) &=& \sum^{N_k -1}_{j=1} \left [ \twostack{\inf}{x_j^k \le x \le
x^k_{j+1}} f(x)\right ]
\chi_{[x^k_j, x^k_{j+1})}\\ u_k(x) &=& \sum^{N_k-1}_{j=1} \left
[\twostack{\mbox{suf}}{x^k_j \le x \le x^k_{j+1}} f(x)
\right ]
\chi_{[x^k_j, x^k_{j+1})}.
\end{eqnarray*} If $P_{k+1}$ is a refinement of $P_k$, i.e. $P_k \subset
P_{k+1}$,  then both $\ell_k$ and $u_k $ are uniformly bounded, $\ell_k$
is increasing, $u_k$ is decreasing,
$\ell_k \le u_k$ and by the DCT
\[
\int \lim \ell_k= \int \lim u_k = \int^b_a f(x) dx
\] since $\lim \ell_k$ and $\lim u_k$ are measurable and 
\[
\lim \ell_k \le f \le \lim u_k
\] we have $\lim u_k - \lim \ell_k = 0$ a.e.  Therefore $f = \lim u_k$ a.e.,
which shows that $f$ is measurable and in $L^1$.

 \medskip
\noindent (b)\quad  Assume $f$ is Riemann integrable and let
\[ Z = [a,b] - \{x: \ell = f = u\},
\] where $\ell = \lim \ell_k$ and $u = \lim u_k$.  We have $m(Z) = 0$.  
Denote by $E$ the set of all endpoints of all partitions $P_k$.  We have
$m(E) = 0$.  We shall show that if $x \notin Z \cup E$ then  $f$ is
continuous at $x$.  If not, then there is
$\varepsilon > 0$ such that $u_k (x) - \ell_k(x) > \varepsilon$ for all
$k$, which gives $u(x) - \ell (x) \ge \varepsilon$, which contradicts the
fact that $x \notin Z$.


To prove the converse, assume that $f$ is continuous a.e. in $[a,b]$.  Let
$P_k$ be a sequence of partitions of $[a,b]$ with their mesh going to
zero, and let $\ell_k$ and
$u_k$ be as in (a).  If $f$ is continuous at $x$ then both $\ell_k$ and 
$u_k$ converge to $f(x)$.  Since $m(D_f) = 0$ we have that $\ell_k
\longrightarrow f$ a.e. and $u_k \longrightarrow f$ a.e.  Then by the DCT
\[
\int^b_a \ell_k \longrightarrow \int^b_a f \mbox{ and } \int^b_a u_k
\longrightarrow \int^b_a f
\] This implies that $f$ is Riemann integrable.

\bigskip
\noindent {\bf Relation Between Riemann Improper  and
Lebesgue's Integral.} The Riemann integral is defined only for a subclass
of function with bounded domain and bounded range.  If either the
domain or the range is unbounded then the improper Riemann integral
may be defined as a limit of Riemann integrals provided such a limit
exists.  For example, if $f: \mathbb{R} \longrightarrow
\mathbb{C}$ and  $f$ is Riemann integrable on every subinterval of
$\mathbb{R}$ then the improper integral of $f$ is defined by
\[
\int^\infty_{- \infty} f(x) dx = \lim_{M \to \infty} \int^M_{-M} f (x)  dx,
\mbox{provided that this limit exists.}
\]

\begin{examp}  $\displaystyle{\int^\infty_{- \infty} e^{-x^2} dx =
\lim_{M \to
\infty} \int^M_{-M} e^{-x^2} dx}.$
\end{examp} While if a function $f: (a, b] \longrightarrow \mathbb{C}$ is
unbounded near $a$, but $f$ is Riemann integrable on every subinterval
$(\alpha, b]$ of $(a, b]$ then the improper integral of $f$  is defined by
\[
\int^b_a f(x) dx = \lim_{\alpha \to a} \int^b_\alpha f(x) dx, \mbox{
provided that this limit exists.}
\]
\begin{examp}  $\displaystyle{\int^1_0 \frac{1}{\sqrt{x}} dx =
\lim_{\alpha \to 0} \int^1_\alpha \frac{1}{\sqrt{x}} dx.}$
\end{examp} If a function is Lebesgue integrable,  and Riemann
integrable on the  subintervals used in the definition of the improper
integral then its improper integral exists and is finite. On the other
hand, the improper integral may be finite without the function to be
Lebesgue integral.

For example, if
\[ f = \sum^\infty_{j=1} \frac{(-1)^j}{j} \chi_{(j, j+1]}
\] then improper integral $\int^\infty_1 f(x) dx$ is finite, while $f$ is 
not in $L^1(1, \infty)$.   Another interesting example is given by the
function
\[ f(x) = \frac{\sin x}{x}, \ - \infty < x < \infty.
\] The improper integral $\int^\infty_{- \infty} f(x) dx$ is finite while 
$\notin L^1(\mathbb{R})$.


\begin{exercise}  Prove that $\displaystyle f(x) = \frac{\sin x}{x} \notin
L^1(\mathbb{R})$ but its improper integral is finite.
\end{exercise}




\section{ Modes of Convergence}
 Assume $(X, \mathcal{M}, \mu)$ be a measure space and $f_n: X
\longrightarrow
\mathbb{C}$ be a sequence of measurable functions.  So far we have seen
the following types of convergence.
\begin{itemize}
\item[a.]  uniform in $X$
\item[b.] pointwise in $X$
\item[c.] a.e. in $X$
\item[d.] $L^1$-convergence.
\end{itemize} Next we shall define another useful notion of convergence
with the help of the measure $\mu$.  We say that  $\{f_n\}$
\textbf{converges  in measure} to a function $f$, denoted by $f_n
\stackrel{\mu}{\longrightarrow} f$, if for any $\varepsilon > 0$
\[
\mu\left (\{x \in X: |f_n (x) - f(x) | \ge \varepsilon \}\right )
\longrightarrow 0
\mbox{ as } n \to \infty.
\]
  We say that
$\{f_n\}$ is Cauchy in measure if for any $\varepsilon > 0$.
\[
\mu\left ( \left \{x \in X: |f_n(x) - f_m (x) | \ge \varepsilon \right  \}
\right )
\longrightarrow 0 \mbox{ as } n, m \to \infty.
\]
\bigskip
\noindent
\textbf{Basic Relations.}  If $(X, \mathcal{M}, \mu)$ is a measure  space
then:

\begin{itemize}
\item[1.]  uniform convergence $\not\Longrightarrow L^1$-convergence.
($f_n = n^{-1}\chi_{(0, n)}$)
\item[2.]  $L^1$-convergence $\not\Longrightarrow$ a.e. convergence.
\item[3.] $L^1$-convergence $\Longrightarrow$ convergence in measure.
\item[4.]  convergence in measure $\not\Longrightarrow
L^1$-convergence. ($f_n = n\chi_{(0, 1/n)}$)
\item[5.]  a.e. convergence $\not\Longrightarrow$ conv. in measure 
($f_n = \chi_{(n, n+1)}$)
\end{itemize}


\noindent {\em Proof:} (1)  In $(\mathbb{R}, \mathcal{L}, m)$ let $f_n =
\frac{1}{n} \chi_{(0, n)}$ then
$f_n
\longrightarrow 0$ uniformly but $\int f_n = 1 \not\rightarrow 0$.
 
\smallskip
\noindent (2)  On [0,1] consider the sequence $f_n$ defined by
\begin{eqnarray*} f_1 &=& \chi_{[0,1]}\\ f_2 &=& \chi_{\left [0,
\frac{1}{2} \right ]}, f_3 = \chi_{\left [ \frac{1}{2}, 1\right ]} \qquad
\\ f_4 &=& \chi_{\left [ 0, \frac{1}{2^2} \right ]}, \ f_5 = \chi_{\left [
\frac{1}{2^2},
\frac{2}{2^2} \right ]}, \ f_6 = \chi_{\left [ \frac{2}{2^2}, \frac{3}{2^2}
\right ]},
\ f_7 = \chi_{\left [ \frac{3}{2^2}, 1\right ]}\\ f_8 &=& \chi_{\left [
0,\frac{1}{2^3}\right ]} , \ f_9 = \chi_{\left [
\frac{1}{2^3},
\frac{2}{2^3} \right ]} , \dots.\end{eqnarray*} Then $\int f_n =
\frac{1}{2^k} \longrightarrow 0 $ as $k \to \infty$, but
$f_n
\not\longrightarrow 0$ a.e. since for every $x \in [0,1]$ there are
infintely many
$f_n$ with $f_n (x) = 1.$

\smallskip
\noindent (3)  Assume $\int_X |f_n - f| \longrightarrow 0$, and let
$\varepsilon > 0$.  Define
\[E_{n, \varepsilon} \doteq \{x \in X: |f_n (x) - f(x) | \ge \varepsilon \}.
\] Then
\[
\int_X |f_n - f| \ge \int_{E_{n, \varepsilon}} |f_n - f| \ge \int_{E_{n,
\varepsilon}}
\varepsilon = \varepsilon \mu(E_{n, \varepsilon}).
\] If we let $n \to \infty$ we obtain $\varepsilon \mu (E_{n, \varepsilon})
\longrightarrow 0, $ or $\mu(E_{n, \varepsilon}) \longrightarrow 0$,
which is convergence in measure of $f_n$.

\smallskip
\noindent (4)   Again by using $f_n = \frac{1}{n} \chi_{(0, n)}$ we see that
$f_n
\longrightarrow 0$ in measure, but $f_n \not\rightarrow 0 $ in $L^1$.

\begin{theorem} Let $(X, \mathcal{M}, \mu)$ a measure space, and
$f_n: X \longrightarrow \mathbb{C}$ a sequence of functions.  Then
\begin{itemize}
\item[1.]   $f_n \longrightarrow f$ in measure $\Longleftrightarrow
\{f_n\}$ is Cauchy in measure.
\item[2.]  $f_n \longrightarrow f$ in measure $\Longrightarrow $ there is
a subsequence $f_{n_j} \to f$ a.e.
\end{itemize}
\end{theorem}

\medskip
\noindent {\em Proof:} \textbf{1.}  Assume that $f_n
\stackrel{\mu}{\longrightarrow} f$. Since for any $\varepsilon > 0 $
\begin{eqnarray*}
\left \{x: |f_n(x) - f_m(x) | \ge \varepsilon \right \} &\subset&
\left \{x: |f_n(x) - f(x)|\ge\frac{\varepsilon}{2}\right \}\\ &\cup& \left
\{x : |f_m(x) - f(x) | \ge \frac{\varepsilon}{2}\right \}
\end{eqnarray*} we have that $ \{f_n\}$ is Cauchy in measure.

Conversely, assume that $\{f_n\}$ is Cauchy in measure.  Then for each $j
\in
\mathbb{N}$ we can choose $f_{n_j} \doteq	 g_j$ and $f_{n_{j+1}} \doteq
g_{j+1}$ such that
\[ E_j \doteq \{x: |g_j(x) - g_{j+1} (x)| \ge 2^{-j}\} \Longrightarrow
\mu(E_j) \le 2^{-j}
\] The $\{f_{n_j}\}$ is chosen to form a subsequence of $\{f_n\}$.  Then
define
\[ F_k = \overset{\infty}{\underset{j=k}{\cup}} E_j.
\] We have
\[
\mu(F_k) \le \sum^\infty_{j=k} \mu(E_j) \le \sum^\infty_{j=k} 2^{-j} =
2^{1-k}.
\] If $ x \in F^c_k $ and $i \ge j \ge k$ then 
\[ (\ast) \qquad  |g_i(x) - g_j(x)| \le \sum^{i-1}_{\ell=j} |g_{\ell + 1} (x) -
g_\ell (x)|
\le \sum^{i-1}_{\ell = j}2^{-\ell} \le 2^{1 - j},
\] which implies that $\{g_j\}$ is pointwise Cauchy in $F^c_k$, and thus
converges in $F^c_k$.  Now let
\[ F = \overset{\infty}{\underset{k=1}{\cap}} F_k = \lim \sup E_j
\] We have
\[\mu(F) \le \mu(F_k) \le 2^{1-k} , \mbox{ all } k\Longrightarrow \mu(F)
= 0.
\] Thus $\{g_j\}$ converges on $F^c$ to a function $f(x)$. If we define 
$f(x) = 0$ for
$x \in F$ then we have $g_j \longrightarrow f$ a.e. in $X$.  Thus $f$ is
measurable.  This shows part (2).  To show that $g_j
\stackrel{\mu}{\longrightarrow } f$ we let $i \rightarrow \infty$ in
$(\ast)$ and we obtain
\[ |g_j(x) - f(x) | \le 2^{1 - j}, \ j \ge k, \ x \in F^c_k.
\] which implies that $g_j \stackrel{\mu}{\longrightarrow} f$ since
$\mu(F_k) \to 0$ as $k \to \infty$.  Finally we obtain that $f_n
\longrightarrow f$ from the relation
\begin{eqnarray*}
\{x: |f_n(x) - f(x) | \ge \varepsilon \} &\subset& \left \{x: (f_n(x) - g_j(x)|
\ge
\frac{\varepsilon}{2}\right \}\\ &\cup& \left \{x: |g_j (x) - f(x) | \ge
\frac{\varepsilon}{2} \right \}.
\end{eqnarray*}


\begin{corollary} $f_n \longrightarrow f$ in $L^1$ then there is a
subsequence $\{f_{nj}\}$ such that $f_{nj}
\longrightarrow f$ a.e.
\end{corollary}

\begin{corollary} If $(X,\mathcal{M},\mu)$  is a measure space, then
$L^1(X)$ is complete.
\end{corollary}
{\bf Proof.} Let $\{f_{n}\}$ be a cauchy sequence in $L^1$. Then there
is a subsequence $\{f_{n_j}\}$  converging a.e. to  a function $f$.
Applying Fatou's lemma  for each fixed $n$ we obtain
\[
\int  |f_n -f|=\int \lim \inf_{j} |f_n -f_{n_j}|\le \lim \inf_{j} \int
|f_n -f_{n_j}|\le
\varepsilon,
\]
for any given $\varepsilon$ if $n>N(\varepsilon)$. $\square$

\bigskip If $f_n = \chi_{(n, n+1)}$ then $f_n \longrightarrow 0 $ but $f_n
\not\longrightarrow 0$ in measure.  This happens since $m(\mathbb{R})
= \infty$.  If the measure of the space $X$ is finite, then pointwise
convergence implies convergence in measure.  In fact, the following
stronger result holds.

\begin{theorem}[Egoroff's]  Let $(X, \mathcal{M},
\mu)$ be a measure space with $\mu(X) < \infty$, and $f_n: X
\longrightarrow \mathbb{C}$ be a sequence of measurable functions
converging to $f: X \longrightarrow
\mathbb{C}$ a.e.  Then for any $\varepsilon > 0 $ there exist $E \in
\mathcal{M}$ such that
\[
\mu(E) < \varepsilon \mbox{ and } f_n \longrightarrow f \mbox{
uniformly on } E^c.
\]
\end{theorem}

\noindent {\em Proof:  }  By defining $f_n(x)$ to be a fixed number for all
$n$ and all
$x$ when $\{f_n(x)\}$ does not converge we may assume that $f_n  (x)
\longrightarrow f(x)$ for all $x \in X$.  For $k, n \in \mathbb{N}$ let $E_n
(k)$ be the ``bad'' set
\[ E_n (k) \doteq \overset{\infty}{\underset{m=n}{\cup}} \{x \in X:
|f_m(x) - f(x) |\ge\frac{1}{k} \}.
\] Since $f_n(x) \longrightarrow f(x), x \in X$, we have that
\[
\overset{\infty}{\underset{n=1}{\cap}}  E_n (k) = \emptyset.
\] Since $E_n(k)$ is decreasing as $n$ increases we have
\[
\lim_{n \to \infty} E_n (k) = 0.
\] Let $\varepsilon > 0$.  Then for each $k$ there exists $n_k$ such that
\[
\mu(E_{n_k}(k)) \le \varepsilon 2^{-k}.
\] If we let $E = \overset{\infty}{\underset{k=1}{\cup}}  E_{n_k}(k)$ then
\[
\mu(E) \le \sum^\infty_{k=1} \mu(E_{n_k} (k)) \le \sum \varepsilon
2^{-k} \le \varepsilon,
\] and
\[ |f_n(x) - f(x) | < \frac{1}{k}, \ n \ge n_k, \ \forall x \in E^c,
\] which shows that $f_n \longrightarrow f$ uniformly on $E^c$.


\begin{corollary}  If $\mu(X) <
\infty$ and $f_n: X \longrightarrow
\mathbb{C}$ measurable with $f_n \longrightarrow f$ a.e., then $f_n
\longrightarrow f$ in measure.
\end{corollary}


\section{Product measures}
 Here we shall prove Fubini's theorem which formally stated is
\[
\int \int f(x, y) dx dy = \int \left ( \int f(x, y) dx \right ) dy = \int
\left (
\int f(x, y) dy \right ) dx.
\] For this we shall need to define a ``product'' measure on a product  of
measure spaces.

Let $(X, \mathcal{M}, \mu)$ and $(Y, \mathcal{N}, \nu)$ be two measure
spaces.  Then as we have seen before the product $\sigma$-algebra
$\mathcal{M} \otimes \mathcal{N} $ is generated by the family of the
\textbf{rectangles}
\[
\mathcal{E} = \{A \times E: A \in \mathcal{M}, E \in \mathcal{N}\}.
\]

\begin{lemma}   $\mathcal{E}$ is an elementary family on $X \times Y$.
\end{lemma}

\medskip
\noindent {\em Proof:}  If $A \times E$ and $B \times F$ are two
rectangles then their intersection is a rectangle, since 
$(A \times E)
\cap (B \times E) = (A \cap B) \times (E \cap F).$  Also the compliment of
a rectangle $A
\times E$ is written as
\[(A \times E)^c = (X \times E^c) \cup (A^c \times E),
\] which is a rectangle.

\medskip
\noindent If $\mathcal{A}$ is the collection of all finite disjoint union of
rectanlges then by Proposition 1.7, $\mathcal{A}$ is an algebra.  To
motivate the definition of a premeasure in $\mathcal{A}$ assume that $A
\times E$ is a rectangle which is a union of the disjoint rectangles $\{A_j
\times E_j\}^\infty_{j=1}$ i.e.
\[ A \times E = \bigcup^\infty_{j=1} (A_j \times E_j), \mbox{ disjoint.}
\] Then
\[
\chi_A(x) \chi_E(y) = \chi_{A \times E} (x, y) = \sum_j \chi_{A_j \times
E_j} (x, y) = \sum_j \chi_{E_j} (x) \chi_{E_j} (y).
\] Integrating with respect to $x$ and using the MCT gives
\[
\mu(A) \chi_E(y) = \sum_j \mu(A_j) \chi_{E_j} (y).
\] Then integrating with respect to $y$ gives
\begin{equation*}
\mu(A) \nu(E) = \sum_j \mu(A_j) \nu(E_j)\tag{1}
\end{equation*} Therefore if $\overset{n}{\underset{j=1}{\cup}}   (A_j
\times E_j)$ is a finite union of disjoint rectangles, then we define
\begin{equation*}
\pi\left ( \bigcup^n_{j=1} (A_j \times E_j \right ) = \sum^n_{j=1}
\mu(A_j) \cdot \nu(E_j)\tag{2}
\end{equation*}

\begin{lemma} $\pi$ is a premeasure on $\mathcal{A}$.
\end{lemma}

\noindent {\em Proof:}  We show that $\pi$ is well defined i.e. if
$\overset{n}{\underset{j=1}{\cup}}  A_j \times E_j =
\overset{m}{\underset{k=1}{\cup}}  B_k \times F_k$ and disjoint, then
 \[
\sum^n_{j=1} \mu(A_j) \nu(E_j) = \sum^m_{k=1} \mu(B_k) \nu(F_k).
\] For this it suffices to find a common refinement of both unions. i.e.  we
write
\[
\overset{n}{\underset{j=1}{\cup}}  A_j \times E_j =
\overset{n}{\underset{j=1}{\cup}} 
\overset{m}{\underset{k=1}{\cup}}  (A_j
\cap B_k) \times (E_j \cap F_k) = \overset{m}{\underset{k=1}{\cup}} 
B_k \times F_k
\] and use (1).  To complete the proof we need to show additivity of $\pi$
which follows also from (1).


\begin{exercise}  Complete the proof of the above lemma.
\end{exercise}

\medskip
\noindent Then $\pi$ defines an outer measure $\pi^*$ on $\mathcal{P}(X
\times Y)$.  That is, for $B \subset X \times Y$
\begin{eqnarray*}
\pi^* (B) &\doteq& \inf \left \{\sum^\infty_{j=1} \pi (B_j): \{B_j\}
\subset \mathcal{A} \mbox{ with } \bigcup^\infty_{j=1} B_j \supset
B\right \}\\ &\twostack{(2)}{=}& \inf \left \{\sum^\infty_{j=1} \mu(A_j)
\nu(E_j): A_j
\in \mathcal{M}, E_j \in \mathcal{N} \mbox{ and } \bigcup (A_j \times E_j)
\supset B\right \}
\end{eqnarray*}

\begin{exercise}  Prove (2).
\end{exercise}

\medskip
\noindent Then by Caratheodory's definition a subset $B \subset X
\times Y$  is
$\pi^*$-measurable if
\[
\pi^* (G) = \pi^* (G \cap B) + \pi^*(G \cap B^c), \ \forall G \in \mathcal{P}
(X \times Y).
\] By Caratheodory's Theorem the $\pi^*$-measurable sets form a
$\sigma$-algebra and $\pi^*|_{\mathcal{A}} = \pi$.  Therefore
$\mathcal{M} \otimes \mathcal{N}$, which is the algebra generated by
$\mathcal{A}$ is contained in $\pi^*$-measurable sets.  The restriction
of $\pi^*$ on $\mathcal{M} \otimes \mathcal{N}$ is called the
\textbf{product} of the measures $\mu$ and $\nu$,  denoted by
$\mu \times \nu$.  As we shall see soon $\mu \times \nu$ is almost 
never complete.  However it is $\sigma$-finite if both $\mu$ and
$\nu$ are
$\sigma$-finite.  In fact, if $X = \overset{\infty}{\underset{j=1}{\cup}} 
A_j$ with
$\mu(A_j) < \infty$, and $Y = \overset{\infty}{\underset{k=1}{\cup}}  E_k$
with
$v(E_j < \infty)$.  Then
 \[ X \times Y = \overset{\infty}{\underset{j, k =1}{\cup}}  A_j \times E_k
\mbox{ with } \mu
\times \nu (A_j \times E_k) < \infty.
\]
 Next, if $E \subset X \times Y$ then for each $x \in X$ we define
the
\textbf{$\mathbf{x}$-section} $E_x$ of $E$ by
\[ E_x = \{y \in Y: (x, y) \in E\}\subset Y, 
\]
and the $\mathbf{y}$-\textbf{section} $E^y$ of $E$ by
\[ E^y = \{x \in X: (x, y) \in E\} \subset X.  
\]
Similarly, if $f(x, y)$ is a function from $X \times Y\to Z$ then we define
the
$\mathbf{x}$-\textbf{section} $f_x$  and the
$\mathbf{y}$-\textbf{section}
$f^y$ of
$f$ by
\[ f_x (y) = f(x, y) \mbox{ and } f^y (x) = f(x, y).
\]
\begin{prop}
\begin{itemize}
\item[a.]  $E \in \mathcal{M} \otimes \mathcal{N} \Longrightarrow E_x \in
\mathcal{N}, \ \forall x \in X$ and $E^y \in \mathcal{M}, \ \forall y \in Y.$
\item[b.] $f$ is $\mathcal{M} \otimes \mathcal{N}$ measurable
$\Longrightarrow f_x$ is $\mathcal{N}$-measurable for all $x \in X$, and 
$f^y$ is
$\mathcal{M}$-measurable for all $y \in Y$.
\end{itemize}
\end{prop}

\medskip
\noindent {\em Proof:}  (a)  Let
\[
\mathcal{R} = \{E \subset X \times Y: E_x \in \mathcal{N}, \ \forall x \in X,
\mbox{ and } E^y \in \mathcal{M}, \ \forall y \in Y\}.
\] Then every rectangle $A \times B \in \mathcal{M} \otimes
\mathcal{N}$ is contained in $\mathcal{R} $ since
\[ (A \times B)_x = \left \{ \begin{array}{ll} B, &x \in A\\
\emptyset, &x \notin A \end{array} \right . \mbox{ and } (A \times B)^y
= \left
\{ \begin{array}{ll} A, &y \in B\\
\emptyset, &y \notin B.\end{array}\right .
\] Also if $E_j \in \mathcal{R}$ then
$\overset{\infty}{\underset{j=1}{\cup}} E_j \in
\mathcal{R}$ since 
\[
\left ( \overset{\infty}{\underset{j=1}{\cup}}E_j\right )_x =
\overset{\infty}{\underset{j=1}{\cup}} (E_j)_x \mbox{ and } \left (
\overset{\infty}{\underset{j=1}{\cup}} E_j \right )^y =
\overset{\infty}{\underset{j=1}{\cup}} (E_j)^y.
\] 
Also it easily seen that   $E\in \mathcal{R}$ implies that
  $E^c\in \mathcal{R}$.
Thus $\mathcal{R}$ is a $\sigma$-algebra containing all rectangles.  
 Therefore
$\mathcal{R}\supset \mathcal{M} \otimes \mathcal{N}$.

\noindent (b)
  Assume that $f: X \times Y \longrightarrow Z,$ where $Z$ is a
measurable space.  Then for any $B \subset Z$ we have
\[ (f_x)^{-1} (B) = (f^{-1} (B))_x \mbox{ and } (f^{-1})^y (f^{-1} (B))^y.
\] If $f$ is $\mathcal{M} \times \mathcal{N}$ measurable then $f^{-1}(B)
\in
\mathcal{M} \otimes \mathcal{N}$ and the above relations together with
(a) imply that $f_x$ is $\mathcal{N}$-measurable, and $f^y$ is
$\mathcal{M}$-measurable.

\begin{corollary} $\mu \times \nu$  is almost never complete, even if
both $\mu$ and $\nu$ are complete.
\end{corollary}

\noindent {\em Proof:}  Let $\emptyset \notin Z \in \mathcal{M}$ such
that
$\mu(Z) = 0$ and $E \in \mathcal{P} (Y) - \mathcal{N}$.  Then $B
\doteq Z \times E \subset Z
\times Y$ and $\mu \times \nu(Z \times Y) = 0$. 
 By (a) $B \notin \mathcal{M}
\otimes \mathcal{N}$ since for all $x \in Z$ we have $B_x = E \notin
\mathcal{N}$.


\begin{definition}  A \textbf{monotone class} on a set $X$ is a subset
$\mathcal{C} \in\mathcal{P} (X)$ which is closed  under countable increasing 
union and countable decreasing intersections, i.e. it satisfies the properties
\begin{itemize}
\item[a.] $E_j \in \mathcal{C}$ and $E_j \subset E_{j+1} \Longrightarrow
\overset{\infty}{\underset{j=1}{\cup}} E_j \in \mathcal{C}$.
\item[b.] $E_j \in \mathcal{C} $ and $E_{j+1} \subset E_j \Longrightarrow
\overset{\infty}{\underset{j=1}{\cap}} E_j \in \mathcal{C}.$
\end{itemize}
\end{definition}

\begin{examp}  Every $\sigma$-algebra on $X$ is a monotone class.

\medskip
\noindent If $\mathcal{E} \subset \mathcal{P} (X)$ then the intersection
of all  monotone classes containing  $\mathcal{E}$ is a monotone, called
the monotone class generated by $\mathcal{E}$.  It is  the smallest
monotone class containing
$\mathcal{E}$.
\end{examp}

\begin{lemma}  If $\mathcal{A}$  is an algebra on
$X$ then the monotone class generated by $\mathcal{A}$  is equal to the
$\sigma$-algebra generated by $\mathcal{A}$.
\end{lemma}

\noindent {\em Proof:}  See Book.

\medskip
\noindent The next result is the Fubini-Tonelli theorem for a
characteristic  function of a set
$E \in \mathcal{M} \otimes \mathcal{N}$.

\begin{theorem}  Let $(X, \mathcal{M}, \mu)$ and $(Y,
\mathcal{N},
\nu)$ be two $\sigma$-finite measure spaces.  If $E \in \mathcal{M} 
\otimes
\mathcal{N}$ then the functions $x \longmapsto \nu(E_x)$ and $y 
\longmapsto
\mu(E^y)$ are measurable on $X$ and $Y$, respectively and 
\[
\mu \times \nu(E) = \int \nu(E_x) d\mu(x) = \int \mu(E^y) d \nu(y).
\]
\end{theorem}

\noindent {\em Proof:}  First we shall assume that $\mu(X) < \infty$ and
$\nu(Y) <
\infty$.  Let
\[
\mathcal{C} = \{E \in \mathcal{M} \otimes \mathcal{N}: \mbox{theorem
holds for } E\}.
\] If $E = A \times B$ is a rectangle then  $\nu(E_x) = \chi_A (x) \nu(B)$ is
$\mathcal{M}$-measurable, $\mu(E^y) = \mu(A) \chi_B(y)$ is
$\mathcal{N}$-measurable, and 
\begin{eqnarray*}
\mu \times \nu (E) &=& \mu(A) \nu(B) = \int \chi_A (x) \nu(B) d \mu(x) =
\int
\nu(E_x) d \mu(x)\\ &=& \int \mu(A) \chi_B (y) d \nu(y) = \int \mu(E^y) d
\mu(y).
\end{eqnarray*} Thus $A \times B \in \mathcal{C}$.  By the additivity of
measures and integrals, we see that $E \in \mathcal{C}$ if $E \in
\mathcal{A}$, where  $\mathcal{A}$ is the algebra generated by 
the rectangles. 
 Next let
$E_j
\in \mathcal{C}$ such that $E_j \subset E_{j+1}$. 
 We will show that $E =\overset{\infty}{\underset{j=1}{\cup}}E_j  \in
\mathcal{C}$.  If $f_j(x) \doteq
\nu((E_j)_x)$ then $f_j \ge 0 $ is a measurable and increasing sequence
on $X$ and such that
$f_j(x) \longrightarrow f(x) = \nu(E_x)$.  Then $x \longmapsto \nu(E_x)$
is $\mathcal{M}$-measurable and by the  M C T
\begin{eqnarray*}
\int \nu(E_x) d \mu(x) &=& \lim \int \nu((E_j)_x) d \mu (x). \\ &=& \lim
\mu \times \nu (E_j) = \mu \times \nu(E).
\end{eqnarray*} 
Similarly we show that 
\[
\int \mu(E_y) d \nu(y) = \mu \times \nu(E).
\] Now let $E_j \in \mathcal{C}$ such that $E_{j+1} \subset E_j$.  We will
show that $E =\overset{\infty}{\underset{j=1}{\cap}}E_j \in
\mathcal{C}$.    We have that the function
$g_j(x) = \nu((E_j)_x)$ is in $L^1(x)$ since  $0 \le g_j(x) \le \nu(Y)$, and
$\mu(X) < \infty$.  Since $g_j (x) \searrow g(x) \doteq \nu(E_x)$ we have
$x
\longmapsto \nu(E_x)$ is $\mathcal{M}$-measurable and by the  
DCT we have
\begin{eqnarray*}
\int \nu(E_x) d \mu(x) &=& \lim \int \nu((E_j)_x) d \mu(x)\\ &=& \lim
\mu
\times \nu(E_j) = \mu\times \nu(E).
\end{eqnarray*} Similarly we show that $y \longmapsto \mu(E^Y)$ is
$\mathcal{N}$-measurable and that
\[
\int \mu(E^y) d \nu(y) = \mu \times \nu (E).
\] Therefore $E \in \mathcal{C}$, and this shows that $\mathcal{C}$ is a
monotone class. Since $\mathcal{A} \subset \mathcal{C}$ we have
$\mathcal{M} \otimes
\mathcal{N} \subset \mathcal{C}$.  This completes the proof of the
theorem in the case that both $X$ and $Y$ have finite measures.  

\vspace{.2cm} 
For the general case we write
\[ X = \overset{\infty}{\underset{j=1}{\cup}} X_j \times Y_j, \ X_j \times
Y_j
\subset X_{j+1}
\times Y_{j+1}, \mbox{ and } \mu(X_j) + \nu(Y_j) < \infty.
\] If $E \in \mathcal{M} \times \mathcal{N}$ then by applying the  finite
case for each $j$ to $E \cap (X_j \times Y_j)$ and the MCT we obtain
\begin{eqnarray*}
\mu \times \nu(E) &=& \lim_{j \to \infty} \mu \times \nu(E \cap (X_j
\times Y_j))\\ &=& \lim_{j \to \infty} \int \nu \left [ (E \cap (X_j \times
Y_j))_x\right ] d
\mu(x)\\ &=& \lim_{j \to \infty} \int \chi_{X_j} \nu (E_x \cap Y_j) d
\mu(x)\\ &=& \int \chi_X \nu(E_x \cap Y) d \mu(x) = \int_X \nu(E_x) d
\mu(x) 
\end{eqnarray*} Similarly we obtain
\[
\mu \times \nu(E) = \int_Y \mu(E^y) d \mu(y).
\] 
This completes the proof of the theorem. \,$\square$


\vspace{.25cm}
Next we shall discuss the Fubini-Tonelli Theorem.
Thus far we have shown that if $(X, \mathcal{M},
\mu)$ and  $(Y, \mathcal{N},
\nu)$ are two $\sigma$-finite measure spaces, and if $E \in
\mathcal{M} \otimes \mathcal{N}$ then
\begin{itemize}
\item[1.]  $x \longmapsto \nu(E_x)$ is measurable in $X$.
\item[2.]  $y \longmapsto \mu(E^y)$ is measurable in $Y$.
\item[3.]  $\mu \times \nu(E) = \int \nu(E_x) d \mu(x) = \int
\mu(E^y) d \nu(y)$.
\end{itemize}

\vspace{.15cm}
\noindent
In terms of the characteristic function $\chi_E (x, y)$ the
above statements read

\vspace{.15cm}
\begin{itemize}
\item[$1^\prime.$]  $x \longmapsto g(x) = \int_Y(\chi_E)_x
d \nu \in L^+ (X)$.
\item[$2^\prime.$]  $y \longmapsto h(y) = \int_X (\chi_E)_y
d \mu \in L^+ (Y).$
\item[$3^\prime$.]  
$\int_{X \times Y} \chi_E d (\mu \times \nu) = \int_X
\left [
\int_Y \chi_E (x, y) d \nu(y) \right ]d \mu(x)$ 

\hspace{3.cm} $= \int_Y \left [ \int_X \chi_E(x, y) d \mu(x) \right ] d
\nu(y).$
\end{itemize}  

\vspace{.25cm}
\noindent
\textbf{Recall.}  $L^+$ is the set of nonnegative measurable
functions.  The next theorem says that this is true for more
general functions $f(x, y)$.

\vspace{.25cm}
\noindent
\begin{theorem}[The Fubini-Tonelli Theorem.]  Let $(X,
\mathcal{M}, \mu)$ and $(Y, \mathcal{N}, \nu)$ be two
$\sigma$-finite measure spaces.  Then the following hold:

\vspace{.25cm}
\noindent
\textbf{(a)(Tonelli).}    If $f \in L^+ (X \times Y)$ then 
\begin{itemize}
\item[1.]  $g(x) = \int_Y f_x d \nu \in L^+ (X)$
\item[2.]  $h(y) = \int_X f^y d \mu \in L^+ (Y)$
\item[3.]  $\int_{X \times Y} fd (\mu \times \nu) =
\int_X \left [ \int_Y f(x, y) d \nu(y) \right ] d \mu(x)$

\hspace{3.cm} 
$= \int_Y \left [ \int_X f(x, y) d \mu(x) \right ] d \nu(y)$
\end{itemize}

\vspace{.25cm}
\noindent
\textbf{(b)(Fubini).}  If $f \in L^1(X \times Y)$ then
\begin{itemize}
\item[1.]  $f_x \in L^1 (\nu) $ a.e. in $X$, and $g(x) = \int f_x d
\nu \in  L^1(\mu)$.
\item[2.]  $f^y \in L^1 (\mu)$ a.e. in $Y,$ and $h(y) =
\int f^y d \mu \in L^1 (\nu)$
\item[3.]  same as in Tonelli.
\end{itemize}
\end{theorem}
{\em Proof.} 
(a)  As we have already seen, (a) is
true for characteristic functions of sets $E \in \mathcal{M}
\otimes \mathcal{N}$.  And by linearity it is also true for
simple functions in $\varphi \in L^+(X \times Y)$.  Now
to prove it for a general function $ f \in L^+ (X \times Y)$ we
choose an increasing sequence $\{\varphi_j\} \subset L^+$
such that $\varphi_j \nearrow f$, and use the MCT to obtain
that $g_j = \int (\varphi_j)_x d \nu \nearrow g(x) \in
L^+(\mu)$, and  $ h_j = \int (\varphi_j)^y d \mu \nearrow h(y) \in
L^+(\mu)$, and that 

\begin{eqnarray*}
\int f d(\mu \times \nu) &=& \lim\limits_{j \to \infty} \int \varphi_j
d (\mu \times \nu)\\
&=& \lim\limits_{j \to \infty} \int_X \left [ \int_Y \varphi_j (x, y) d
\nu(y) \right ] d \mu(x)\\
&=& \lim\limits_{j \to \infty} \int_X g_j (x) d \mu(x)\\
&=& \int_X g(x) d \nu(x) = \int_X \left [ \int_Y f(x, y) d \nu(y)
\right ] d \mu(x).
\end{eqnarray*}
Similarly we show that

\[
\int fd (\mu \times \nu) = \int_Y \left [ \int_X f(x, y) d
\mu(x) \right ] d \nu(y).
\]
This completes the proof of Tonelli's Theorem.

\medskip
\noindent
(b)  Observe that if $f \ge 0$ and $f \in L^1 (X
\times Y)$ then by part (3) of Tonelli's Theorem
\[
g(x) = \int_Y f_x d \nu < \infty \mbox{ a.e. in }x, \mbox{ and }
h(y) = \int_X f^y d \mu< \infty \mbox{ a.e. in } y.
\]
Therefore to obtain Fubini's Theorem it suffices to apply
Tonelli's Theorem in $f^+$ and $f^-$.



\vspace{.15cm}
\noindent
Next we state the Fubini-Tonelli Theorem for the
completion of $\mu \times \nu$.

\vspace{.25cm}
\noindent
\begin{theorem}[Fubini-Tonelli Theorem for the Completion of $\mu
\times \nu$.]  Let $(X, \mathcal{M}, \mu)$ and $(Y,
\mathcal{N}, \nu)$ be two $\sigma$-finite and $(X \times Y,
\mathcal{L}, \lambda)$ be the completion of $(X \times Y,
\mathcal{M} \otimes \mathcal{N}, \mu \times \nu)$.  If $f$ is
$\mathcal{L}$-measurable and either (a) $f \ge 0$ or (b) $f \in
L^1(\lambda)$ then the following holds.

\begin{itemize}
\item[$\bullet$]  $f_x$ is $\mathcal{N}$-measurable for a.e.
$x \in X$.
\item[$\bullet$]  $f^y$ is $\mathcal{M}$-meaurable for a.e.
$y \in Y$.
\item[$\bullet$]  $f_x \in L^1(\nu)$ for a.e. $x \in X$ if (b)
holds.
\item[$\bullet$]  $f^y\in L^1(\mu)$  for a.e.
$y \in Y$ if (b) holds.
\item[$\bullet$]  $g(x) = \int f_x d \nu$ is
$\mathcal{M}$-meaurable and  $h(y) = \int f^y d \mu$ is
$\mathcal{N}$-measurable.
\item[$\bullet$]  $g(x) \in L^1 (\mu)$ and $h(y) \in L^1(\nu)$ if
(b) holds.
\item[$\bullet$]  $\int f d \lambda = \int_X \left [ \int_Y f(x,
y) d \nu(y) \right ] d \nu(x) = \int_Y \left [ \int_X f(x, y) d
\mu(x) \right ] d \nu(y).$
\end{itemize}
\end{theorem}

\noindent
{\bf Remark.}  $f \in L^1 (X \times Y)$ is essential
for part (3) in Fubini's theorem. 
 For example, consider the
function $f(x, y)$ defined on $\mathbb{R}^2$ to be zero
outside $\underset{j=1}{\overset{\infty}{\cup}} I_j,$ where
$I_j$ are the  rectangles shown below.

%\begin{center}
%\scalebox{.7}{\includegraphics{Fubini_ex_1}}
%\end{center}


On each one of the four equal subrectangles of a rectangle $I_j$ we
define $f$  to be equal to $(\text{Area}(I_j))^{-1}$
multiplied by the number listed in the subrectangle. 
Then we have
\[
\int f(x, y) dx = \int f(x, y) dy = 0,
\]
and
\[
\int |f| dx dy = \sum_j \int_{I_j} |f| = \sum_j 1 = \infty.
\]


\section{The Lebesgue Integral on $\mathbb{R}^n$}
Recall that to construct the Lebesgue measure on $\mathbb{R}$ we
started with the premeasure of length $\ell$ on the algebra of the finite
disjoint unions of half-open intervals $(a, b]$, and then for any set $A$
in
$\mathbb{R}$ we defined the outer measure
\[
m^\ast (A) = \inf \left \{ \sum^\infty_{j=1} (b_j - a_j): A \subset
\overset{\infty}{\underset{j=1}{\cup}} (a_j, b_j]\right \}.
\]
Then a set $E$ was defined to be $m^\ast$-measurable if
\[
m^* (A) = m^*(A \cap E) + m^*(A \cap E^c),
\, \forall A\subset \mathbb{R}^n
\]
We showed that the family $\mathcal{L}$ of the $m^*$-measurable
sets is a $\sigma$-algebra, $m = m^*|_{\mathcal{L}}$ is a measure
called the Lebesque masure, and that the measure of any interval is
its length.  We also showed that for any $E \in \mathcal{L}$

\begin{eqnarray*}
m(E) &=& \mbox{inf}\left \{\sum^\infty_{j=1} (b_j - a_j):
\overset{\infty}{\underset{j=1}{\cup}} (a_j, b_j) \supset E \right \}\\
&=& \mbox{inf } \{ m(U): U \mbox{ open } \supset E \}\\
&=& \mbox{sup} \{m(K): K \mbox{ compact} \subset E\}
\end{eqnarray*}

In addition, we have shown that $E \in\mathcal{L}$ iff
\begin{itemize}
\item[(a)]  $E = V - N_1,$ where $V$ is $G_\delta$ and $m(N_1)=0$
if
\item[(b)] $E = F \cup N_2$, where $F$ is $F_\sigma$ and $m(N_2) = 0$.
\end{itemize}

Next we define the Lebesgue measure on $\mathbb{R}^n$.  It is the
completion of the product measure $m^n = m\times \dots \times m$ on
the product $\sigma$-algebra $\mathcal{L}^n = \mathcal{L} \otimes
\dots \otimes \mathcal{L}$.  More precisely, the finite disjoint unions of
rectangles $E_1 \times \dots \times E_n, E_j \in \mathcal{L}$, form an
algebra on $\mathbb{R}^n$ and $m^n$ on $\mathcal{A}$ defined by
\[
m^n \left (\overset{k}{\underset{j=1}{\cup}}(E^j_1 \times \dots \times
E^j_n) \right ) = \sum m(E^j_1) \dots m(E^j_n),
\]
where $\{E^j_1 \times \dots \times E^j_n\}^k_{j=1}$ are disjoint, is a
premeasure on $\mathcal{A}$.  This premeasure defines the outer
measure of a set $A \subset \mathbb{R}^n$ by 

\[
(m^n)^*(A) = \inf \left \{\sum^\infty_{j=1} m(E^j_1) \dots m(E^j_n):
\overset{\infty}{\underset{j=1}{\cup}} E^j_1 \times \dots \times E^j_n
 \supset A, E^j_k \in \mathcal{L}\right \}.
\]

Then $E \subset \mathbb{R}^n$ is Lebesgue measurable in $\mathbb{R}^n$
if
\[
(m^n)^* (A) = (m^n)^* (A \cap E) + (m^n)^* (A \cap E^c), \ \forall A
\subset \mathbb{R}^n.
\]
The family $(\mathcal{L}^n)^*$ of the $(m^n)^*$-measurable sets is a 
$\sigma$-algebra, and $(m^n)^*$ restricted to $(\mathcal{L}^n)^*$ is a
complete measure. 

\begin{definition}  $(\mathcal{L}^n)^* \doteq \mathcal{L}$ denotes the
Lebesgue measurable sets in $\mathbb{R}^n$ and $(m^n)^* \doteq m$
denotes the Lebesgue measure in $\mathbb{R}^n$.
\end{definition}

\vspace{.25cm}
\noindent
\textbf{Note.} $m \times \dots \times m = (m^n)^*$ restricted on
$\mathcal{L} \otimes \dots \otimes \mathcal{L}$ is the product measure
on $\mathbb{R}^n$, which as we have seen is not complete.

\vspace{.20cm}
\noindent
The following is a generalization of a Theorem in 1-Dimension.

\begin{theorem}  Let $E \in \mathcal{L}$.  Then
\begin{itemize}
\item[(a)]  
$\displaystyle m(E) = \inf \{m (U): U \mbox{\text{open }}\supset E\}
= \mbox{sup} \{m(K): K \mbox{\text{compact } } \subset
E\}.$
\item[(b)]  $E = V - N_1 = F\cup N_2,$ \textit{where $V$ is $G_\delta, F$
is $F_\sigma$ and $m(N_1) = m(N_2) = 0.$}
\item[(c)]  \textit{If $m(E) < \infty$, then for any $\varepsilon > 0$  there
exists a finite collection of disjoint  cubes $\{Q_j\}^N_{j=1}$ such that}
\[
m\left (E \Delta (\overset{N}{\underset{j=1}{\cup}} Q_j)
\right ) < \varepsilon.
\]
\end{itemize}
\end{theorem}

\vspace{.25cm}
\noindent
\textbf{Note.}  A cube is a rectangle of the form $I_1 \times \dots
\times I_n$ where $I_j$ are intervals in $\mathbb{R}$.

\vspace{.25cm}
\noindent
{\em Proof.}
(a)  Let $\varepsilon > 0$.  By definition there exists a sequence
of rectangles
\[
R_j = E^j_1 \times \dots \times E^j_n, \ E^j_i \in \mathcal{L}^1
\]
such that $E \subset \overset{\infty}{\underset{j=1}{\cup}} R_j$ and 
\[
\sum^\infty_{j=1} m(R_j) \le m(E) + \varepsilon.
\]
Now fix $j$ and apply the corresponding theorem in $\mathbb{R}$ for
each side of $R_j$ to obtain an open set $U_j \supset R_j$ such that
\[
m(U_j) \le m(R_j) + \varepsilon \cdot 2^{-j}.
\]
Then the $U = \overset{\infty}{\underset{j=1}{\cup}} U_j$ is an open set in
$\mathbb{R}^n$ with $U \supset E$ and 
\[
m(U) \le \sum^\infty_{j=1} m(U_j) \le \sum^\infty_{j=1} m(R_j) +
\varepsilon \le m(E) + 2\varepsilon.
\]
This proves the first part of (a).  The proof of the second part of (a), and
(b) is very similar to the corresponding proof in $\mathbb{R}^1$.

\medskip
\noindent
(c) Let $\varepsilon > 0$.  Since $m (E) < \infty$ the open  sets
$U_j$ in (a) can be chosen  such that $U_j = G^j_1 \times \dots \times
G^j_n$, where $G^j_i$ are open in $\mathbb{R}$ and $m^1(G^j_i) < \infty$. 
Since every open set in $\mathbb{R}$ is a countable  union of open
intervals, we can find a union of disjoint and finitely many open
rectangles $T_j  \subset U_j$ such that
\[
m(U_j) < m(T_j) + \varepsilon 2^{-j}
\]
Then for any $N \in \{1, 2, \dots\}$ we have
\begin{eqnarray*}
m\left ( E \Delta (\overset{N}{\underset{j=1}{\cup}} T_j)\right ) &=& m
\left (E-(\overset{N}{\underset{j=1}{\cup}} T_j)\right ) + m \left (
\overset{N}{\underset{j=1}{\cup}} T_j - E\right )\\
&\le& m \left ( \overset{N}{\underset{j=1}{\cup}} (U_j - T_j)\right ) + m
\left (
\overset{\infty}{\underset{j=N+1}{\cup}} U_j\right ) + m\left (
\overset{\infty}{\underset{j=1}{\cup}} U_j - E\right ).
\end{eqnarray*}
Now we choose $N$ large enough so that
\[
m\left ( \overset{\infty}{\underset{j=N+1}{\cup}} U_j\right ) <
\varepsilon.
\]
Then from the last inequality we obtain
\[
m\left ( E \Delta \overset{N}{\underset{j=1}{\cup}} T_j\right ) \le
\varepsilon +
\varepsilon + \varepsilon = 3\varepsilon.
\]

This proves (c) since $\overset{N}{\underset{j=1}{\cup}} T_j$ can be
expressed as a finite union of disjoint cubes.


\begin{theorem}  Let $f \in L^1(\mathbb{R}^n)$ and
$\varepsilon > 0$.  Then
\begin{itemize}
\item[(a)]  There exists $\varphi = \sum\limits^N_{j=1} a_j
\chi_{Q_j}$ where $Q_j$ are cubes such that 
\[
\int_{\mathbb{R}^n} |f - \varphi | < \varepsilon.
\]
\item[(b)]  There exists a compactly supported continuous
function
$g$ such that
\[
\int_{\mathbb{R}^n} |f-g| < \varepsilon.
\]
\end{itemize}
\end{theorem}

\vspace{.25cm}
\noindent
{\em Proof.}  The proof is very similar to the case $n =1$.  It suffices
to construct a continuous function approxiating the characteristic 
function of a cube.  This can be done in the following way.  Let us say $Q =
(a_1, b_1) \times \dots \times (a_n, b_n)$.  Then define
\[
h^k_j(x_j) = \left \{ \begin{array}{ll}
1 \mbox{ for } x \in \left (a_j + \frac{1}{k}, b_j - \frac{1}{k} \right )\\
0 \mbox{ for } x \notin (a_j, b_j)\\
\mbox{linear for } x \in\left  (a_j, a_j + \frac{1}{k}\right ) \cup \left ( b_j -
\frac{1}{k}, b_j\right ).\end{array}
\right.
\]
and such that $h_j$ is continuous on $\mathbb{R}$.  Then the function $h_k
= h^k_1 \dots h^k_n$ has the desired properties.

\vspace{.25cm}
\noindent
Next we shall show that the Lebesque measure in $\mathbb{R}^n$ is
invariant under a translation.

\begin{theorem}  Let $E \in \mathcal{L}^n$, and $y \in
\mathbb{R}^n$.  Then
\begin{itemize}
\item[(a)]  $E + y \in \mathcal{L}^n$ and $m (E + y) = m(E)$
\item[(b)]  If $f \ge 0$, or $f \in L^1 (\mathbb{R}^n)$, then
$\int_{\mathbb{R}^n} f(x+y) dx = \int_{\mathbb{R}^n} f(x) dx.$
\end{itemize}
\end{theorem}

\noindent
{\em Proof.}
(a) Recall that the outer measure of a set $A \subset
\mathbb{R}^n$ has been defined by 
\[
(m^n)^*(A) = \inf \left \{\sum^\infty_{j=1} m^1 (E^j_1) \dots m^1
(E^j_n):  \overset{\infty}{\underset{j=1}{\cup}} (E^j_1 \times \dots
\times E^j_n) \supset A, E^j_i \in \mathcal{L}^1\right \}
\]
Since for any rectangle $R = E_1 \times \dots \times E_n, E_j \in
\mathcal{L}^1$, we have $R +  y = (E_1 + y_1)\times \dots \times
(E_n + y_n)$. Then, by the corresponding theorem in  one dimension,
we have 
$y + R$ is a rectangle and that $m(y + R) = m(R)$.  Therefore by
definition
\[
(m^n)^* (A+y) = (m^n)^* (A), \ \forall A \subset \mathbb{R}^n;
\]
that is the Lebesgue outer measure in $\mathbb{R}^n$ is invariant
under translations. Using  this and the measurability of $E$,
 for any set
$A \in \mathbb{R}^n$ we have

\begin{eqnarray*}
(m^n)^*(A) = (m^n)^* (A - y) &=& (m^n)^* ([A-y] \cap E) + (m^n)^* ([A-y]
\cap E^c)\\
&=& (m^n)^* (A \cap [E + y]) + (m^n)^* (A \cap [E + y]^c).
\end{eqnarray*}
Therefore $E + y \in \mathcal{L}^n$ with $m (E+y) = m(E)$.

\medskip
\noindent
(b)  If $f = \chi_E, E \in \mathcal{L}^n$, then $\chi_E(x + y) =
\chi_{E-y}(x),$ and
\[\int \chi_E(x+y) dx = \int \chi_{E-y} (x) dx = m(E-y) = m(E) = \int
\chi_E (x) dx.
\]
Therefore (b) holds for characteristic functions and by the linearity of
the integral it also follows for  simple functions.  If $f \ge 0$, then we
choose nonnegative and mesurable simple functions $\varphi_j$ such
that
$\varphi_j \nearrow f$.  Then by applying the MCT we obtain
\[
\int f(x+y) dx = \lim \int \varphi_j (x+y) dx = \lim \int \varphi_j (x) dx
= \int f(x) dx.
\]
If $f \in L^1 (\mathbb{R}^n)$ then we write $f = g + ih, g = g^+ - g^-, h =
h^+ - h^-$ and apply the last result.  This completes the proof of the
theorem.

\vspace{.15cm}
\noindent
Next we shall prove the following useful theorem.

\begin{theorem}  Every open set in $\mathbb{R}^n$ can be
written as a countable union of almost disjoint (nonoverlapping) 
(closed) cubes.
\end{theorem}

\begin{no}  If $n = 1$ then any open set can be written as a countable
union of disjoint intervals.
\end{no}


\noindent
{\em Proof.}  Let $K_0$ be the family of the cubes defined by the
lattice $\mathbb{Z}^n$. i.e. each cube has vertices on $\mathbb{Z}^n$ and
edge length 1. Then bisecting each edge  of a cube in $K_0$ gives $2^n$
 cubes of edge length $2^{-1}$, and obtain a new family denoted by
$K_1.$  Continuing this way for each $j$ we obtain a countable family of
almost disjoint cubes $K_j$ with  edge $2^{-j}$, and $K_j$ results for
$K_{j-1}$ by bisecting then the edge of each cube in $K_{j-1}$.

Now let $V$ be an open set in $\mathbb{R}^n$. Call $S_0$ the collection
of cubes in $K_0$, which are contained in $V, S_1$ the collection of
cubes in
$K_1$ which are contained in $V$and are not subcubes of a cube in
$S_0$, continuing in this way,  call $S_j$ the cubes in $K_j$ which are
contained in $V$ and are not subcubes of any  cube in $S_0 \cup S_1 \cup
\dots \cup S_{j-1}$.  Since any point $x \in V$ belongs to at least one
cube in $K_j$, for all $j$, the diameter of the cubes in $K_j$ tends to zero
as $j \to \infty$,  and since $V$ is open, we have that 
\[
V = \underset{Q \in S}{\cup} Q, \mbox{ where } S =
\overset{\infty}{\underset{j=1}{\cup}} S_j.
\]
This completes the proof of the lemma.

\begin{re}  Each collection $K_j$ of cubes constructed above is
called a family of diadic cubes.
\end{re}

\begin{theorem}  If $f: \mathbb{R}^n \longrightarrow
\mathbb{R}^n$ is such that
\[
|f(x) - f(y) | \le c |x-y|, \mbox{ some } c > 0,
\]
then $f(E) \in \mathcal{L}^n$ for any $E \in \mathcal{L}^n$.
\end{theorem}

\begin{no}  Such a function $f$ is called a Lipschitz transformation of
$\mathbb{R}^n$.
\end{no}

\vspace{.25cm}
\noindent
{\em Proof.}  First observe that $f$ is continuous.  Therefore it maps
compact into compact sets.  If $E \in \mathcal{L}^n$ then $E = F \cup Z$
where $F $ is $F_\sigma$ and $m(Z) = 0$.  Since $f(E) = f(F) \cup f (Z)$ it
suffices to show that $f(F)$ is $F_\sigma$ and $m(f(Z)) = 0$.  Since $F =
\overset{\infty}{\underset{j=1}{\cup}} F_j$, with $F_j$ closed, it suffices
to show that for each $j\ f(F_j)$ is $F_\sigma$.  Let $F_{j, k} = F_j
\cap \{|x|
\le k\}$.  Then each $F_{j, k}$ is compact and $F_j =
\overset{\infty}{\underset{k=1}{\cup}} F_{j, k}$.  Therefore
\[
f(F_j) = \overset{\infty}{\underset{k=1}{\cup}} f(F_{j,k}) \in F_\sigma.
\]
Next we shall show that $m(f(Z)) = 0$.  Let $\varepsilon > 0 $.  Since $m(Z)
= 0$, there exists $V$ open in $\mathbb{R}^n$ such that $Z \subset V$ and 
$m(V) < \varepsilon$.  By the last lemma we  write $V =
\overset{\infty}{\underset{j=1}{\cup}} Q_j $ where $Q_j$ are closed cubes
since $f(V) \subset \overset{\infty}{\underset{j=1}{\cup}} F(Q_j), 
f(Q_j)$ compact and $m(f(Q_j)) \le cm(Q_j)$ we have
\[
m(f(Z)) \le m(f(V)) \le \sum^\infty_{j=1} m(f(Q_j)) \le c
\sum^\infty_{j=1} m(Q_j) = cm(V)
\]
which gives $m(f(Z)) < \varepsilon$, or that  $m(f(Z)) = 0$.  This proves
the lemma.

\vspace{.25cm}
\noindent
Next we shall prove the change of variables formula
when the transformation is linear.

\begin{theorem}  Let $T: \mathbb{R}^n
\longrightarrow \mathbb{R}^n$ be an invertible linear
transformation.  Then
\begin{itemize}
\item[(a)] If $f$ is Lebesgue measurable in
$\mathbb{R}^n$ then so is $f \circ T$.  If,  in addition, $f
\ge 0$ or $f \in L^1(\mathbb{R}^n)$ then
\[
\int_{\mathbb{R}^n} f(x) dx = |\mbox{det} T|
\int_{\mathbb{R}^n} f \circ T (x) dx
\]
\item[(b)]  If $E \in \mathcal{L}^n$ then $T(E) \in
\mathcal{L}^n$ with $m(T(E)) = |\mbox{det} T| m(E).$
\end{itemize}
\end{theorem}

\begin{no}  Here we shall also denote by $T$ the matrix that
defines $T$ in the standard orthonormal basis $\{e_1, \dots,
e_n\}$ in $\mathbb{R}^n$ i.e. $T$ is the $n \times n$ matrix with
columns $Te_1, \dots, Te_n$.
\end{no}

\vspace{.25cm}
\noindent
{\em Proof.}  First observe that
\[
|Tx - Ty| = |T(x-y)| \le \|T\|\cdot |x-y|.
\]
Therefore by the last theorem $T(E) \in \mathcal{L}^n$ for any
$E \in \mathcal{L}^n$.  Since $T$ is invertible, we also have
\[
|T^{-1} x - T^{-1}y| \le \|T^{-1}\| \cdot|x-y|
\]
and therefore $T^{-1} (E) \in \mathcal{L}^n$ for any $E \in
\mathcal{L}^n$.  Thus for any $a > 0 $ we have
\[
(f \circ T)^{-1} (a, \infty) = T^{-1} (f^{-1} (a, \infty)) \in
\mathcal{L}^n
\]
since $f^{-1} (a, \infty) \in \mathcal{L}^n$.  Also if we apply the
formula in (a) with $f = \chi_{T(E)}$ we obtain

\begin{eqnarray*}
m(T(E)) &=& \int \chi_{T(E)} (x) dx = |\mbox{det} T| \int\chi_{T(E)}
T(x) dx\\
&=& |\mbox{det} T| \int \chi_E(x) dx = |\mbox{det} T|m(E).
\end{eqnarray*}
Thus to complete the proof of the theorem it suffices to prove
the transformation formula in (a).  Since any invertible matrix
can be written a compostion of elementary matrices
corresponding to the following elementary transformations:

\begin{eqnarray*}
&\bullet& T_1 (x_1, \dots, x_j, \dots, x_n) = (x_1, \dots, cx_j,
\dots, x_n), \ 1 \le j \le n, \ c \ne 0\\
&\bullet& T_2 (x_1, \dots, x_j, \dots, x_n) = (x_1, \dots, x_j +
x_k, \dots, x_n), \ j \ne k\\
&\bullet& T_3 (x_1, \dots, x_j, \dots, x_k, \dots, x_n) = (x_1,
\dots, x_k, \dots, x_j, \dots, x_n), \ j < k,
\end{eqnarray*}
and since for any $T, S$ invertible linear transformations we
have
\begin{eqnarray*}
\int f(x) dx &=& |\mbox{det} T| \int f \circ T(x) dx\\
&=& |\mbox{det} T| |\mbox{det} S| \int f \circ T \circ S(x) dx\\
&=& |\mbox{det} T \circ S| \int f \circ (T \circ S) (x) dx,
\end{eqnarray*}
the formula will be proved once we prove it separately for
each one of the transformations $T_1, T_2$ and $T_3$.  For
$T_1$ formula follows by the Fubini theorem and the
1-dimensional formula
\[
\int f(t) dt = |c| \int f(ct) dt,
\]
which can be proved easily for characteristic, then simple and
then general functions.  For $T_3$ formula follows by applying
the Fubini-Tonelli Theorem to change the order of integration. 
And for $T_2$ it follows by applying the Fubini-Tonelli
Theorem to reduce it to the 1-dimensional formula
\[
\int f(x_1, \dots, x_j + x_k, \dots, x_n) dx_j = \int f(x_1, \dots,
x_j, \dots, x_n)dx_j
\]
which has been proved above.  Note det $T_1 = |c|, |$det $ T_2|
=|$det$  T_3| = 1.$  This completes the proof of the theorem.

\begin{corollary}  The Lebesgue measure is
rotation-invariant.
\end{corollary}

{\bf Remark.} Recall that a linear transformation
 $T:\mathbb{R}^n\to \mathbb{R}^n$ is a rotation
if $TT^*=I$.

\vspace{.15cm}
\noindent
 Next we shall prove the change of variables
formula for general functions.  But, before that, we recall some
definitions.  Let $\Omega$ be an open set in $\mathbb{R}^n$.  A
function
\[
G = (g_1, \dots, g_n): \Omega \longrightarrow \mathbb{R}^n
\]
is of Class $C^1$ if all component functions $g_j$ have
continuous first order partial derivatives.  Then we shall
denote by
\[ D_x G = \left [
\begin{array}{ll}
&\frac{\partial g_1}{\partial x_1}(x), \dots, \frac{\partial
g_1}{\partial x_n}(x)\\
&\vdots\\
&\frac{\partial g_n}{\partial x_1}(x), \dots, \frac{\partial
g_n}{\partial x_n}(x) \end{array} \right ]
\]
the matrix of all first order partial derivatives.  If in addition,
$G$ is injective and $D_x G$  is invertible for all $x \in \Omega$
then $G$ is called a $C^1$ diffeomorphism.

\begin{theorem} (The Change of Variables Formula) 
Let $\Omega$ be an open set in $\mathbb{R}^n$ and $G:
\Omega \longrightarrow \mathbb{R}^n$ be a $C^1$
diffeomorphism.  Then
\begin{itemize}
\item[(a)]  For any Lebesgue measurable function $f$
on $G(\Omega)$ the function $f \circ G$ is Lebesgue measurable
on $\Omega$.  If, in addition, $f \ge 0 $ or $f \in L^1
(G(\Omega))$ then
\[
\int_{G(\Omega)} f(x) dx = \int_\Omega f \circ G(x)
|\mbox{det} D_x G| dx
\]
\item[(b)]  If $E \subset \Omega$ and $E \in
\mathcal{L}^n$ then $G (E) \in \mathcal{L}^n$ and
\end{itemize}
\[
m(G(E)) = \int_E |\mbox{det} D_x G| dx.
\]
\end{theorem}
For proving the theorem, we shall need the following.

\begin{prop}  Let $(X, \mathcal{M}, \bar \mu)$ be the
completion of a measure space $(X, \mathcal{M}, \mu)$.   If $F$
is an $\bar{\mathcal{M}}$-measurable function on $X$ then there
exist a $\mathcal{M}$-measurable function $f $ on $X$ such that
$f = F$\, $\bar\mu$-almost everywhere in $X$.
\end{prop}

\vspace{.25cm}
\noindent
{\em Proof.}  (see book, p. 46)

\vspace{.25cm}
\noindent
{\em Proof of Theorem.}  Since $(\mathbb{R}^n,
\mathcal{L}^n, m)$ is the completion of $(\mathbb{R}^n,
\mathcal{B}_{\mathbb{R}^n}, m)$ we may assume that $f$ is
Borel measurable.  This implies that $f \circ G$ is Borel
measurable.  Then part (b) follows from (a) applied with $f =
\chi_{G(E)}$, since then
\[
\left (\chi_{G(E)} \circ G) (E) = \chi_{G(E)} (G(E)\right ) = \chi_E.
\]
Thus the proof of the theorem will be complete once we prove
``the formula'' in (a).  In order to be able to describe cubes in a
simple way, we will not use the Euclidean norm but the
$\max$-norm defined by
\[
\|x\| = \underset{1 \le j \le n}{\max} |x_j|
\]
If $Q(a, h) = \{x: \|x-a\| \le h\}$ is a cube in $\Omega$ then by
the mean value theorem we obtain that
\begin{equation*}
\|G(x) - G(a)\|\le h\cdot\underset{y \in Q}{\mbox{sup}} \|D_yG\|, \
\forall x \in Q.  \tag{2}
\end{equation*}
Here for a matrix $A = (a_{ij})$, we use the norm
\[
\|A\| = \max_{1 \le i \le n} \sum^n_{j=1} |a_{ij}|.
\]
Thus the set $G(Q)$ is mapped inside the cube
Q(f(a), b) with  $b = h\cdot \underset{y \in Q}{\mbox{sup}} \|D_y G\|$
we have that
\begin{equation*}
m(G(Q)) \le \left ( \underset{y \in Q}{\mbox{ sup}} \|D_y
G\|\right )^n m (Q).\tag{3}
\end{equation*}
And if $T$ is any invertible linear transformation, then again by
the last theorem appplied to $T^{-1} \circ G(Q)$ we obtain

\begin{equation*}
m(G(Q)) = |\mbox{det} T|m(T^{-1} G(Q)) = |\mbox{det} T| \left (
\underset{y}{\mbox{sup}} \|T^{-1} G\|\right )^n m (Q).\tag{4}
\end{equation*}
Now let $\varepsilon > 0$.  Since $D_y G$ is continuous and
$(D_y G)^{-1} \cdot (D_y G) = I$ there is $\delta > 0 $ such that

\[
\|(D_z G)^{-1} (D_yG) \|^n \le 1 + \varepsilon, \mbox{ if } \|y - z\|
< \delta, \  \forall y, z \in Q.
\]
If we subdivide the cube $Q(a, h)$ into nonoverlapping cubes
$Q_1(x_1, \delta_1)$, $\dots,$ $Q_N(x_N, \delta_N)$, $\delta_j <
\delta$ and apply (4) with $Q$ replaced by $Q_j$ and $T$ by
$D_{x_j} G$ then we obtain

\begin{eqnarray*}
m(G(Q)) &\le& \sum^N_{j=1} m(G (Q_j))\\
&\le& \sum^N_{j=1} |\mbox{det} D_{x_j} G| \left [\underset{y
\in Q_j}{\mbox{ sup}} \|(D_{x_j} G)^{-1} D_yG\|\right ]^n m(Q_j)\\
&\le& (1 + \varepsilon) \sum^N_{j=1} |\mbox{det} D_{x_j}
G|m(Q_j)\\
&\le& (1 + 2 \varepsilon) \int_Q |\mbox{det} D_x G|dx,
\end{eqnarray*}
since the last sum is a Riemann sum for the last integral.  By
letting $\varepsilon \to 0 $ we obtain

\begin{equation*}
m(G(Q)) \le \int_Q |\mbox{det} D_x G| dx.\tag{5}
\end{equation*}
Next we shall prove (5) with $Q$ replaced with any open set $U
\subset \Omega$.  For this we write: $U =
\overset{\infty}{\underset{j=1}{\cup}}Q_j$, where $Q_j$ are
closed and non-overlapping cubes.  Then

\begin{equation*}
m(G(U)) \le \sum^\infty_{j=1} m(G(Q_j)) \overset{(5)}{\le}
\sum^\infty_{j=1} \int_{Q_j}|\mbox{det} D_x G| dx = \int_U
|\mbox{det} D_xG| dx \tag{6}
\end{equation*}
Now we shall generalize (6) to the case where $U$ is replaced
by a Borel set $E \subset \Omega$, which is
\underline{bounded}.  Since $m(E) $ is then infimum of $m(U)$
where $U$ ranges over all open sets containing $E$ we can
find a sequence of open sets $\{V_j\}$ with $V_j \supset E$ and
$\lim m(V_j) = m(E)$.  If we let $U_j = V_1 \cap \dots \cap
V_j$ then we obtain that $\{U_j\}$ is a decreasing
suequence of open subsets of $\Omega$ such that $E \subset
\overset{\infty}{\underset{j=1}{\cap}} U_j$ and
$m(\overset{\infty}{\underset{j=1}{\cap}} U_j - E) = 0$.  Since
$m(U_1) < \infty$ we have

\[
m(G(E)) \le m(G( \overset{\infty}{\underset{j=1}{\cap}} U_j)) =
\lim m(G(U_j)) \overset{(6)}{\le} \lim_{j\to \infty} \int_{U_j}
|\mbox{det} D_x G| dx,
\]
and by applying the DCT to the sequence $\chi_{G_j} |$det
$D_xG|$ (since $G_1$ is bounded)  we obtain
\begin{equation*}
m(G(E)) \le \int_E |\mbox{det} D_x G| dx \tag{7}
\end{equation*}
Finally, if $E$ is a Borel set of infinite measure then we write $E
= \overset{\infty}{\underset{j=1}{\cup}} E_j$ where $E_j = E
\cap \{x: |x| \le j\}$ and use (7) to obtain

\begin{eqnarray*}
m(G(E)) &\le& m(G(\cup E_j)) = \lim_{j \to \infty} m(G (E_j))\\
&\overset{(7)}{\le}& \underset{j \to \infty}{\lim} \int_{E_j}
|\mbox{det} D_x G| dx
\end{eqnarray*}
and by using the MCT to the sequence $\chi_{E_j} |$det $D_x G|$
we obtain

\begin{equation*}
m(G(E)) \le \int_Z |\mbox{det} D_x G| dx,\  E \mbox{ Borel}
\subset \Omega.\tag{8}
\end{equation*}
Now we write (8) in terms of the characteristic
function of $E = G^{-1} (A),$ where $A$ is a Borel set in
$G(\Omega)$.  Then we have

\begin{eqnarray*}
\int_{G(\Omega)} \chi_A(x) dx = m(A) &=& m(G(G^{-1}(A)))\\
&\overset{(8)}{\le}& \int_{G^{-1}(A)} |\mbox{det} D_x G| dx\\
&=& \int X_{G^{-1}(A)} |det D_x G| dx
\end{eqnarray*}
or

\begin{equation*}
\int_{G(\Omega)} \chi_A (x) dx \le \int_\Omega \chi_A \circ G
|\mbox{det} D_x G| dx \tag{9}
\end{equation*}
By using (9) we show that
\begin{equation*}
\int_{G(\Omega)} \varphi (x) dx\le \int_\Omega \varphi \circ
G|\mbox{det} D_x G| dx \tag{10}
\end{equation*}
for any nonnegative simple function $\varphi$ on $G(\Omega)$. 
And finally by using the MCT and (10) we show that
\begin{equation*}
\int_{G(\Omega)} f(x) dx \le \int_\Omega f \circ G |\mbox{det}
D_x G| dx, \tag{11}
\end{equation*}
for any nonnegative function $f$.  Finally by applying the same
arguments for the case where $G$ is replaced by $G^{-1}$ and
$f$ is replaced by (det $D_xG) \cdot [f \circ G]$ we obtain
\[ \int_\Omega f \circ G(x) | \mbox{ det} D_x G| dx \le
\int_{G(\Omega)} f \circ G \circ G^{-1} (x) |\mbox{det}
D_{G^{-1}(x)} G\|\mbox{det} D_x G^{-1}| dx.
\]
Since $(D_{G^{-1}(x)} G)(D_xG^{-1}) = I$ we have
\begin{equation*}
\int_\Omega f \circ G(x) | \mbox{det} D_x G| dx \le
\int_{G(\Omega)} f(x) dx. \tag{12}
\end{equation*}
By (11) and (12) we obtain ``the formula'' for $f \ge 0$.   If $f
\in L^1 (G(\Omega))$ then we write it in terms of nonnegative
function to obtain the formula in this case too.  This completes
the proof of the theorem.

\vspace{.25cm}
\noindent
{\bf Conclusion.}  At this point we have completed our first
objective in this course.  That is to construct a measure in
$\mathbb{R}^n$ such that
\begin{itemize}
\item[1.]  Is complete.
\item[2.]  Invariant under rigid motions.
\item[3.]  Its restriction to cubes is the volume.
\item[4.]  The familiar change of variable formula from
advanced calculus holds.
\end{itemize}
 
\vspace{.25cm}
\noindent
Next we shall discuss spherical coordinates in
$\mathbb{R}^n$, which in $\mathbb{R}^2$ are called
polar.

\vspace{.25cm}
\noindent
{\large\bf Polar Coordinates in $\mathbb{R}^2$.}  Let
\[
\Omega = \{(r, \theta): r > 0, 0 < \theta < 2 \pi\}
\subset \mathbb{R}^2
\]
and $G: \Omega \longrightarrow \mathbb{R}^2$ be
defined by
\[
(r, \theta) \longmapsto G(r, \theta) = (x_1 = r\cos
\theta, \ x_2 = r \sin \theta).
\]
Then
\[
G(\Omega) = \mathbb{R}^2 - \{(x_1, 0): x_1 \ge 0\}.
\]
If $f \ge 0 $ or $f \in L^1(\mathbb{R}^2)$ then by the
change of variables formula and the fact that
$m(\{(x_1, 0): \ x_1 \ge 0\}) = 0$ we have

\[
\int_{\mathbb{R}^n} f(x) dx = \int_{G(\Omega)} f(x)
dx = \int_\Omega f \circ G(r, \theta) |\mbox{det }
D_{(r, \theta)} G| d \theta d r.
\]
Since

\[
D_{(r, \theta)} G = \left [\begin{array}{ll}
\frac{\partial x_1}{\partial r} &\frac{\partial x_1}{\partial
\theta}\\
\\
\frac{\partial x_2}{\partial r}&\frac{\partial x_2}{\partial
\theta}\end{array}
 \right ] = \left [ \begin{array}{ll}
\cos \theta & - r \sin \theta\\
\\
\sin \theta &r \cos \theta\end{array}
\right ]
\]
we have
\[
|\mbox{det }D_{(r, \theta)} G| = |r \cos^2 \theta +
r\sin^2 \theta| = r.
\]
Therefore the formula for the polar coordinates takes
the form
\[
\int_{\mathbb{R}^2} f(x_1, x_2) dx_1 dx_2 =
\int_\Omega f(r \cos \theta, r \sin \theta) r dr
d\theta.
\]

\vspace{.25cm}
\noindent
{\bf Application.}  If $a > 0 $ then
\begin{eqnarray*}
I_2 \doteq \int_{\mathbb{R}^2} e^{-a|x|^2} dx &=&
\int_0^{2 \pi} \int^\infty_0 e^{-ar^2} rdr d\theta =
2\pi \int^\infty_0 e^{-ar^2} r dr\\
&=& - \frac{\pi}{a} e^{- ar^2} \big |^\infty_0 =
\frac{\pi}{a}.\end{eqnarray*}
Since by Fubini's theorem

\[
I_2 = \int_{\mathbb{R}} e^{-ax^2_1} dx_1
\int_{\mathbb{R}} e^{-ax^2_2} dx_2 = I^2_1
\]
we have
\[
I_1 = \int_{\mathbb{R}} e^{-ax^2} dx = \left (
\frac{\pi}{a} \right )^{\frac{1}{2}}.
\]
This and Fubini's theorem give

\[
I_n \doteq \int_{\mathbb{R}^n} e^{-a|x|^2} dx = \left
( \int_{\mathbb{R}} e^{-ax^2_1} dx_1\right ) \dots
\left ( \int_{\mathbb{R}} e^{-ax^2_n} dx_n\right ) =
I^n_1.
\]
Therefore we obtain the very useful formula
\[
 \int_{\mathbb{R}^n} e^{-a|x|^2} dx = \left (
\frac{\pi}{a} \right )^{\frac{n}{2}}.
\]
{\large \bf Spherical Coordinates in $\mathbb{R}^3$.}  In
$\mathbb{R}^3$ let
\[
\Omega = \{(r, \varphi, \theta): r > 0, 0 < \varphi < \pi,
\ 0 < \theta < 2 \pi\}
\]
and $G: \Omega \longrightarrow \mathbb{R}^3$ be
defined by
\[
(r, \varphi, \theta) \longrightarrow G(r, \varphi,
\theta) = (x_1 = r\sin \varphi \cos \theta, \ x_2 =
r\sin \varphi \sin \theta, \ x_3 = r \cos \varphi)
\]
Then
\[
G(\Omega) = \mathbb{R}^3 - \{(x_1, 0, x_3): \ x_1 \ge
0\}
\]
If $f \ge 0 $ or $f \in L^1 (\mathbb{R}^3)$ then by the
change of variables formula and the fact that
$m(\{(x_1, 0, x_3): x_1 \ge 0\}) = 0$ we obtain
\[
\int_{\mathbb{R}^3} f(x) dx = \int_{G(\Omega)} f(x)
dx = \int_\Omega f \circ G(r, \varphi, \theta)
|\mbox{det } D_{(r, \varphi, \theta)} G| dr d\varphi d
\theta.
\]
Since
\[
D_{(r, \varphi, \theta)} G = \left [ \begin{array}{ll}
\displaystyle{\frac{\partial x_1}{\partial r}  \quad \frac{\partial
x_1}{\partial \varphi} \quad \frac{\partial
x_1}{\partial \theta} }\\
\displaystyle{\frac{\partial x_2}{\partial r} \quad \frac{\partial
x_2}{\partial \varphi} \quad \frac{\partial
x_2}{\partial \theta} }\\
\displaystyle{\frac{\partial x_3}{\partial r} \quad \frac{\partial
x_3}{\partial \varphi} \quad \frac{\partial
x_3}{\partial \theta} }\end{array}  \right ] = \left [
\begin{array}{ll}
\sin \varphi \cos \theta \quad r \cos \varphi \cos
\theta \quad -r \sin \varphi \sin \theta\\
\sin \varphi \sin \theta \quad r \cos \varphi \sin
\theta \quad r \sin \varphi \cos \theta\\
\cos \varphi \qquad -r \sin \varphi \quad\quad\quad 0
\end{array}\right ],
\]
and since
\[|\mbox{det } D_{(r, \varphi, \theta)} G| = r^2 \sin
\varphi dr d \varphi d \theta
\]
the formula for the spherical coordinate changes in
$\mathbb{R}^3$ becomes
\[
\int_{\mathbb{R}^3} f(x) dx = \int^\infty_{r=0}
\int^\pi_{\varphi = 0} \int^{2 \pi}_{\theta = 0} f(r \sin
\varphi \cos \theta, r \sin \varphi \sin \theta, r \cos
\varphi) r^2 \sin \varphi dr d\varphi d \theta .
\]

Explicit formulas for spherical coordinate changes
can be derived for all dimensions.  However, for most
purposes it is sufficient to establish a formula in
which the Lebesgue integration in $\mathbb{R}^n$ is
expressed in terms of the product of the surface
measure $\sigma$ on the unit sphere $S^{n-1}$ and the
measure $r^{n-1} dr$ on $(0, \infty)$.  More precisely,
let
\[
S^{n-1} \doteq \{x \in \mathbb{R}^n: |x| = 1\}, \ r
\doteq |x|, \mbox{ and } x^\prime = \frac{x}{|x|}, \ x
\ne 0.
\]
Then we have the following.

\begin{theorem}  There exists a unique Borel
measure $\sigma = \sigma_{n-1}$ on $S^{n-1}$ such
that if $f$ is measurable and $f \ge 0 $ or $f \in
L^1(\mathbb{R}^n)$ then
\[
\int_{\mathbb{R}^n} f(x) dx = \int^\infty_0
\int_{S^{n-1}} f(rx^\prime) r^{n-1} d \sigma
(x^\prime) dr.
\]
\end{theorem}
{\em Proof:}  (see Book.)

\begin{corollary}  If $f$ is measurable on
$\mathbb{R}^n, f \ge 0 $ or $f \in L^1$, and $f(x)
= g(|x|)$ where $g$ is a function on $(0, \infty)$ then

\[
\int_{\mathbb{R}^n} f(x) dx = \sigma (S^{n-1})
\int^\infty_0 g(r) r^{n-1} dr
\]
\end{corollary}

\begin{corollary} Let $a > 0, B = \{x \in
\mathbb{R}^n: |x| < a\}$, and $f$ a measurable function
on $\mathbb{R}^n$.

\begin{itemize}
\item[(a)]  If $|f(x)| \le c|x|^{- \alpha}$ on $B$ for some
$c > 0$ and $\alpha < n$ then $f \in L^1 (B)$.
\item[(b)]  If $|f(x)| \ge c|x|^{- \alpha}$ on $B^c$ for
some $c > 0 $ and $\alpha \ge n$ then $f \notin
L^1(B).$ 
\item[(c)]  If $|f(x)| \le c|x|^{- \alpha}$ on $B^c$ for
some $c > 0 $ and $\alpha > n $ then $f \in L^1(B^c)$
\item[(d)]  If $|f(x)| \ge c|x|^{- \alpha}$ on $B^c$ for
some $c > 0 $ and $\alpha \le n $ then $f \notin L^1
(B^c)$.
\end{itemize}
\end{corollary}

\vspace{.25cm}
\noindent
{\em Proof:}  It follows by the last corollary
applied to $|x|^{- \alpha} \chi_B$ and $|x|^{-\alpha}
\chi_{B^c}$.

\end{document}
