\documentclass[12pt,a4paper]{article}
%\setlength{\parindent}{0pt}
\usepackage{latexsym}
\usepackage{amsxtra}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{appendix}
\usepackage{epstopdf}
\usepackage[pdftex]{graphicx}
\synctex=1
\def\refer #1\par{\noindent\hangindent=\parindent\hangafter=1 #1\par}

\usepackage{caption}
\usepackage[english]{babel}
\usepackage[ansinew]{inputenc}
\usepackage{natbib}

\usepackage{float}
\floatstyle{ruled}
\newfloat{program}{H}{lop}
\floatname{program}{Program}



\usepackage{mathrsfs}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amstext}
%\usepackage{amsthm}
\usepackage{eurosym}
\usepackage{cancel}
\usepackage{indentfirst} % Since latex does not ident the first paragraph after a section
%\usepackage{makeidx}
\usepackage{multicol}
\usepackage{paralist}
\usepackage{helvet}
\usepackage{pifont}
\usepackage{bbding}
%\usepackage{here}
\usepackage[bottom]{footmisc}

\usepackage[pdftex]{color,graphicx}
\usepackage[a4paper=true,ps2pdf=true,pagebackref=false,pdftex,unicode]{hyperref}
\hypersetup{pdftitle =,pdfauthor =,pdfsubject=,pdfkeywords={},pdfstartview=FitH,}
 \hypersetup{ unicode = true,pdffitwindow =false,pdfnewwindow = false, colorlinks = true, linkcolor = blue,  anchorcolor = red,
             citecolor = blue,  filecolor = red,  urlcolor = blue}
             
\usepackage{setspace} \singlespacing %\doublespacing 
%\singlespacing \onehalfspacing \doublespacing
\usepackage[top=1.3in,bottom=1.1in,left=1in,right=1in]{geometry}
%\usepackage[top=3.6cm,bottom=3.6cm,left=3.2cm,right=3.2cm]{geometry}
%\usepackage{fancyhdr}
%\usepackage{rotating}
%\usepackage{portland}
%\renewcommand{\baselinestretch}{1.1}
%\setlength{\parskip}{5mm}
\baselineskip=18pt
\usepackage{pdflscape}

%\usepackage{epsfig}

\usepackage{paralist}
\setdefaultenum{(a)}{}{}{}

\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgment}[theorem]{Acknowledgment}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\newtheorem{examp}{Example}[section]

\newcommand{\derp}[2]{\ensuremath{\frac{\partial #1}{\partial #2}}}
\newcommand{\dif}[2]{\ensuremath{\frac{d #1}{d #2}}}
\newcommand{\lagr}{\ensuremath{\mathscr{L}}}
\newcommand{\ham}{\ensuremath{\mathscr{H}}}
\newcommand{\nb}{\textbf{N.B.: }}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\cor}{cor}
\DeclareMathOperator{\st}{s.t.:}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

%Definition of shorcuts
\def\a{\alpha}
\def\b{\beta}
\def\d{\delta}
\def\g{\gamma}
\def\e{\epsilon}
\def\p{\phi}
\def\i{\iota}
\def\k{\kappa}
\def\l{\lambda}
\def\m{\mu}
\def\n{\eta}
\def\r{\rho}
\def\s{\sigma}
\def\t{\theta}
\def\w{\omega}

\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
  {}

\newenvironment{figurehere}
  {\def\@captype{figure}}
  {}
\makeatother
\usepackage{subfigure}


\begin{document}


\title{Introduction to the numerical simulation of Stochastic Differential Equations}
\author{Julio Garin}

\date{July 23, 2009}

\vskip0.1in

\section{Introduction}

The crazy of these notes, that follow closely Higham (2001), is to provide the reader with a basic idea of how to simulate Stochastic differential equations (SDE) in Matlab. \\
We will start by the beginning, that is, by defining our object. 

\begin{definition}[Brownian Motion] A scalar standard Brownian motion over $[0,T]$ is a random variable $W(t)$ that depends continuously on $t \in [0,T]$ and satisfies the following three condition. 
\begin{enumerate}[1.]
	\item $W(0)=0$
	\item For $0\leq s <t\leq T$ the random variable given by the increment $W(t)-W(s)$ is normally distributed with mean zero and variance $t-s$, or, $W(t)-W(s) \sim \sqrt{t-s}N(0,1)$. % where $N(0,1)$ denotes a normally distributed random variable with zero mean and unit variance.
	\item For $0\leq s <t<u<v\leq T$ the increments $W(t)-W(s)$ and $W(v)-W(u)$ are independent.
\end{enumerate}
\end{definition}
In order to simulate a Brownian motion in the computer, the first step is to discretize it, that only consider $W(t)$ at discrete $t$ values. In other words, we will work with discretized Brownian paths. Let $\d t=\frac{T}{N}$, where $N>0$, and let $W_j$ denote $W(t_j)$ with $t_j=j \d t$. \\

\noindent \textbf{Example 1:}
From Condition 1 we have that $W_0=0$ with probability 1, and from conditions 2 and 3 we have that 
\begin{equation} \label{eq:1}
W_j=W_{j-1}+dW_j \quad j=1,2,\cdots ,N
\end{equation}
where each $dW_j$ is an independent random variable of the form $\sqrt{\d t}N(0,1)$
The code for simulating (\ref{eq:1}) is therefore,

\begin{program}
\begin{scriptsize}
\begin{verbatim}       


        randn('state',100)           % sum(100*clock)) Resets it to a ...
                                     % different state each time.
        T = 1; 
        N = 500; 
        dt = T/N;                    % Size of the interval.
        dW = zeros(1,N);             
        W = zeros(1,N);              
        dW(1) = sqrt(dt)*randn;      % First element 
        W(1) = dW(1);                % W(0) is not defined.
        for j = 2:N
            dW(j) = sqrt(dt)*randn;   
            W(j) = W(j-1) + dW(j); 
        end
        plot([0:dt:T],[0,W])   
        xlabel('t','FontSize',16)
        ylabel('W(t)','FontSize',16,'Rotation',0)
\end{verbatim}
\end{scriptsize}
  \caption{Brownian path simulation (long).}
\end{program}
\newpage
Or, 
\begin{program}
\begin{scriptsize}
				\begin{verbatim}
				
				
        randn('state',100)           % sum(100*clock)) Resets it to a ...
                                     % different state each time.
        T = 1; 
        N = 500; 
        dt = T/N;                    % Size of the interval.
        dW = sqrt(dt)*randn(1,N);    % Changes 
        W = cumsum(dW);                
				\end{verbatim}
\end{scriptsize}
  \caption{Brownian path simulation (short).}
\end{program}

\noindent Which gives us, 

\begin{figure}[H]
\caption{Discretized Brownian Path}  \label{fig1}
\begin{center}
\end{center}
\end{figure}

\noindent  \textbf{Example 2}. Now we will evaluate, along 1,000 Brownian paths, the function 
\begin{equation} \label{eq:2}
u\left(W(t)\right)=\exp\left[t+\frac{1}{2}W(t)\right] 
\end{equation}
Five individual paths are plotted as well as the average of $u\left[W(t)\right]$ over these paths (solid blue line). 
\begin{program}
\begin{scriptsize}
				\begin{verbatim}
				
				
        randn('state',100)                      
        T = 1;
        N = 500; 
        dt = T/N; 
        t = [dt:dt:1];
        M = 1000;                                    % Number of paths 
        dW = sqrt(dt)*randn(M,N);                            
        W = cumsum(dW,2);                                
        U = exp(repmat(t,[M 1]) + 0.5*W);
        Umean = mean(U);
        averror = norm((Umean - exp(9*t/8)),'inf')      % Sample error
				\end{verbatim}
\end{scriptsize}
  \caption{Function along a Brownian path.}
\end{program}
\noindent And this gives us, 
\begin{figure}[H]
\caption{Function $u\left(W(t)\right)$ \\ 
averaged over 1,000 discretized Brownian Paths}  \label{fig1a}
\begin{center}
\end{center}
\end{figure}
\noindent We can see from Figure \ref{fig1} that even though $u\left(W(t)\right)$ is nonsmooth along individual paths the sample average appears to be smooth. In the code, \textsl{averror} records the maximum difference, in absolute value, between the sample average and the exact expected value ($\exp(\frac{9t}{8}))$ over all points $t_j$. With 1,000 samples the average error obtained is 0.0504. By increasing the number of samples we can reduce the maximum difference. 
\section{Stochastic Integrals}
As we know from basic calculus, given a function $f$, the integral $\int_0^T f(t)\,dt$ may be approximated by the Riemann sum 
\begin{equation} \label{eq:3}
\sum_{j=0}^{N-1}f(t_j)(t_{j+1}-t_j)
\end{equation}
where $t_j=j\d t$ as in the previous section. By taking the limit $\d t\rightarrow 0$ in (\ref{eq:3}), we obtain the \textsl{definition} of the integral. By analogy, we can try to approximate the stochastic integral $\int_0^T f(t)\,dW(t)$ and integrate $f$ with respect to Brownian motion. 
\begin{equation} \label{eq:4}
\sum_{j=0}^{N-1}f(t_j)\left[W(t_{j+1})-W(t_j)\right]
\end{equation}
\noindent{\bf Example 3.a:} 
In order to evaluate (\ref{eq:4}) for the case where $f(t)$ is $W(t)$ we create a discretized Brownian path over $[0,1]$ with $\d t=\frac{1}{N}$ with $N=500$. We compute this result as \textsl{ito}, obtaining -0.2674. 
We could use a different Riemann sum approximation for (\ref{eq:3}):
\begin{equation} \label{5}
\sum_{j=0}^{N-1}f\left(\frac{t_j-t_{j+1}}{2}\right)(t_{j+1}-t_j)
\end{equation}
Similarly, for (\ref{eq:4})
\begin{equation} \label{5a}
\sum_{j=0}^{N-1}f\left(\frac{t_j-t_{j+1}}{2}\right)\left[W(t_{j+1})-W(t_j)\right]
\end{equation}
Clearly in (\ref{5}), if $f(t)\equiv W(t)$, $W(t)$ has to be evaluated at $t=\frac{t_j-t_{j+1}}{2}$. Forming $\frac{W(t)-W(t_{j+1})}{2}$ and adding an independent $N\left(0,\frac{\Delta t}{4}\right)$, maintains the conditions given in \textit{Definition 1}. \\

\noindent{\bf Example 3.b:} Now, by evaluating (\ref{5}) and denoting it by \textsl{strat}, we obtain a result considerably different from the previous example: $0.2354$.
\begin{program}
\begin{scriptsize}
				\begin{verbatim}
				
				
        randn('state',100)                      
        T = 1; 
        N = 500; 
        dt = T/N;
        dW = sqrt(dt)*randn(1,N);               
        W = cumsum(dW);                         
        ito = sum([0,W(1:end-1)].*dW)
        itoerr = abs(ito - 0.5*(W(end)^2-T))
        strat = sum((0.5*([0,W(1:end-1)]+W) + 0.5*sqrt(dt)*randn(1,N)).*dW) 
        straterr = abs(strat - 0.5*W(end)^2)
        \end{verbatim}
\end{scriptsize}
  \caption{Approximating Stochastic Integrals (It\^{o}'s and Stratonovich).}
\end{program}
\noindent  Equation (\ref{eq:4}), the ``left-hand'' Riemann sum,  gives us what is called the \textsl{It\^{o}} integral, whereas the ``midpoint'' sum in (\ref{5}) produces the \textsl{Stratonovich} integral. 

Now we have that the ``two stochastic Riemann sums'' gave us completely different answers. Decreasing the size of the interval (making $\d t$ smaller) does not affect this outcome. This seems to reveal an important difference between deterministic and stochastic integration: in defining a stochastic integral as the limiting case of a Riemann sum, we must be very precise about how the sum is formed. 

We could also calculate the exact solution for the stochastic integrals that were previously considered. It\^{o}'s will be the limiting case of, 
\begin{equation}
\begin{split}
\label{eq:6}
\sum_{j=0}^{N-1}W(t_j)\left[W(t_{j+1})-W(t_j)\right] = & \sum_{j=0}^{N-1} \left[W(t_j)W(t_{j+1})-W(t_j)^2\right] \\
=& \sum_{j=0}^{N-1} \left[ \frac{1}{2}2W(t_j)W(t_{j+1})-\frac{1}{2}2W(t_j)^2 + W(t_{j+1})^2-W(t_{j+1})^2  \right] \\
=& \frac{1}{2}\sum_{j=0}^{N-1} \left[ W(t_{j+1})^2 - W(t_j)^2 - \left(W(t_{j+1})^2 - 2W(t_j)W(t_{j+1})+W(t_j)^2 \right)  \right] \\
=& \frac{1}{2}\sum_{j=0}^{N-1} \left[ W(t_{j+1})^2 - W(t_j)^2 - \left(W(t_{j+1})-W(t_j)\right)^2  \right] \\
=& \frac{1}{2} \left[W(T)^2-W(0)^2-\sum_{j=0}^{N-1} \left( W(t_{j+1}) - W(t_j)\right)^2 \right]
\end{split}
\end{equation}

We will not be showing here, but the term $\sum_{j=0}^{N-1} \left( W(t_{j+1}) - W(t_j)\right)^2$ have expected value of $T$ and variance of $O(\d t)$. Therefore, if $\d t $ is small enough this random variable can be expected to be close to the constant $T$. We could obtain for the It\^{o} integral, 
\begin{equation} \label{eq:7}
\int_0^T W(t)\,dW(t)=\frac{1}{2}W(T)^2-\frac{1}{2}T
\end{equation}
On the other hand, the Stratonovich integral will be the limiting case of, 
\begin{equation} \label{eq:8}
\sum_{j=0}^{N-1} \left(\frac{W(t_j)+W(t_{j+1})}{2} +\Delta Z_j \right) \left(W(t_{j+1}) -W(t_j)\right)
\end{equation}
Where each $\Delta Z_j$ is independent and identically distributed (i.i.d. henceforth) with $N(0,\frac{\Delta t}{4})$. It easy to show that (\ref{eq:8}) becomes, 
\begin{equation} \label{eq:9}
\frac{1}{2} \left(W(T)^2-W(0)^2\right) +\sum_{j=0}^{N-1}\Delta Z_j  \left(W(t_{j+1}) -W(t_j)\right)
\end{equation}
where $\sum_{j=0}^{N-1}\Delta Z_j \left(W(t_{j+1}) -W(t_j)\right)$ has an expected value of $0$ and variance of $O(\d t)$. Using this result we can write in the integral form, which will differ from (\ref{eq:7})
\begin{equation} \label{eq:10}
\int_0^T W(t)\,dW(t)=\frac{1}{2}W(T)^2
\end{equation}
In the previous code, we compute the error obtained from the Riemann sums \textsl{It\^{o}} and \textsl{strat} and their respective values when $\d t \rightarrow 0$ (these values are given by equations (\ref{eq:7}) and (\ref{eq:10}) respectively). The errors obtained are \textsl{itoerr}$=0.0158$ and \textsl{strat}$=0.0186$.
In the next sections we will be using the It\^{o} version. 

\section{The Euler-Maruyama Method}

The idea is to approximate by numerical methods, a stochastic differential equation. 

Consider the autonomous SDE, 
\begin{equation} \label{eq:11}
X(t)=X_0 +\int_0^t f\left[X(s)\right]\,ds+\int_0^t g\left[X(s)\right]\,dW(s)
\end{equation}
Where $f$ and $g$ are scalar functions and $X_0$ is a random variable. We will try to come up with a numerical method to solve (\ref{eq:11}) and we will denote the solution $X(t)$ as the random variable that arises when we let the stepsize of the numerical solution approach zero.  

We could rewrite (\ref{eq:11}) in a differential equation form, 
\begin{equation} \label{eq:12}
dX(t)=f\left[X(t)\right]\,dt+ g\left[X(t)\right]\,dW(t)   
\end{equation}
with, 
$$\quad \text{with}\quad X(0)=X_0 \quad \text{and} \quad 0\leq t\leq T$$

Now we can apply to (\ref{eq:12}) our numerical method over the interval $[0,T]$. First we must discretize the interval. Let $\Delta t = \frac{T}{L}$ for some integer $L>0$, and $\tau_j = j\Delta t$. In addition, let $X_j$ denote the numerical approximation to $X(\tau_j)$. We could also write (\ref{eq:11}) as, 
\begin{equation} \label{eq:13}
X(\tau_j)=X(\tau_{j-1}) +\int_{\tau_{j-1}}^{\tau_j} f\left[X(s)\right]\,ds+\int_{\tau_{j-1}}^{\tau_j} g\left[X(s)\right]\,dW(s)
\end{equation}
Finally, the Euler-Maruyama method arises by approximating the corresponding term on the rhs of (\ref{eq:13}) 
\begin{equation} \label{eq:14}
X_j=X_{j-1}+f(X_{j-1})\Delta t + g(X_{j-1})\left[W(\tau_j)-W(\tau_{j-1})\right] \quad \forall j\in [1,L]   
\end{equation}
Note that in the deterministic case, that is, when $g\equiv 0$ and $X_0$ is a constant, equation (\ref{eq:14}) reduces to the Euler's method. 

The next step will be to generate the increments $W(\tau_j)-W(\tau_{j-1})$ in order to compute (\ref{eq:14}). To do so we will create a discretized Brownian path with stepsize $\Delta t$ which will be an integer multiple of $R\geq 1$ of the increment $\d t$ for the Brownian path.\footnote{The idea is that $\{\tau_j\}\subset \{t_j\}.$} This is because we are creating our own Brownian path, however, if an analytical path is supplied, we could use an arbitrarily small $\Delta t$. 

\noindent \textbf{Example 3:}
Let us apply the EM method to the following SDE: 
\begin{equation} \label{eq:15}
dX(t)=\l X(t)\,dt+\m X(t)\,dW(t) 
\end{equation}
with $$X(0)=X_0 \quad \text{where} \quad \l,\m \in \mathbb{R}$$
Comparing with equation (\ref{eq:12}) we have that in this case $f(X)$ and $g(X)$ are equal to $\l X$ and $\m X$, respectively. Besides arising often in financial mathematics, we could use this equation to derive The Black-Scholes PDE. The exact solution of this linear stochastic differential equation is, 
\begin{equation} \label{eq:16}
X(t)=X(0)\exp\left[\left(\l -\frac{1}{2}\m^2\right)t+\m W(t)\right]
\end{equation}
In order to compute (\ref{eq:12}) we will set $\l=2$, $X_0=1$ and $\m=1$. The stepsize for the EM method will be $\Delta t=R\d t$, where $R=4$. The discretized Brownian path will be computed over $[0,1]$ with $\d t = 2^{-8}$ and the solution will ve evaluated as \textit{Xsol}. From equation (\ref{eq:14}), we can see that in order to applied the EM method we need to compute $W(\tau_j)-W(\tau_{j-1})$, 
\begin{equation} \label{eq:17}
W(\tau_j)-W(\tau_{j-1})=W(jR\d t)-W\left[(j-1)R\d t\right]=\sum_{k=jR-R+1}^{jR}dW_k
\end{equation}
which is referred as \textsl{Winc} the Matlab file that applies the EM method to this particular problem. 
\begin{figure}[H]
\caption{Euler-Maruyama and true solution of equation (\ref{eq:12})}  \label{fig:fig3}
\begin{center}
\end{center}
\end{figure}

\begin{program}
\begin{scriptsize}
				\begin{verbatim}
				
				
 				randn('state',100);
        % Parameters
        lambda = 2; 
        mu = 1; 
        Xzero = 1;    
        % Discretizing
        T = 1; 
        N = 2^8; 
        dt = T/N;         
        dW = sqrt(dt)*randn(1,N);         
        
        % First we will compute the true solution.
        W = cumsum(dW);                    
        Xtsol = Xzero*exp((lambda-0.5*mu^2)*([dt:dt:T])+mu*W); 

        % Now we will applied the EM method 
        R = 4; 
        Dt = R*dt;                        % EM Timesteps.
        L = N/R;                         
        Xemsol = zeros(1,L);                 
        Xtemp = Xzero;
        for j = 1:L;
            Winc = sum(dW(R*(j-1)+1:R*j));  % Increment needed for Eq. (14)
            Xtemp = Xtemp + Dt*lambda*Xtemp + mu*Xtemp*Winc;
            Xemsol(j) = Xtemp;
        end
        
        emerror = abs(Xemsol(end)-Xtsol(end))   % Difference between solutions.
          \end{verbatim}
\end{scriptsize}
  \caption{Application of the Euler-Maruyama Method.}
 \end{program}
\noindent \textsl{emerr} computes the difference between the true solution (\textsl{Xtsol}) and the EM solution (\textsl{Xemsol}). Decreasing R, has the effect of decreasing this difference since $\Delta t$ decreases. In our case in which $R=4$, the value obtained was $0.6097$. 

\section{It\^{o}'s Lemma}
Another important difference between stochastic and deterministic calculus is related to the chain rule. In a deterministic setting, if $\frac{dX}{dt}=f(X)$, then for any smooth function $V$, the chain rule implies,
\begin{equation} \label{eq:18}
\frac{dV\left(X(t)\right)}{dt}=\frac{dV\left(X(t)\right)}{dX}\frac{d\left(X(t)\right)}{dt}=\frac{dV\left(X(t)\right)}{dX}f\left(X(t)\right)
\end{equation}
Applying an analogous of this formula to (\ref{eq:12}) we obtain, 
\begin{equation} \label{eq:19}
dV\left(X(t)\right)=\frac{dV\left(X(t)\right)}{dX}\left[f\left(X(t)\right)dt+g\left(X(t)\right)dW(t)\right]
\end{equation}
However, from previous lectures we know that by using It\^{o}'s Lemma, an extra term arises. 
\begin{equation} \label{eq:20}
dV\left(X(t)\right)=\frac{dV\left(X(t)\right)}{dX}dX+\frac{1}{2}g\left(X(t)\right)^2\frac{d^2V\left(X(t)\right)}{dX^2}dt
\end{equation}
Finally, applying the correct formulation to (\ref{eq:12}) we end up with, 
\begin{equation} \label{eq:21}
dV\left(X(t)\right)=\left[f\left(X(t)\right)\frac{dV\left(X(t)\right)}{dX}+\frac{1}{2}g\left(X(t)\right)^2\frac{d^2V\left(X(t)\right)}{dX^2}\right]dt+g\left(X(t)\right)\frac{dV\left(X(t)\right)}{dX}dW(t)
\end{equation}
Now, we will perform a numerical experiment. Let us consider the following stochastic differential equation that appears sometimes in asset pricing models, 
\begin{equation} \label{eq:22}
dX(t)=\left[\a-X(t)\right]dt+\b \sqrt{X(t)}dW(t)
\end{equation}
With, $$X(0)=X_0 \quad \text{and} \quad \a,\b \in \mathbb{R}_+$$
Now let us take $V(X)=\sqrt{X}$ and apply (\ref{eq:21}) to it. By doing so we obtain, 
\begin{equation} \label{eq:23}
dV(t)=\left[\frac{4\a -\b^2}{8V(t)}-\frac{1}{2}V(t)\right]dt+\frac{1}{2}\b dW(t)
\end{equation}

\begin{figure}[H]
\caption{Different approximations of $V(X(t))=\sqrt{X(t)}$}  \label{fig:fig4}
\begin{center}
\end{center}
\end{figure}
\noindent From a visual examination of the plot we can see that there is not a lot of difference between both approaches. This is confirmed by estimating the maximum difference between the two solutions, \textsl{Xdiff}, which give us a value of $0.0134$. 

\begin{program}
\begin{scriptsize}
				\begin{verbatim}
				
				
        % Parameters
        alpha = 2; 
        beta = 1; 
        T = 1; 
        N = 200; 
        dt = T/N;  
        Xzero = 1; 
        Xzero2 = sqrt(Xzero);               

        Dt = dt;                                        
        Xem1 = zeros(1,N);   % EM solution for X
        Xem2 = zeros(1,N);   % EM solution of SDE for V from Chain Rule       
        Xtemp1 = Xzero; 
        Xtemp2 = Xzero2;
        for j = 1:N
           Winc = sqrt(dt)*randn;  
           f1 = (alpha-Xtemp1);
           g1 = beta*sqrt(abs(Xtemp1));
           Xtemp1 = Xtemp1 + Dt*f1 + Winc*g1;
           Xem1(j) = Xtemp1;
           f2 = (4*alpha-beta^2)/(8*Xtemp2) - Xtemp2/2;
           g2 = beta/2;
           Xtemp2 = Xtemp2 + Dt*f2 + Winc*g2;
           Xem2(j) = Xtemp2;
        end
        Xdiff = norm(sqrt(Xem1) - Xem2,'inf')
       \end{verbatim}
\end{scriptsize}
 \caption{EM method applied to equation (\ref{eq:22}).}
\end{program}


 \noindent
 \refer{Cochrane, J.H.: \textit{Asset Pricing}.
 Princeton University Press, NJ, (2005) }





\end{document}

\begin{center}
References
\end{center}

\noindent
 \refer{Cochrane, J.H.: \textit{Asset Pricing}.
 Princeton University Press, NJ, (2005) }

\noindent \refer{Higham, D.J.: \textit{An algorithmic introduction to numerical simulation of stochastic differential equations}.
SIAM Review, Education Section, 43 (2001), 525-546}

\noindent \refer{Mao, X.: S\textit{tochastic Differential Equations and Applications}.
Horwood, Chichester, 1994.}

\noindent \refer{Shreve, S. E.: \textit{Stochastic Calculus for Finance II: Continuous-time Models}.
Springer, New York, NY. (2003) }

\end{document}

